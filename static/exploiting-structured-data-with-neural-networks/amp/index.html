
<head>
    <meta charset="utf-8">

    <title>Exploiting Structured Data with Neural Networks</title>

    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">

    <link rel="shortcut icon" href="../../favicon.png">

    <link rel="shortcut icon" href="../../favicon.png" type="image/png">
    <link rel="canonical" href="../index.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <meta property="og:site_name" content="The Gradient">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Exploiting Structured Data with Neural Networks">
    <meta property="og:description" content="Introduction Data across many domains have underlying relational structure that may be represented through graphs. Friendships on a social network, for example, induce a network that connects users with other related users. Autonomous agents, such as a self-driving car, coexist with many objects, such as other vehicles, pedestrians, and stationary">
    <meta property="og:url" content="http://localhost:2368/exploiting-structured-data-with-neural-networks/">
    <meta property="og:image" content="https://images.unsplash.com/photo-1495592822108-9e6261896da8?ixlib=rb-0.3.5&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ&amp;s=c7f31a8c5fd0fea1620b5595ce12eb73">
    <meta property="article:published_time" content="2018-03-09T08:45:00.000Z">
    <meta property="article:modified_time" content="2018-04-11T06:13:53.000Z">
    <meta property="article:tag" content="Overviews">
    
    <meta property="article:publisher" content="https://www.facebook.com/gradientml/">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Exploiting Structured Data with Neural Networks">
    <meta name="twitter:description" content="Introduction Data across many domains have underlying relational structure that may be represented through graphs. Friendships on a social network, for example, induce a network that connects users with other related users. Autonomous agents, such as a self-driving car, coexist with many objects, such as other vehicles, pedestrians, and stationary">
    <meta name="twitter:url" content="http://localhost:2368/exploiting-structured-data-with-neural-networks/">
    <meta name="twitter:image" content="https://images.unsplash.com/photo-1495592822108-9e6261896da8?ixlib=rb-0.3.5&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ&amp;s=c7f31a8c5fd0fea1620b5595ce12eb73">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Gary Gradient">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="Overviews">
    <meta name="twitter:site" content="@gradientml">
    <meta property="og:image:width" content="1080">
    <meta property="og:image:height" content="720">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "The Gradient",
        "logo": "http://localhost:2368/content/images/2018/04/logo3.png"
    },
    "author": {
        "@type": "Person",
        "name": "Gary Gradient",
        "url": "http://localhost:2368/author/gary/",
        "sameAs": []
    },
    "headline": "Exploiting Structured Data with Neural Networks",
    "url": "http://localhost:2368/exploiting-structured-data-with-neural-networks/",
    "datePublished": "2018-03-09T08:45:00.000Z",
    "dateModified": "2018-04-11T06:13:53.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://images.unsplash.com/photo-1495592822108-9e6261896da8?ixlib=rb-0.3.5&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=1080&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ&s=c7f31a8c5fd0fea1620b5595ce12eb73",
        "width": 1080,
        "height": 720
    },
    "keywords": "Overviews",
    "description": "Introduction Data across many domains have underlying relational structure that may be represented through graphs. Friendships on a social network, for example, induce a network that connects users with other related users. Autonomous agents, such as a self-driving car, coexist with many objects, such as other vehicles, pedestrians, and stationary",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:2368/"
    }
}
    </script>

    <meta name="generator" content="Ghost 1.22">
    <link rel="alternate" type="application/rss+xml" title="The Gradient" href="../../rss/index.html">

    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic%7COpen+Sans:700,600,400">
    <style amp-custom>html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,aside,details,figcaption,figure,footer,header,main,menu,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block;vertical-align:baseline}audio:not([controls]){display:none;height:0}[hidden],template{display:none}a{background-color:transparent}a:active,a:hover{outline:0}abbr[title]{border-bottom:1px dotted}b,strong{font-weight:bold}dfn{font-style:italic}h1{margin:0.67em 0;font-size:2em}mark{background:#ff0;color:#000}small{font-size:80%}sub,sup{position:relative;vertical-align:baseline;font-size:75%;line-height:0}sup{top:-0.5em}sub{bottom:-0.25em}img{border:0}amp-img{border:0}svg:not(:root){overflow:hidden}figure{margin:1em 40px}hr{box-sizing:content-box;height:0}pre{overflow:auto}code,kbd,pre,samp{font-family:monospace, monospace;font-size:1em}button,input,optgroup,select,textarea{margin:0;color:inherit;font:inherit}button{overflow:visible}button,select{text-transform:none}button,html input[type="button"],input[type="reset"],input[type="submit"]{cursor:pointer;-webkit-appearance:button}button[disabled],html input[disabled]{cursor:default}button::-moz-focus-inner,input::-moz-focus-inner{padding:0;border:0}input{line-height:normal}input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}input[type="number"]::-webkit-inner-spin-button,input[type="number"]::-webkit-outer-spin-button{height:auto}input[type="search"]{-webkit-appearance:textfield}input[type="search"]::-webkit-search-cancel-button,input[type="search"]::-webkit-search-decoration{-webkit-appearance:none}fieldset{margin:0 2px;padding:0.35em 0.625em 0.75em;border:1px solid #c0c0c0}legend{padding:0;border:0}textarea{overflow:auto}optgroup{font-weight:bold}table{border-spacing:0;border-collapse:collapse}td,th{padding:0}html{max-height:100%;height:100%;font-size:62.5%;-webkit-tap-highlight-color:rgba(0, 0, 0, 0)}body{max-height:100%;height:100%;color:#3a4145;background:#f4f8fb;letter-spacing:0.01rem;font-family:"Merriweather", serif;font-size:1.8rem;line-height:1.75em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"kern" 1;-moz-font-feature-settings:"kern" 1;-o-font-feature-settings:"kern" 1}::-moz-selection{background:#d6edff}::selection{background:#d6edff}h1,h2,h3,h4,h5,h6{margin:0 0 0.3em 0;color:#2e2e2e;font-family:"Open Sans", sans-serif;line-height:1.15em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-moz-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-o-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1}h1{text-indent:-2px;letter-spacing:-1px;font-size:2.6rem}h2{letter-spacing:0;font-size:2.4rem}h3{letter-spacing:-0.6px;font-size:2.1rem}h4{font-size:1.9rem}h5{font-size:1.8rem}h6{font-size:1.8rem}a{color:#4a4a4a}a:hover{color:#111}p,ul,ol,dl{margin:0 0 2.5rem 0;font-size:1.5rem;text-rendering:geometricPrecision;-webkit-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-moz-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-o-font-feature-settings:"liga" 1, "onum" 1, "kern" 1}ol,ul{padding-left:2em}ol ol,ul ul,ul ol,ol ul{margin:0 0 0.4em 0;padding-left:2em}dl dt{float:left;clear:left;overflow:hidden;margin-bottom:1em;width:180px;text-align:right;text-overflow:ellipsis;white-space:nowrap;font-weight:700}dl dd{margin-bottom:1em;margin-left:200px}li{margin:0.4em 0}li li{margin:0}hr{display:block;margin:1.75em 0;padding:0;height:1px;border:0;border-top:#efefef 1px solid}blockquote{box-sizing:border-box;margin:1.75em 0 1.75em 0;padding:0 0 0 1.75em;border-left:#4a4a4a 0.4em solid;-moz-box-sizing:border-box}blockquote p{margin:0.8em 0;font-style:italic}blockquote small{display:inline-block;margin:0.8em 0 0.8em 1.5em;color:#ccc;font-size:0.9em}blockquote small:before{content:"\2014 \00A0"}blockquote cite{font-weight:700}blockquote cite a{font-weight:normal}mark{background-color:#fdffb6}code,tt{padding:1px 3px;border:#e3edf3 1px solid;background:#f7fafb;border-radius:2px;white-space:pre-wrap;font-family:Inconsolata, monospace, sans-serif;font-size:0.85em;font-feature-settings:"liga" 0;-webkit-font-feature-settings:"liga" 0;-moz-font-feature-settings:"liga" 0}pre{overflow:auto;box-sizing:border-box;margin:0 0 1.75em 0;padding:10px;width:100%;border:#e3edf3 1px solid;background:#f7fafb;border-radius:3px;white-space:pre;font-family:Inconsolata, monospace, sans-serif;font-size:0.9em;-moz-box-sizing:border-box}pre code,pre tt{padding:0;border:none;background:transparent;white-space:pre-wrap;font-size:inherit}kbd{display:inline-block;margin-bottom:0.4em;padding:1px 8px;border:#ccc 1px solid;background:#f4f4f4;border-radius:4px;box-shadow:0 1px 0 rgba(0, 0, 0, 0.2), 0 1px 0 0 #fff inset;color:#666;text-shadow:#fff 0 1px 0;font-size:0.9em;font-weight:700}table{box-sizing:border-box;margin:1.75em 0;max-width:100%;width:100%;background-color:transparent;-moz-box-sizing:border-box}table th,table td{padding:8px;border-top:#efefef 1px solid;vertical-align:top;text-align:left;line-height:20px}table th{color:#000}table caption + thead tr:first-child th,table caption + thead tr:first-child td,table colgroup + thead tr:first-child th,table colgroup + thead tr:first-child td,table thead:first-child tr:first-child th,table thead:first-child tr:first-child td{border-top:0}table tbody + tbody{border-top:#efefef 2px solid}table table table{background-color:#fff}table tbody > tr:nth-child(odd) > td,table tbody > tr:nth-child(odd) > th{background-color:#f6f6f6}table.plain tbody > tr:nth-child(odd) > td,table.plain tbody > tr:nth-child(odd) > th{background:transparent}iframe,amp-iframe,.fluid-width-video-wrapper{display:block;margin:1.75em 0}.fluid-width-video-wrapper iframe,.fluid-width-video-wrapper amp-iframe{margin:0}textarea,select,input{margin:0 0 5px 0;padding:6px 9px;width:260px;outline:0;border:#e7eef2 1px solid;background:#fff;border-radius:4px;box-shadow:none;font-family:"Open Sans", sans-serif;font-size:1.6rem;line-height:1.4em;font-weight:100;-webkit-appearance:none}textarea{min-width:250px;min-height:80px;max-width:340px;width:100%;height:auto}input[type="text"]:focus,input[type="email"]:focus,input[type="search"]:focus,input[type="tel"]:focus,input[type="url"]:focus,input[type="password"]:focus,input[type="number"]:focus,input[type="date"]:focus,input[type="month"]:focus,input[type="week"]:focus,input[type="time"]:focus,input[type="datetime"]:focus,input[type="datetime-local"]:focus,textarea:focus{outline:none;outline-width:0;border:#bbc7cc 1px solid;background:#fff}select{width:270px;height:30px;line-height:30px}.clearfix:before,.clearfix:after{content:" ";display:table}.clearfix:after{clear:both}.clearfix{zoom:1}.main-header{position:relative;display:table;overflow:hidden;box-sizing:border-box;width:100%;height:50px;background:#5ba4e5 no-repeat center center;background-size:cover;text-align:left;-webkit-box-sizing:border-box;-moz-box-sizing:border-box}.content{background:#fff;padding-top:15px}.blog-title,.content{margin:auto;max-width:600px}.blog-title a{display:block;padding-right:16px;padding-left:16px;height:50px;color:#fff;text-decoration:none;font-family:"Open Sans", sans-serif;font-size:16px;line-height:50px;font-weight:600}.post{position:relative;margin-top:0;margin-right:16px;margin-left:16px;padding-bottom:0;max-width:100%;border-bottom:#ebf2f6 1px solid;word-wrap:break-word;font-size:0.95em;line-height:1.65em}.post-header{margin-bottom:1rem}.post-title{margin-bottom:0}.post-title a{text-decoration:none}.post-meta{display:block;margin:3px 0 0 0;color:#9eabb3;font-family:"Open Sans", sans-serif;font-size:1.3rem;line-height:2.2rem}.post-meta a{color:#9eabb3;text-decoration:none}.post-meta a:hover{text-decoration:underline}.post-meta .author{margin:0;font-size:1.3rem;line-height:1.3em}.post-date{display:inline-block;text-transform:uppercase;white-space:nowrap;font-size:1.2rem;line-height:1.2em}.post-image{margin:0;padding-top:3rem;padding-bottom:30px;border-top:1px #E8E8E8 solid}.post-content amp-img,.post-content amp-anim{position:relative;left:50%;display:block;padding:0;min-width:0;max-width:112%;width:calc(100% + 32px);height:auto;transform:translateX(-50%);-webkit-transform:translateX(-50%);-ms-transform:translateX(-50%)}.footnotes{font-size:1.3rem;line-height:1.6em;font-style:italic}.footnotes li{margin:0.6rem 0}.footnotes p{margin:0}.footnotes p a:last-child{text-decoration:none}.site-footer{position:relative;margin:0 auto 20px auto;padding:1rem 15px;max-width:600px;color:rgba(0,0,0,0.5);font-family:"Open Sans", sans-serif;font-size:1.1rem;line-height:1.75em}.site-footer a{color:rgba(0,0,0,0.5);text-decoration:none;font-weight:bold}.site-footer a:hover{border-bottom:#bbc7cc 1px solid}.poweredby{display:block;float:right;width:45%;text-align:right}.copyright{display:block;float:left;width:45%}</style>

    <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
    <script async src="https://cdn.ampproject.org/v0.js"></script>

    

</head>

<body class="amp-template">
    <header class="main-header">
        <nav class="blog-title">
            <a href="../../">The Gradient</a>
        </nav>
    </header>

    <main class="content" role="main">
        <article class="post">

            <header class="post-header">
                <h1 class="post-title">Exploiting Structured Data with Neural Networks</h1>
                <section class="post-meta">
                    <p class="author">by <a href="../../author/gary/">Gary Gradient</a></p>
                    <time class="post-date" datetime="2018-03-09">2018-03-09</time>
                </section>
            </header>
            <figure class="post-image">
                <amp-img src="https://images.unsplash.com/photo-1495592822108-9e6261896da8?ixlib=rb-0.3.5&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ&amp;s=c7f31a8c5fd0fea1620b5595ce12eb73" width="600" height="400" layout="responsive"></amp-img>
            </figure>
            <section class="post-content">

                <div class="kg-card-markdown"><h2 id="introduction">Introduction</h2>
<p>Data across many domains have underlying relational structure that may be represented through graphs. Friendships on a social network, for example, induce a network that connects users with other related users. Autonomous agents, such as a self-driving car, coexist with many objects, such as other vehicles, pedestrians, and stationary obstacles, that together induce a multimodal network that represents spatial relationships between these entities. The labels applied to images may share semantic relationships inferred from WordNet or some other knowledge graph. Additionally, the local topology of an array of sensors may be represented through a graph embedding.<br>
$\newcommand{\argmin}{\text{arg min}}\newcommand{\argmax}{\text{arg max}}\newcommand{\minimize}{\text{minimize}}\newcommand{\maximize}{\text{maximize}}\newcommand{\abs}{\text{abs}}$</p>
<p></p>
<p>Despite this, relational structure is often discarded when developing machine learning models. For example, image classification tasks over photos scraped from social media may disregard the underlying social structure in spite of its apparent usefulness in characterizing entities. Recently, a plethora of papers have approached the "fashion-recommendation problem", which seeks to determine sets of clothing items that constitute a cohesive "style", often personalized for a particular individual. One successful approach has been to scrape images from a social media site, such as Instagram, extract the collection of clothing items present in the image, and then use the popularity of the image as a proxy for the fashionability of the basket of clothing. While it is certainly possible to learn fashion-styles in this way, the social network and other graphs, such as the Amazon product graph, contain valuable structural information that may improve the model.</p>
<p>The reason structural information is often ignored is clear: it has been notoriously challenging to extract meaningful features from graphs in general. Although adjacency matrices (or, equivalently, incidence vectors) are the most straightforward way to express graphs in vector form, this representation is indefeasibly large for real-world graphs, which often contain millions, if not billions, of nodes. Additionally, since graphs are inherently an unordered structure, it does not necessarily make sense to specify an ordering as implied by an adjacency matrix. Convolutional neural networks (CNNs) applied to image classification and natural language processing (NLP) successfully leverage the strong locality in images and human language. Since pixel and text neighborhoods are of constant size, a constant-sized weight matrix may be convolved with the structured input to produce higher-order features. Since the shape of node neighborhoods in a graph are variable (some nodes may be low-degree while others may be high-degree), there is not an immediate graph analogue to convolutional layers.</p>
<p>We shall explore a variety of recent developments in machine learning that attempt to leverage graphical structure in order to either directly improve performance or decrease the number of parameters used in the model. We shall first consider techniques such as <em>DeepWalk</em> and <em>node2vec</em> that seek to extract structural summary features from the neighborhood of nodes. Then, we shall look at <em>graph convolutional networks</em> (GCNs) and dynamic graphical filters that generalize CNNs to arbitrary graph structures through spectral analogy.</p>
<h2 id="higherorderlatentfeatures">Higher-order latent features</h2>
<p>We may construct a graph <em>summary</em> in order to simply machine learning over graphs. Given a graph $\mathbf{G} = (\mathbf{V},\mathbf{E})$, we would like to identify a latent $d$-dimensional feature vector for each vertex, which encodes its neighborhood in $d$-dimensional space. Generically, the neighborhood of a vertex $v \in \mathbf{V}$ is any subset $N(v) \subset \mathbf{V}$. However, meaningful definitions of neighborhoods typically aim to capture a notion of locality, such as the 1-hop or 2-hop neighborhood around a vertex. Recently, random walks have been preferred as a way to generate neighborhoods without assuming too much about the structure of a network. Random walks are a more flexible definition of neighborhoods that provide stronger weighting to more closely related nodes. Since the length and character of random walks can be tuned, they may also improve training times and decrease the complexity of models seeking to leverage graph structure.</p>
<p>We would like to learn a function $\Phi:\mathbf{V}\rightarrow{\mathbb R}^d$ such that $\Phi(u)$ and $\Phi(v)$ map to similar vectors in ${\mathbb R}^d$ if $u$ and $v$ have similar neighborhoods in $\mathbf{G}$. The image of $\mathbf{G}$ under $\Phi$ may be represented as a small matrix $\mathbf{F} \in {\mathbb R}^{\abs{V} \times d}$ of latent features. After performing this embedding of $\mathbf{G}$ into $(\abs{V}\times d)$-dimensional space, we may augment the features $\mathbf{X}$ associated with the vertices $\mathbf{V}$ with the structural summary $\mathbf{F}$. Since all available data has been transformed into tractable vector form, we may  solve our machine learning problem using any traditional method such as neural networks.</p>
<h1 id="learninglatentfeatures">Learning latent features</h1>
<p>Let us consider $\mathbf{G} = (\mathbf{V},\mathbf{E})$ with node-wise features $\mathbf{X}$. Our goal is to determine for each $v^i \in \mathbf{V}$ some $d$-dimensional vector $\mathbf{f}^i$ such that $\mathbf{f}^i$ expresses the neighborhood of $v^i$.</p>
<p>The <strong>DeepWalk</strong> algorithm approaches this problem through optimization techniques originally developed for natural language modeling <sup>1</sup>. Assuming that the degree distribution of $\mathbf{G}$ follows a power-law distribution (which is true of many real-world social graphs), short random (first-order) walks over $\mathbf{G}$ rooted at $v^i$ obey Zipf's law (also see <sup>2</sup> for more details on power law distributions in social graphs) and thereby may be viewed as "sentences" over a special language. Generalizing the language modeling task of maximizing the probability of words given their contexts, the DeepWalk algorithm seeks to learn a latent features that estimate the likelihood that vertex $u_i$ is observed in a random walk given its context $u_1,\ldots,u_{i - 1}$:</p>
<p>\begin{equation}<br>
\maximize_{\mathbf{F} \in {\mathbb R}^{n\times d}} \quad \Pr{u_i~|~(\mathbf{f}^{u_1},\ldots,\mathbf{f}^{u_{i-1}})}<br>
\end{equation}</p>
<p>This objective is untractable as the length of the random walks grows, so the authors adopted a certain language modeling relaxation that re-frames the problem in terms of predicting the (unordered) context of a vertex $u_i$ in a random walk given its latent representation $\mathbf{f}^{u_i}$. Phrased as an optimization problem:</p>
<p>\begin{equation}<br>
\minimize_{\mathbf{F}\in {\mathbb R}^{n\times d}}\quad - \log \Pr{{u_{i - w},\ldots,u_{i-1},u_{i+1},\ldots,u_{i + w}}~|~\mathbf{f}^{u_i}}<br>
\label{eqobj}<br>
\end{equation}</p>
<p>with sliding window length $w &gt; 0$, this objective captures an unordered sense of locality and may be trained incrementally.</p>
<p>But how, exactly, would one go about minimizing this objective over the random walk space of $\mathbf{G}$? The most obvious way to update $\mathbf{F}$ is to iteratively sample $v \in \mathbf{V}$, conduct a (truncated) random walk $\mathcal{W} = v_1,\ldots,v_t$ rooted at $v$, and consider $u_{k}$ for $k \in [j-w,j+w]$ for each $v_j \in \mathcal{W}$. We may compute $J(\mathbf{F}) = -\log\Pr{u_k~|~F_{v_j}}$ and then update $\mathbf{F} = \mathbf{F} - \alpha \frac{\partial J}{\partial F}$, which is a single step of gradient descend. Yet, this update procedure masks the complexity of the final gradient descend step $\mathbf{F} = \mathbf{F} - \alpha \frac{\partial J}{\partial F}$, which may require small updates to $\abs{\mathbf{V}} \times d$ parameters. In the paper <sup>3</sup>, the authors give a two-part algorithm, <strong>DeepWalk</strong> and <strong>SkipGram</strong> (also see <sup>4</sup>), that provides a training approach to minimize the objective by considering only a small subset of the negative examples. The authors construct a hierarchical binary classifier that reduces the number of parameters that need to be updated in a single gradient descent step from $O(\abs{\mathbf{V}}\times d)$ to $O(\log\abs{\mathbf{V}} \times d)$.</p>
<p>Grover and Leskovec <sup>5</sup> suggest a more flexible second-order random walk approach to neighborhood generation with similar NLP motivations to <strong>DeepWalk</strong>. Their algorithm, <strong>node2vec</strong>, generalizes across a variety of sampling strategies with two parameters $ p,q \in {\mathbb R}$ that influence the "locality" and "breadth-first" character of the walk, respectively. The authors explain that machine learning tasks over nodes in a graph typically seek to leverage two types of structural features, which they refer to as the "homophily hypothesis" and the "structural hypothesis" of neighborhoods. Under the homophily hypothesis, highly connected nodes are part of the same neighborhood since they are proximate to each other in the underlying graph. Under the structural hypothesis, nodes that serve similar structural functions (for example, nodes that act as a hub) are part of the same neighborhood due to their higher-order structural significance. The two tuning parameters provide a smooth way to interpolate between these two notions of neighborhood.</p>
<p>The <strong>node2vec</strong> paper makes a conditional independence assumption that states the likelihood of seeing any node in the random walk is independent of the other nodes that appear in the random walk. This assumption is not precise, but simplifies our objective from earlier by expressing</p>
<p>\begin{equation}<br>
\Pr{u_1,\ldots,u_t~|~\mathbf{f}^{j}} = \prod_{i=1}^t \Pr{u_i~|~\mathbf{f}^{j}}<br>
\end{equation}</p>
<p>The paper additionally imposes a symmetry constrain in the feature space under which the feature vector of a node and a node in its neighborhood have symmetric effect on each other. This property is reflective of both the "homophily hypothesis" and the "structural hypothesis", which are inherently symmetric interpretations of neighborhoods in graphs. One popular way to capture symmetric similarity in low-dimensional embeddings is through a softmax unit parameterized by the dot-product of the feature vectors. This symmetry assumptions yields an explicit formulation of the <strong>node2vec</strong> objective function:</p>
<p>\begin{equation}<br>
\maximize_{\mathbf{F}\in {\mathbb R}^{n\times d}} \sum_{v^i \in \mathbf{V}}<br>
\left( -\log \Gamma_i + \sum_{v^j \in N(v^i)} \mathbf{f}^{i}\cdot \mathbf{f}^{j}\right)<br>
\end{equation}</p>
<p>where $\Gamma_i = \sum_{v^j \in \mathbf{V}} \exp(\mathbf{f}^i\cdot \mathbf{f}^j)$ is the per-node partition function (which is a byproduct of the softmax normalization) and $N(u)$ is the neighborhood of $u \in \mathbb{V}$ generated by $(p,q)$-random-walk sampling strategy. Just as with <strong>DeepWalk</strong>, the <strong>node2vec</strong> objective may be optimized by sampling $v_i \in \mathbf{V}$, taking a $(p,q)$-random-walk $v_1,\ldots,v_t$ rooted at $v_i$, and then updating $\mathbf{F}$ through gradient descent with respect to each $(\mathbf{f}^i, \mathbf{f}^j)$ pair. While the second term of the <strong>node2vec</strong> objective is only dependent on $(\mathbf{f}^{i},\mathbf{f}^j)$ for $v_j \in N(v)$, the partition $\Gamma_i$ is a function of $\mathbf{f}^j$ for all $v_j \in \mathbf{V}$. For social graphs with hundreds of millions or billions of nodes, it is computationally intractable to update $O(\abs{V}\times d)$ parameters for every random walk that we sample. Using negative sampling, we may approximate $\sum_i \log \Gamma_i$ without incurring computational cost quadratic in network size (see <sup>6</sup> for additional details).</p>
<h2 id="latentfeaturesovertemporalnetworks">Latent features over temporal networks</h2>
<p>Neither <strong>DeepWalk</strong> nor <strong>node2vec</strong> are designed to immediately provide latent feature representations of temporal networks that generalize over time. While one may compute a separate embedding at each time step, there are two immediate downsides to this process: firstly, it may be extremely expensive to run <strong>DeepWalk</strong> or <strong>node2vec</strong> on many snapshots of the network and, secondly, there is no formal guarantee that multiple applications of <strong>DeepWalk</strong> or <strong>node2vec</strong> produce similar latent feature spaces.</p>
<p>We may attempt to resolve the first problem by reusing as much structural information as possible between successive applications of our summarization algorithm. Instead of reapplying <strong>DeepWalk</strong> or <strong>node2vec</strong> after each modification of $\mathbf{G}$, we may wait until enough edges have been modified to significantly compromise the quality of the original embedding. Exactly how many edges constitute a "significant" enough change in the structure of $\mathbf{G}$ depends highly on the role of the modified edges in $\mathbf{G}$ and the $(p,q)$-character of the random walk. For example, consider the following pairs of graphs:</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p>Neighborhoods are similar in $K_6 - e$ and $K_6$, but very different in $P_6$ and $C_6$.</p>
<p>In this figure, the neighborhoods of nodes in $K_6 - e$, the complete graph on six nodes with an edge removed, induced by a first-order random walk, are very similar to the neighborhoods of $K_6$. However, the marginal edge that transforms the path graph, $P_6$, into a cycle, $C_6$, significantly alters the first-order random walk neighborhoods of many nodes in graph. In general, edges that acts as "bridges" between otherwise disconnected or weakly connected parts of a network are liable to have greater impact on node neighborhoods.</p>
<p>Luckily, many real world graphs, such as social networks, contain a large, strongly connected core with many properties similar to complete graphs. In the case of first-order random walks (such as those used in <strong>DeepWalk</strong>), the marginal addition or deletion of an edge in the expander-like core a network has very little impact on node embeddings. Additionally, for social graphs, many edge additions are simply ``triangle closing" events, which have very little impact on the neighborhoods of nodes outside that immediate locality.</p>
<p></p>
<p>The above discussion of properties of first-order random walks and their relationship to graph connectivity does not necessarily extend to the second-order random walks used by the <strong>node2vec</strong> algorithm. Many of the guarantees pertaining to first-order random walks on $\mathbf{G}$ and the connectivity of $\mathbf{G}$ are implied by spectral properties of the graph Laplacian matrix of $\mathbf{G}$. However, higher-order random walks may not respect as strong of a relationship between the neighborhood induced by the random walk and local connectivity. For example, <strong>node2vec</strong> parameterized by $p \gg 1$ and $q \ll 1$, which corresponds to a random walk biased to stray away from its origin, does not generate neighborhoods that strongly depend on the <em>local</em> connectivity of the root node. These DFS-like walks may relate more strongly to <em>global</em> measures of connectivity as all random-walks have a tendency to be funneled through global bottleneck edges. To the best of my knowledge, there is no literature that studies the relationship between the neighborhoods induced by various second-order random walks and more primitive graph properties.</p>
<p>I believe that some combination of dynamic graph partitioning and iterative updates to the graph summary matrix $\mathbf{F}$ may yield a computationally tractable approach to temporal graph latent feature extraction. In general, further unification of Knowledge Discovery and Data Mining (KDD) literature with the state-of-the-art combinatorial and spectral algorithms may provide greater insight into graph representations. For temporal graphs, tools such as the dynamic spectral sparsifier given by <em>Abraham et al.</em> (2016) may inspire one approach towards incremental updates of graph embeddings of time-varying networks <sup>7</sup>. However, the current asymptotically-best dynamic spectral sparsifiers are impractically slow on the scale of real-world graphs (although sub-logarithmic algorithms exist, the constant factor of these algorithms may be enormous). Further work relating spectral properties of graphs to latent features may be valuable for data mining over temporal graphs such as social networks.</p>
<p>While unexplored in the literature, <strong>node2vec</strong> could potentially be generalized to temporal networks by introducing an additional parameter $\lambda$ that influences the likelihood of random walks to traverse temporal boundaries. Just as the random walk parameters $p$ and $q$ influence the locality and breadth-first character of the walk, $\lambda$ may express the temporal affinity. Some prediction tasks may assume a "temporal locality" in which frames of $\mathbf{G}$ at similar timestamps are strongly related while other problems may have longer range temporal relationships with weaker locality. Although GCNs, which shall be described in the subsequence section, provide a way to capture spatio-temporal structural features, a generalization of second-order $(p,q)$-random-walks to temporal networks may provide another way to approach machine learning over temporal graphs.</p>
<h1 id="graphconvolutionalnetworks">Graph convolutional networks</h1>
<p>While the latent feature approaches outlined in the last section provide a context-free schema to summarize arbitrary graphs into lower-dimensional space (possibly with a few tunable parameters), these approaches cannot capture all possible interactions between structural features and per-node features. Since the graph summary is constructed independent of the problem to which it is applied, per-node attributes that impact the role of nodes in the graph are never reflected in the structural features captured by the summary. For example, a multimodal attributed network of users and pages in which edges correspond to friendships (user-user), hyperlinks (page-page), and likes (user-page) could benefit from an end-to-end learning approach since the structural features relevant to users and pages are not necessarily the same. Although we know whether a node corresponds to a user or a page a priori, graph summary models such as <strong>node2vec</strong> treat all nodes as equivalents, which may omit useful structure from some summary vectors. Perhaps we can do better by fitting structural features at the same time as the parameters of the model to produce a more "fine-tuned" notion of structure per node. An end-to-end approach to machine learning tasks over graphs may be able to discover more context dependent features than <strong>node2vec</strong> or <strong>DeepWalk</strong>.</p>
<p>Convolutional layers have replaced a great deal visual and textual feature engineering with fully trainable end-to-end CNN models. Instead of developing features by hand, one may fit the parameters of a CNN to identify local structures that are highly dependent on the particular learning problem. Convolutional layers in a traditional CNN compute:</p>
<p>$\mathbf{y}_i = \mathbf{b} + {\sum_{m = 1}^M {\mathbf{W}_m \mathbf{x}_{j(m,i)}}}$</p>
<p>for weight matrices $\mathbf{W}_m\in {\mathbb R}^{E\times D}$, inputs $\mathbf{x}_j \in {\mathbb R}^{D}$, activations $\mathbf{y}_i \in {\mathbb R}^E$, and index map $j:M\times M\rightarrow M$ that sends $(m,i)$ to the index of the $m$th neighbor of the $i$th input. Framing our equation in terms of image data for an $h\times w$ pixel image, let $M = h\times w$, $D$ equal the number of input channels, $E$ equal the number of output channels, and $j$ map neighborhoods in a "spiral-like" fashion (see <sup>8</sup> for helpful figures). Often, the matrices $\mathbf{W}_m = \mathbf{0}$ for $m &gt; M'$ where $M'$ is some small constant, which significantly improves training time in practice and helps to reduce over-fitting.</p>
<p>To generalize the equation to graphs, <em>Verma et al.</em> propose a soft-assignment function $q_m : {\mathbb R}^D\times {\mathbb R}^D\rightarrow {\mathbb R}$ and modified equation:</p>
<p>\begin{equation}<br>
y_i = \mathbf{b} + \sum_{m = 1}^M \frac{1}{|\mathcal{N}_i|} \sum_{j \in \mathcal{N}_i} q_m(\mathbf{x}_i,\mathbf{x}_j)\mathbf{W}_m \mathbf{x}_{j}<br>
\end{equation}</p>
<p>where $\mathcal{N}_i$ is the neighborhood of the $i$th datum. Soft-assignment makes sense in the context of graphs since node neighborhoods $\mathcal{N}_i$ are unordered. This equation reduces to the last one when $\mathcal{N}_i = {1,\ldots,M}$, $q_m:{\mathbb R}^D\times {\mathbb R}^D\rightarrow {0,1}$ and $q_m(\mathbf{x}_i,\mathbf{x}_j) = 1$ iff $j(m,i) = j$. The authors suggest that</p>
<p>\begin{equation}<br>
q_m(\mathbf{x}_i,\mathbf{x}_j) \propto \exp\left(\mathbf{u}^T_m\mathbf{x}_i + \mathbf{v}^T_m \mathbf{x}_j + c_m\right)<br>
\label{asym}<br>
\end{equation}</p>
<p>where $\mathbf{u}_m$, $\mathbf{v}_m$, and $c_m$ can either be learned along with $\mathbf{W}_m$ or selected a priori. This equation may be seen as a relaxation of the "neighborhood symmetry" assumption made by Grover and Leskovec that permits asymmetric relationships smoothed by a soft-max unit. This relaxation is flexible enough to capture asymmetric "similarity-from" and "similarity-to" relationships with soft application that is particularly important for learning over multimodal or directed networks.</p>
<p>Various other authors have suggested alternatives to the GCN described above, including spectral filters that leverage the relationship between graph Laplacians and community structure in graphs. For example, Defferrard <em>et al.</em> (2016) provides a local spectral filtering approach that considers only interactions within an order $k$-ball around each node <sup>9</sup>. These local spectral filters are promising for two reasons: firstly, they provide an alternative generalization of CNNs to graphs by considering strictly low-dimensional filters in the spectral domain of $\mathbf{G}$, and, secondly, for many of the reasons detailed earlier, spectral-based GCNs may be able to capture stronger community-based properties of $\mathbf{G}$ than  spatial GCNs.</p>
<h1 id="conclusion">Conclusion</h1>
<p>Machine learning over graphs is a thriving topic of academic interest. So far, most applications of machine learning over graphs have concerned social networks, product recommendation, or proof-of-concept. Yet, relational structure is abundant everywhere. Imagine leveraging the relational structure defined over objects by WordNet to improve the performance of object detection in computer vision. Imagine social robots deriving interpersonal relationships from the spatial relationships between the bodies of human subjects. And, imagine bioinformatic algorithms that directly harness molecular structure to reduce the complexity of lattice based approaches to protein-folding simulation. While I cannot promise that any of the approaches described in this paper will necessarily improve on the current state-of-the-art in these applications, I think that using all available structure may be a productive front for future research.</p>
</div>

            </section>

        </article>
    </main>
    <footer class="site-footer clearfix">
        <section class="copyright"><a href="../../">The Gradient</a> © 2018</section>
        <section class="poweredby">Proudly published with <a href="https://ghost.org">Ghost</a></section>
    </footer>
</body>
