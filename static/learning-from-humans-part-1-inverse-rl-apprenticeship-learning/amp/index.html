
<head>
    <meta charset="utf-8">

    <title>Learning From Humans: Inverse RL &amp; Apprenticeship Learning</title>

    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">

    <link rel="shortcut icon" href="../../favicon.png">

    <link rel="shortcut icon" href="../../favicon.png" type="image/png">
    <link rel="canonical" href="../index.html">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <meta property="og:site_name" content="The Gradient">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Learning From Humans: Inverse RL &amp; Apprenticeship Learning">
    <meta property="og:description" content="One of the goals of AI research is to teach machines how to do the same things people do, but better. In the early 2000s, this meant focusing on problems like flying helicopters and walking up flights of stairs. Now, these problems have been solved, and we’ve achieved near-human">
    <meta property="og:url" content="http://localhost:2368/learning-from-humans-part-1-inverse-rl-apprenticeship-learning/">
    <meta property="og:image" content="http://localhost:2368/content/images/2018/03/DSC_0709.jpg">
    <meta property="article:published_time" content="2018-03-17T01:14:54.000Z">
    <meta property="article:modified_time" content="2018-04-06T04:19:51.000Z">
    <meta property="article:tag" content="Overviews">
    
    <meta property="article:publisher" content="https://www.facebook.com/gradientml/">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Learning From Humans: Inverse RL &amp; Apprenticeship Learning">
    <meta name="twitter:description" content="One of the goals of AI research is to teach machines how to do the same things people do, but better. In the early 2000s, this meant focusing on problems like flying helicopters and walking up flights of stairs. Now, these problems have been solved, and we’ve achieved near-human">
    <meta name="twitter:url" content="http://localhost:2368/learning-from-humans-part-1-inverse-rl-apprenticeship-learning/">
    <meta name="twitter:image" content="http://localhost:2368/content/images/2018/03/DSC_0709.jpg">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Gary Gradient">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="Overviews">
    <meta name="twitter:site" content="@gradientml">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "The Gradient",
        "logo": "http://localhost:2368/content/images/2018/04/logo3.png"
    },
    "author": {
        "@type": "Person",
        "name": "Gary Gradient",
        "url": "http://localhost:2368/author/gary/",
        "sameAs": []
    },
    "headline": "Learning From Humans: Inverse RL &amp; Apprenticeship Learning",
    "url": "http://localhost:2368/learning-from-humans-part-1-inverse-rl-apprenticeship-learning/",
    "datePublished": "2018-03-17T01:14:54.000Z",
    "dateModified": "2018-04-06T04:19:51.000Z",
    "image": "http://localhost:2368/content/images/2018/03/DSC_0709.jpg",
    "keywords": "Overviews",
    "description": "One of the goals of AI research is to teach machines how to do the same things people do, but better. In the early 2000s, this meant focusing on problems like flying helicopters and walking up flights of stairs. Now, these problems have been solved, and we’ve achieved near-human",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:2368/"
    }
}
    </script>

    <meta name="generator" content="Ghost 1.22">
    <link rel="alternate" type="application/rss+xml" title="The Gradient" href="../../rss/index.html">

    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic%7COpen+Sans:700,600,400">
    <style amp-custom>html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}article,aside,details,figcaption,figure,footer,header,main,menu,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block;vertical-align:baseline}audio:not([controls]){display:none;height:0}[hidden],template{display:none}a{background-color:transparent}a:active,a:hover{outline:0}abbr[title]{border-bottom:1px dotted}b,strong{font-weight:bold}dfn{font-style:italic}h1{margin:0.67em 0;font-size:2em}mark{background:#ff0;color:#000}small{font-size:80%}sub,sup{position:relative;vertical-align:baseline;font-size:75%;line-height:0}sup{top:-0.5em}sub{bottom:-0.25em}img{border:0}amp-img{border:0}svg:not(:root){overflow:hidden}figure{margin:1em 40px}hr{box-sizing:content-box;height:0}pre{overflow:auto}code,kbd,pre,samp{font-family:monospace, monospace;font-size:1em}button,input,optgroup,select,textarea{margin:0;color:inherit;font:inherit}button{overflow:visible}button,select{text-transform:none}button,html input[type="button"],input[type="reset"],input[type="submit"]{cursor:pointer;-webkit-appearance:button}button[disabled],html input[disabled]{cursor:default}button::-moz-focus-inner,input::-moz-focus-inner{padding:0;border:0}input{line-height:normal}input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}input[type="number"]::-webkit-inner-spin-button,input[type="number"]::-webkit-outer-spin-button{height:auto}input[type="search"]{-webkit-appearance:textfield}input[type="search"]::-webkit-search-cancel-button,input[type="search"]::-webkit-search-decoration{-webkit-appearance:none}fieldset{margin:0 2px;padding:0.35em 0.625em 0.75em;border:1px solid #c0c0c0}legend{padding:0;border:0}textarea{overflow:auto}optgroup{font-weight:bold}table{border-spacing:0;border-collapse:collapse}td,th{padding:0}html{max-height:100%;height:100%;font-size:62.5%;-webkit-tap-highlight-color:rgba(0, 0, 0, 0)}body{max-height:100%;height:100%;color:#3a4145;background:#f4f8fb;letter-spacing:0.01rem;font-family:"Merriweather", serif;font-size:1.8rem;line-height:1.75em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"kern" 1;-moz-font-feature-settings:"kern" 1;-o-font-feature-settings:"kern" 1}::-moz-selection{background:#d6edff}::selection{background:#d6edff}h1,h2,h3,h4,h5,h6{margin:0 0 0.3em 0;color:#2e2e2e;font-family:"Open Sans", sans-serif;line-height:1.15em;text-rendering:geometricPrecision;-webkit-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-moz-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1;-o-font-feature-settings:"dlig" 1, "liga" 1, "lnum" 1, "kern" 1}h1{text-indent:-2px;letter-spacing:-1px;font-size:2.6rem}h2{letter-spacing:0;font-size:2.4rem}h3{letter-spacing:-0.6px;font-size:2.1rem}h4{font-size:1.9rem}h5{font-size:1.8rem}h6{font-size:1.8rem}a{color:#4a4a4a}a:hover{color:#111}p,ul,ol,dl{margin:0 0 2.5rem 0;font-size:1.5rem;text-rendering:geometricPrecision;-webkit-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-moz-font-feature-settings:"liga" 1, "onum" 1, "kern" 1;-o-font-feature-settings:"liga" 1, "onum" 1, "kern" 1}ol,ul{padding-left:2em}ol ol,ul ul,ul ol,ol ul{margin:0 0 0.4em 0;padding-left:2em}dl dt{float:left;clear:left;overflow:hidden;margin-bottom:1em;width:180px;text-align:right;text-overflow:ellipsis;white-space:nowrap;font-weight:700}dl dd{margin-bottom:1em;margin-left:200px}li{margin:0.4em 0}li li{margin:0}hr{display:block;margin:1.75em 0;padding:0;height:1px;border:0;border-top:#efefef 1px solid}blockquote{box-sizing:border-box;margin:1.75em 0 1.75em 0;padding:0 0 0 1.75em;border-left:#4a4a4a 0.4em solid;-moz-box-sizing:border-box}blockquote p{margin:0.8em 0;font-style:italic}blockquote small{display:inline-block;margin:0.8em 0 0.8em 1.5em;color:#ccc;font-size:0.9em}blockquote small:before{content:"\2014 \00A0"}blockquote cite{font-weight:700}blockquote cite a{font-weight:normal}mark{background-color:#fdffb6}code,tt{padding:1px 3px;border:#e3edf3 1px solid;background:#f7fafb;border-radius:2px;white-space:pre-wrap;font-family:Inconsolata, monospace, sans-serif;font-size:0.85em;font-feature-settings:"liga" 0;-webkit-font-feature-settings:"liga" 0;-moz-font-feature-settings:"liga" 0}pre{overflow:auto;box-sizing:border-box;margin:0 0 1.75em 0;padding:10px;width:100%;border:#e3edf3 1px solid;background:#f7fafb;border-radius:3px;white-space:pre;font-family:Inconsolata, monospace, sans-serif;font-size:0.9em;-moz-box-sizing:border-box}pre code,pre tt{padding:0;border:none;background:transparent;white-space:pre-wrap;font-size:inherit}kbd{display:inline-block;margin-bottom:0.4em;padding:1px 8px;border:#ccc 1px solid;background:#f4f4f4;border-radius:4px;box-shadow:0 1px 0 rgba(0, 0, 0, 0.2), 0 1px 0 0 #fff inset;color:#666;text-shadow:#fff 0 1px 0;font-size:0.9em;font-weight:700}table{box-sizing:border-box;margin:1.75em 0;max-width:100%;width:100%;background-color:transparent;-moz-box-sizing:border-box}table th,table td{padding:8px;border-top:#efefef 1px solid;vertical-align:top;text-align:left;line-height:20px}table th{color:#000}table caption + thead tr:first-child th,table caption + thead tr:first-child td,table colgroup + thead tr:first-child th,table colgroup + thead tr:first-child td,table thead:first-child tr:first-child th,table thead:first-child tr:first-child td{border-top:0}table tbody + tbody{border-top:#efefef 2px solid}table table table{background-color:#fff}table tbody > tr:nth-child(odd) > td,table tbody > tr:nth-child(odd) > th{background-color:#f6f6f6}table.plain tbody > tr:nth-child(odd) > td,table.plain tbody > tr:nth-child(odd) > th{background:transparent}iframe,amp-iframe,.fluid-width-video-wrapper{display:block;margin:1.75em 0}.fluid-width-video-wrapper iframe,.fluid-width-video-wrapper amp-iframe{margin:0}textarea,select,input{margin:0 0 5px 0;padding:6px 9px;width:260px;outline:0;border:#e7eef2 1px solid;background:#fff;border-radius:4px;box-shadow:none;font-family:"Open Sans", sans-serif;font-size:1.6rem;line-height:1.4em;font-weight:100;-webkit-appearance:none}textarea{min-width:250px;min-height:80px;max-width:340px;width:100%;height:auto}input[type="text"]:focus,input[type="email"]:focus,input[type="search"]:focus,input[type="tel"]:focus,input[type="url"]:focus,input[type="password"]:focus,input[type="number"]:focus,input[type="date"]:focus,input[type="month"]:focus,input[type="week"]:focus,input[type="time"]:focus,input[type="datetime"]:focus,input[type="datetime-local"]:focus,textarea:focus{outline:none;outline-width:0;border:#bbc7cc 1px solid;background:#fff}select{width:270px;height:30px;line-height:30px}.clearfix:before,.clearfix:after{content:" ";display:table}.clearfix:after{clear:both}.clearfix{zoom:1}.main-header{position:relative;display:table;overflow:hidden;box-sizing:border-box;width:100%;height:50px;background:#5ba4e5 no-repeat center center;background-size:cover;text-align:left;-webkit-box-sizing:border-box;-moz-box-sizing:border-box}.content{background:#fff;padding-top:15px}.blog-title,.content{margin:auto;max-width:600px}.blog-title a{display:block;padding-right:16px;padding-left:16px;height:50px;color:#fff;text-decoration:none;font-family:"Open Sans", sans-serif;font-size:16px;line-height:50px;font-weight:600}.post{position:relative;margin-top:0;margin-right:16px;margin-left:16px;padding-bottom:0;max-width:100%;border-bottom:#ebf2f6 1px solid;word-wrap:break-word;font-size:0.95em;line-height:1.65em}.post-header{margin-bottom:1rem}.post-title{margin-bottom:0}.post-title a{text-decoration:none}.post-meta{display:block;margin:3px 0 0 0;color:#9eabb3;font-family:"Open Sans", sans-serif;font-size:1.3rem;line-height:2.2rem}.post-meta a{color:#9eabb3;text-decoration:none}.post-meta a:hover{text-decoration:underline}.post-meta .author{margin:0;font-size:1.3rem;line-height:1.3em}.post-date{display:inline-block;text-transform:uppercase;white-space:nowrap;font-size:1.2rem;line-height:1.2em}.post-image{margin:0;padding-top:3rem;padding-bottom:30px;border-top:1px #E8E8E8 solid}.post-content amp-img,.post-content amp-anim{position:relative;left:50%;display:block;padding:0;min-width:0;max-width:112%;width:calc(100% + 32px);height:auto;transform:translateX(-50%);-webkit-transform:translateX(-50%);-ms-transform:translateX(-50%)}.footnotes{font-size:1.3rem;line-height:1.6em;font-style:italic}.footnotes li{margin:0.6rem 0}.footnotes p{margin:0}.footnotes p a:last-child{text-decoration:none}.site-footer{position:relative;margin:0 auto 20px auto;padding:1rem 15px;max-width:600px;color:rgba(0,0,0,0.5);font-family:"Open Sans", sans-serif;font-size:1.1rem;line-height:1.75em}.site-footer a{color:rgba(0,0,0,0.5);text-decoration:none;font-weight:bold}.site-footer a:hover{border-bottom:#bbc7cc 1px solid}.poweredby{display:block;float:right;width:45%;text-align:right}.copyright{display:block;float:left;width:45%}</style>

    <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
    <script async src="https://cdn.ampproject.org/v0.js"></script>

    

</head>

<body class="amp-template">
    <header class="main-header">
        <nav class="blog-title">
            <a href="../../">The Gradient</a>
        </nav>
    </header>

    <main class="content" role="main">
        <article class="post">

            <header class="post-header">
                <h1 class="post-title">Learning From Humans: Inverse RL &amp; Apprenticeship Learning</h1>
                <section class="post-meta">
                    <p class="author">by <a href="../../author/gary/">Gary Gradient</a></p>
                    <time class="post-date" datetime="2018-03-16">2018-03-16</time>
                </section>
            </header>
            <figure class="post-image">
                <amp-img src="http://localhost:2368/content/images/2018/03/DSC_0709.jpg" width="600" height="400" layout="responsive"></amp-img>
            </figure>
            <section class="post-content">

                <div class="kg-card-markdown"><p>One of the goals of AI research is to teach machines how to do the same things people do, but better.</p>
<p>In the early 2000s, this meant focusing on problems like <a href="http://www.youtube.com/watch?v=M-QUkgk3HyE">flying helicopters</a> and <a href="http://www.youtube.com/watch?v=aqCmX5dMYHg">walking up flights of stairs</a>. Now, these problems have been solved, and we’ve achieved near-human performance on harder problems like recognizing images and playing simple Atari games.</p>
<p>However, there’s still a massive list of problems where humans outperform machines. Our advantage lies in solving problems that aren’t as well defined, like judging a well-executed backflip, cleaning a room while minimizing side effects, and perhaps the most human problem of them all, “reasoning about people’s values.” To solve these problems, machines need information about the world as well as a way to learn about the people within it.</p>
<p><strong>How can machines learn from humans?</strong> We’ll be splitting this big question into three sub-questions:</p>
<ol>
<li>How can an agent learn a policy that performs as well as an expert policy?</li>
<li>How can we scale the rate at which agents are able to learn from people?</li>
<li>How do we deal with the obvious fact that people do not behave optimally?</li>
</ol>
<p>To answer our first question, we’ll start this series off with an overview of some of the foundational work on inverse reinforcement learning, apprenticeship learning, and imitation learning. The unifying thread between these techniques is that they all provide a way of integrating information from human experts into reinforcement learning methods, achieving impressive performance in driving simulators and autonomous helicopter flight tests, and gesturing at some of the key problems that later work will attempt to solve.</p>
<h1 id="inversereinforcementlearning">Inverse Reinforcement Learning</h1>
<br>
<h2 id="problem">Problem</h2>
<p>In a traditional RL setting, the goal is to learn a decision process which maximizes some predefined reward function. Inverse reinforcement learning (IRL), as described by Andrew Ng and Stuart Russell in 2000, flips the problem and instead attempts to extract the reward function from the observed behavior of an agent.</p>
<p>Consider the task of autonomous driving. A naive approach would be to create a reward function that captures the desired behavior of a driver, like stopping at red lights, staying off the sidewalk, avoiding pedestrians, etc. Unfortunately, this would require both an exhaustive list of every feature we’d want to consider as well as a list of weights describing how important each feature is (imagine having to decide exactly how much more important pedestrians are than stop signs).</p>
<p>Instead, IRL takes a set of human-generated driving data and extracts an approximation of the reward function that the person is operating under. In doing this, all of the information necessary for solving a problem is essentially encoded within the reward function. As Ng and Russell put it, <strong>“the reward function, rather than the policy, is the most succinct, robust, and transferable definition of the task,”</strong> since the problem of finding the right policy is already solvable with traditional RL methods.</p>
<p>For our self-driving car example, we’d be using human driving data to directly learn what the right feature weights are for the reward. Since the task is described by the reward function, we do not even need to know what the specifics of the human policy are, so long as we have the right reward function to optimize for. In the general case, IRL can be seen as a method for leveraging expert knowledge to convert a task description into a compact reward function.</p>
<p>The main problem when converting a complex task into a simple reward function is that a given policy may be optimal for many different reward functions. That is, even though we have the actions from an expert, there exist many different reward functions (some of them degenerate) that the expert might be attempting to maximize. For example, all policies are optimal for the reward function that is zero everywhere, so this reward function is always a possible solution to the IRL problem. Ideally, we would want a reward function that captures meaningful information about the task and is able to clearly differentiate between desired and undesired policies.</p>
<p>To solve this, Ng and Russell formulate inverse reinforcement learning as an optimization problem where we seek to maximize certain properties of the reward function, under the constraint that the given expert policy is still the optimal solution for any reward function we consider.</p>
<br>
<h2 id="algorithms">Algorithms</h2>
<br>
<h3 id="inversereinforcementlearninginfinitestatespaces">Inverse Reinforcement Learning in Finite State Spaces</h3>
<p>First, consider the set of optimal policies for a Markov Decision Process (MDP) with a finite state space $S$, set of actions $A$, and transition probability matrices $P_a$ for each action. We say that a policy $\pi$ given by $\pi(s) = a_1$ is optimal if and only if for all other actions $a$, the reward vector $R$ (which lists the rewards for each possible state) satisfies:</p>
<p>$ (P_{a1}−P_a)(I − \gamma P_{a_1})^{−1} R \succeq 0$</p>
<p>The proof from Ng’s paper is given in the Appendix, but the importance of the theorem is that <strong>it allows us to use linear programming to find a feasible point of the constraints in this equation</strong>. Ng and Russell then formulate IRL as an optimization problem using the following heuristics for what makes a reward function “fit” the expert data well:</p>
<p>Maximize “the difference between the quality of the optimal action and the quality of the next best action” (subject to a bound on the magnitude of the reward, to prevent arbitrarily large differences). The intuition here is that we want a reward function that clearly distinguishes the optimal policy from other possible policies.<br>
Minimize the size of the rewards in the reward function/vector. The intuition is that using small rewards corresponds to some kind of “simplicity” prior. Ng and Russell choose the L1 norm with an adjustable penalty coefficient, which comes with the nice property of the reward vector being non-zero in only a few states.</p>
<p>While these constraints are intuitive, the authors note that other useful constraints may exist. In particular, later work, such as Maximum Entropy Inverse Reinforcement Learning (Ziebart et. al.), and Bayesian Inverse Reinforcement Learning (Ramachandran et. al.), are two examples of different criteria that have been used to achieve impressive results.</p>
<br>
<h3 id="inversereinforcementlearningfromsampledtrajectories">Inverse Reinforcement Learning from Sampled Trajectories</h3>
<p>Ng and Russell also describe IRL algorithms for cases where, instead of a full optimal policy, we can only sample trajectories from an optimal policy. In applied cases, especially those dealing with human expert data, this situation is a bit more common.</p>
<p>In this formulation of the problem, we replace the reward vector that we used for finite state spaces with a linear approximation of the reward function which uses basis functions $\phi_i$ from $\mathbb{R}^n \rightarrow \mathbb{R}$ to obtain real-valued features $\phi_i(s)$. These features are a way to capture important information from a high-dimensional state space (for instance, instead of storing the location of a car during every time-step, we can store its average speed as a feature). For each feature $\phi_i(s)$ and weight $\alpha_i$ we have:</p>
<p>$ R(s) = \alpha_1\phi_1(s) + \alpha_2\phi_2(s) + . . . + \alpha_d\phi_d(s) $</p>
<p>and <strong>our objective is to find the best-fitting values for each feature-weight</strong> $\alpha_i$.</p>
<p>The general idea behind IRL with sampled trajectories is to <strong>iteratively improve a reward function by comparing the value of the assumed optimal policy with a set of k generated policies</strong>. We initialize the algorithm by randomly generating weights for the estimated reward function and initialize our set of candidate policies with a randomly generated policy. Then, the key steps are algorithm are to:</p>
<ol>
<li>Estimate the value of our optimal policy for the initial state $\hat{V}^\pi(s_0)$, as well as the value of every generated policy $\hat{V}^{\pi_i}(s_0)$ by taking the average cumulative reward of many randomly sampled trials.</li>
<li>Generate an estimate of the reward function $R$ by solving a linear programming problem. Specifically, set $\alpha_i$ to maximize the difference between our optimal policy and each of the other $k$ generated policies:</li>
</ol>
<p>$ \mathrm{maximize~} \sum_{i=1}^k f(\hat{V}^{\pi^*}(s_0) − \hat{V}^{\pi_i}(s_0))$<br>
$ \mathrm{with~} f(x)=2x \mathrm{~if~} x &lt; 0 \mathrm{~else~} f(x)=x \mathrm{~to~penalize~valuing~a~policy~higher~than~the~expert}$<br>
$ \mathrm{s.t.~} |\alpha_i| \leq 1, ~ i=1, . . ., d $</p>
<ol start="4">
<li>After a large number of iterations, end the algorithm at this step.</li>
<li>Otherwise, use a standard RL algorithm to find the optimal policy for $R$. This policy may be different from the given optimal policy, since our estimated reward function is not necessarily identical to the reward function we are searching for.</li>
<li>Add the newly generated policy to the set of $k$ candidate policies, and repeat the procedure.</li>
</ol>
<br>
<h2 id="results">Results</h2>
<p>When tested on grid-worlds and the mountain car problem, and executed with the correct penalty terms, IRL is able to achieve high levels of similarity with the desired reward function. Remarkably, there was <strong>no statistically significant difference between the reward for the true optimal policy and the reward for the policy generated using IRL’s estimation of the reward function</strong> when testing on the actual MDP.</p>
<p>On the practical side, Pieter Abbeel and Andrew Ng combine inverse reinforcement learning with apprenticeship learning to <a href="http://ai.stanford.edu/~pabbeel//irl/">mimic various driving styles in these self-driving car simulators</a>.</p>
<h1 id="apprenticeshiplearning">Apprenticeship Learning</h1>
<br>
<h2 id="problem">Problem</h2>
<p>One interesting alternative to learning a reward function from an expert is to directly learn a policy that is guaranteed to have comparable performance to the expert policy. This is particularly useful if we have an expert policy that, while not truly optimal, is a satisfactory approximation to optimal behavior. Here, an apprenticeship learning algorithm formulated by Pieter Abbeel and Andrew Ng in 2005, gives us a solution. This algorithm takes an MDP and an approximately optimal “teacher” policy, and then <strong>learns a policy with comparable or better performance to the teacher’s policy using minimal exploration</strong>.</p>
<p>This minimal exploration property turns out to be very useful in fragile tasks like autonomous helicopter flight. A traditional reinforcement learning algorithm might start out exploring at random, which would almost certainly lead to a helicopter crash in the first trial. Ideally, we’d be able to use expert data to start with a baseline policy that can be safely improved over time. We would also expect this baseline policy to be significantly better than a randomly initialized policy, which speeds up convergence.</p>
<br>
<h2 id="algorithm">Algorithm</h2>
<p>The main idea behind Abbeel and Ng’s algorithm is to use trials from the expert policy to obtain information about the underlying MDP, then iteratively execute a best estimate of the optimal policy for the actual MDP. Executing a policy also gives us data about the transitions in the environment, which we can then use to improve the accuracy of the estimated MDP. The key steps of the algorithm for a discrete environment are:</p>
<p>Learning about the MPD with the expert policy:</p>
<ol>
<li>Run a fixed amount of trials using the expert policy, recording every state-action trajectory.</li>
<li>Estimate the transition probabilities for every state-action pair using the recorded data via maximum likelihood estimation.</li>
<li>Estimate the value of the expert policy by averaging the total reward in each trial.</li>
</ol>
<p>Learning a new policy:</p>
<ol start="4">
<li>Learn an optimal policy for the estimated MDP using any standard reinforcement learning algorithm.</li>
<li>Test the learning policy on the actual environment.</li>
<li>If performance is not close enough to the value of the expert policy, add the state-action trajectories from this trial to the training set and repeat the procedure to learn a new policy.</li>
</ol>
<p>The benefit of this method is that <strong>at each stage, the policy being tested is the best estimate for the optimal policy of the system</strong>. There is diminished exploration, but the core idea of apprenticeship learning is that we can assume the expert policy is already near-optimal.</p>
<br>
<h2 id="results">Results</h2>
<br>
<h3 id="theoreticalresults">Theoretical Results</h3>
<p>One of the key results in apprenticeship learning is that <strong>the reward of the learned policy, evaluated in the true MDP, is near-optimal compared to the teacher’s policy</strong>.</p>
<p>The main idea in the proof is that the estimated model remains accurate for evaluating the teacher’s policy, even after data from the estimated policy has been added. Another important point is that when estimating any state-action pair in the dynamics of the system, either:</p>
<ol>
<li>A state-action pair will be visited a “small” number of times, and therefore will not contribute significantly to the overall estimated reward.</li>
<li>A state-action pair will be frequently visited by the learned policy, and of these visits, only a “small” number of these must occur until the state-action pair is accurately modeled.</li>
</ol>
<p>Abbeel and Ng formalize these notions and produce an interesting theoretical proof of the near-optimality of the estimated policy relative to the teacher’s policy, which we will not cover, but that can be seen in the original paper.</p>
<br>
<h3 id="practicalresults">Practical Results</h3>
<p>Coates, Abbeel, and Ng have a particularly amazing application of apprenticeship learning in successfully <a href="https://youtu.be/VCdxqn0fcnE">teaching a helicopter to autonomously fly a complete aerobatic airshow</a>.</p>
<br>
<h1 id="furtherwork">Further Work</h1>
<p>The foundational methods of inverse reinforcement learning and apprenticeship learning, as well as the similar method of imitation learning are able to achieve their results by leveraging information gleaned from a policy executed by a human expert. However, in the long run, the goal is for machine learning systems to <strong>learn from a wide range of human data and perform tasks that are beyond the abilities of human experts</strong>.</p>
<p>In particular, when discussing potential avenues for further research in inverse reinforcement learning, Andrew Ng and Stuart Russell highlight <strong>suboptimality in the agent’s actions</strong> as an important problem. In the context of human experts, it’s clear that while experts may be able to provide a baseline level of performance that RL algorithms could learn from, it will rarely be the case that the expert policy given is actually the optimal one for the environment. The next problem to solve in inverse reinforcement learning would be to extract a reward function without the assumption that the human policy is optimal. In part 3 of this series, when we consider approaches to learning the reward functions of even more complex tasks, we’ll be examining a few methods of factoring uncertainty into the estimation of the true reward function as a way of solving this problem of suboptimality.</p>
<p>Another remarkable extension to inverse reinforcement learning is one that does not require an optimal policy, and instead considers <strong>learning behaviors that agents can identify, but not necessarily demonstrate</strong>, meaning that only a classifier is needed. For instance, it’s easy for people to identify whether an agent in a physics simulator is running correctly, but almost impossible to manually specify the right control sequence given the degrees of freedom in a robotics simulator. This extension would allow reinforcement learning systems to achieve human-approved performance without the need for an expert policy to imitate.</p>
<p>Finally, while the authors also describe the use of linear function approximation to apply IRL to large state spaces, the challenge in going from 2000 to 2018 is to <strong>scale up these methods to work with deep learning</strong>. We’ll be covering a few novel methods of doing just this in the next entry in this series: Deep learning from humans.</p>
<br>
<h1 id="appendix">Appendix</h1>
<p>Proof of Ng’s IRL Theorem:<br>
</p>
<br>
<h1 id="references">References</h1>
<ul>
<li><a href="https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf">Abbeel, Pieter and Ng, Andrew Y. (2004) Apprenticeship learning via inverse reinforcement learning.</a></li>
<li><a href="https://dl.acm.org/citation.cfm?id=1102352">Abbeel, Pieter and Ng, Andrew Y. (2005) Exploration and apprenticeship learning in reinforcement learning.</a></li>
<li><a href="https://arxiv.org/pdf/1606.06565.pdf">Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dan Mané. (2016) Concrete Problems in AI Safety.</a></li>
<li><a href="https://dl.acm.org/citation.cfm?id=1538812">Coates, Adam and Abbeel, Pieter and Ng, Andrew (2009). Apprenticeship learning for helicopter control.</a></li>
<li><a href="http://ai.stanford.edu/~ang/papers/icml00-irl.pdf">Ng, Andrew and Russell, Stuart. (2000) Algorithms for inverse reinforcement learning.</a></li>
<li><a href="https://www.aaai.org/Papers/IJCAI/2007/IJCAI07-416.pdf">Ramachandran, Deepak and Amir, Eyal. (2007) Bayesian inverse reinforcement learning.</a></li>
<li><a href="https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf">Brian D. Ziebart, Andrew Maas, J.Andrew Bagnell, and Anind K. Dey. (2008) Maximum Entropy Inverse Reinforcement Learning.</a></li>
</ul>
</div>

            </section>

        </article>
    </main>
    <footer class="site-footer clearfix">
        <section class="copyright"><a href="../../">The Gradient</a> © 2018</section>
        <section class="poweredby">Proudly published with <a href="https://ghost.org">Ghost</a></section>
    </footer>
</body>
