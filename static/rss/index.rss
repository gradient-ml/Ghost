<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/" version="2.0"><channel><title>The Gradient</title><description>Democratizing AI Through Discussion and Distillation</description><link>http://localhost:2368/</link><image><url>http://localhost:2368/favicon.png</url><title>The Gradient</title><link>http://localhost:2368/</link></image><generator>Ghost 1.22</generator><lastBuildDate>Sat, 14 Apr 2018 06:29:44 GMT</lastBuildDate><atom:link href="http://localhost:2368/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title>Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!</title><description>A simple allegory motivates questioning a foundational pillar of AI research</description><link>http://localhost:2368/does-reinforcement-learning-actually-make-sense/</link><guid isPermaLink="false">5ad18868f092d13d8a5762eb</guid><category>Perspectives</category><dc:creator>Andrey Kurenkov</dc:creator><pubDate>Sat, 17 Mar 2018 01:17:07 GMT</pubDate><media:content url="http://localhost:2368/content/images/2018/03/human-priors.png" medium="image"/><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;h2 id="theresaprobleminthefoundationsofreinforcementlearninghereswhatwecandoaboutit"&gt;There's a problem in the foundations of reinforcement learning. Here's what we can do about it&lt;/h2&gt;
&lt;img src="http://localhost:2368/content/images/2018/03/human-priors.png" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;&lt;p&gt;Steel yourself, dear reader, for this is a long essay but one well worth reading. If you stick with it, you will be rewarded with a fun allegory, radical questioning of the basis of one of AI's core fields, a critical take on AlphaGo Zero, a rebuke of false assumptions held by many, an enumeration of various methods of incorporating prior knowledge and instruction into Deep Learning, and a radical conclusion.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;An expanded extract from this article, titled 'AlphaGo Zero Is Not A Sign of Imminent Human-Level AI' can be read on &lt;a href="http://www.skynettoday.com/content/editorials/is-alphago-zero-overrated/"&gt;Skynet Today&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id="theallegoryoftheboardgame"&gt;The Allegory of the Board Game&lt;/h3&gt;
&lt;p&gt;Imagine: a friend of yours invites you to play a board game you have never played. In fact, you have never played any board games, nor indeed any kind of game. Your friend tells you what the valid moves are, but not what they mean or how the game is scored.  So, you start playing — no more questions, no more explanations. And you lose. And lose. And lose. And... lose some more. After all, you don't know the rules or the objective of the game, and have no prior experience to draw on.  Slowly you figure out some patterns in your losses. You’re still losing, but not as quickly anymore. After a few weeks of consecutive play and many thousands of games, you even manage to just barely win.&lt;/p&gt;
&lt;p&gt;Silly, right? Why didn't you just ask what the goal of the game is and how it is supposed to be played? Yet the above paragraph is how the majority of &lt;a href="http://theai.wiki/Reinforcement%20Learning"&gt;Reinforcement Learning (RL)&lt;/a&gt; methods still work today.&lt;/p&gt;
&lt;figure&gt;
      &lt;img src="https://draftin.com:443/images/56452?token=x-CH-CY5uyLtTMOZZi1QenH_byGZFUyKx0s_dP6gvbb_XSMeUjNc6AMXAhm99dEm2ouHhnfHjaBVMLRcFG_iUwo" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;RL is one of the major branches of AI. So questioning its basic formulation is a pretty big deal...&lt;a href="https://steemit.com/ai/@sykochica/artificial-intelligence-part-2-artificial-general-intelligence-and-artificial-consciousness"&gt;&lt;b&gt;(source)&lt;/b&gt;&lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Some context for those who have not studied the dark arts of AI: RL is one of the few basic subfields within AI. Its aim to have an 'agent' (you, in the board game allegory) interact with an 'environment' (the board) to learn what 'actions' (board game moves) it needs to take in any given environment 'state' (board game configuration) to maximize its 'long term reward' (the final board game score). Typically, this setup is used to learn a 'skill' (playing a board game).&lt;/p&gt;
&lt;p&gt;In the typical model of reinforcement learning, the agent begins only with knowledge of which actions are possible; it knows nothing else about the world, and it's expected to learn the skill solely by interacting with the environment and receiving rewards after every action it takes. In other words, the agent learns 'from scratch'. This sort of learning has notably been used to tackle games like &lt;a href="http://theai.wiki/TD-Gammon"&gt;backgammon&lt;/a&gt; and more recently Go, as well as various problems in robotics and elsewhere. Let's call this learning-from-scratch approach 'pure RL'.&lt;/p&gt;
&lt;figure&gt;
      &lt;img src="https://draftin.com:443/images/56340?token=E0_De8QweCDLkpmSQMU3gigFg56wdbx5Ugu5paki2Ew8QOFXno5yyZxJh5_3i9skeDXoy8bIXovT-Wvkxh1aMGg" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;RL visualized as a gif. In the board game story, an 'episode' would be one full game. In this example, and in many RL problems, only the last state has a non-zero reward which is why it only changes after the last action. &lt;a href="https://mpatacchiola.github.io/blog/2017/01/15/dissecting-reinforcement-learning-2.html"&gt;&lt;b&gt;(source)&lt;/b&gt;&lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Research in RL has recently been reinvigorated by &lt;a href="http://theai.wiki/Deep%20Learning"&gt;Deep Learning&lt;/a&gt;, but the basic model hasn't changed much — fundamentally, much if not most modern RL research, deep or otherwise, is based on pure RL. This learning-from-scratch approach (having an agent learning what actions to take based only on rewards it gets from the environment after each action taken) goes back to the very creation of RL as a research field and is encoded in the formulation of its most fundamental equations. There is also research on 'model-based' methods, in which the agent has a model of the environment (knows how each move changes the state of the board), but beyond that still needs to learn purely from the reward signal. For brevity, let's include model-based RL under the umbrella term pure RL.&lt;/p&gt;
&lt;p&gt;So here’s the basic question: &lt;strong&gt;how reasonable it is to design pure RL AI models if it makes so little intuitive sense?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If it's so absurd to conceive of a human learning a new board game through pure RL, should we not wonder if pure RL is a flawed framework for how AI agents should learn? And more generally, does it really make sense to start learning a new skill with neither prior experience nor any higher-level instruction beyond this low level 'this is a good/bad move' reward signal?&lt;/p&gt;
&lt;p&gt;This is a Big Question in the field - a question about the basic formulation of one of the core areas of research within AI. It’s a question about whether we should provide AI agents with some form of prior experience or high-level explanation — not just in board games, but also in potentially all applications of RL, ranging from teaching robots to execute particular physical manipulation to optimizing energy consumption in data warehouses. As stated - a Big Question. So, we will explore the answer to this question carefully and methodically, as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We'll begin by showing the major accomplishments of pure RL are not as impressive as they may seem.&lt;/li&gt;
&lt;li&gt;We'll go further by enumerating the precise limitations pure RL imposes on AI agents.&lt;/li&gt;
&lt;li&gt;Then we'll overview the different established approaches within AI to address those limitations (chiefly, &lt;strong&gt;meta-learning&lt;/strong&gt; and &lt;strong&gt;zero-shot learning&lt;/strong&gt;). After all, I'm not going to offer complaints without solutions.&lt;/li&gt;
&lt;li&gt;And finally, get to a survey of monumentally exciting work based on these approaches, and conclude with what that work implies for the future of RL and AI as a whole&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So, without further ado, let's get going.&lt;/p&gt;
&lt;figure&gt;
      &lt;img src="https://draftin.com:443/images/56145?token=d8aM-GVFQh9hRwS5TPjXRglcVNPa4FnMr2g-gWFA68i74HWsrKHeZpgFrW31FpT1o0w4AnGqPRJ2V3jUNvuXeHA" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;The RL slide seen in a thousand slide decks, the formulation that everyone agrees on. But should they?&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id="doespurerlactuallymakesense"&gt;Does Pure RL Actually Make Sense?&lt;/h3&gt;
&lt;p&gt;Many people’s immediate response goes something like the following:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Sure, it makes sense to use pure RL — AI agents are not humans and do not have to learn like us, and pure RL has already been shown to solve all sorts of complex problems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I disagree. By definition, AI research involves the ambition to enable machines to do things they are incapable of but humans and animals are capable of. Therefore, inspiration from (or, at the very least, comparison to) human intelligence is inherently appropriate. And as for pure RL’s supposed “effectiveness,” I'll go further: pure RL has not been used to solve any truly complex problems.&lt;/p&gt;
&lt;p&gt;But what about AlphaGo Zero, the pure RL model that learned to play Go? Indeed, AlphaGo Zero is arguably the most impressive and definitely the most praised accomplishment of pure RL. It is therefore also a good exemplar to represent all the various accomplishments of pure RL. So let’s take a closer look at AlphaGo Zero and see what makes it so great – and why that’s not enough.&lt;/p&gt;
&lt;h4 id="whyalphagozeroisgreat"&gt;Why AlphaGo Zero Is Great&lt;/h4&gt;
&lt;p&gt;For those unaware, AlphaGo Zero is DeepMind's recent successor to AlphaGo (the program that first beat humanity's best at Go). Unlike the original AlphaGo, which learned through a combination of supervised learning and reinforcement learning, AlphaGo Zero learns purely through reinforcement learning and &lt;a href="https://blog.openai.com/competitive-self-play/"&gt;&amp;quot;self-play.&amp;quot;&lt;/a&gt;  Thus, it follows the overall methodology of pure RL quite closely (with the agent starting with zero knowledge and learning from a reward signal just as in the board game analogy), though it also uses a provided model (the rules of the game) and self-play to reliably and continuously get better. Because it was no longer learning its success from humans, AlphaGo Zero was seen by many as even more of a game changer than AlphaGo:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://fortune.com/2017/10/19/google-alphago-zero-deepmind-artificial-intelligence/"&gt;&amp;quot;Google's New AlphaGo Breakthrough Could Take Algorithms Where No Humans Have Gone&amp;quot;&lt;/a&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;quot;While it sounds like some sort of soda, AlphaGo Zero may represent as much of a breakthrough as its predecessor, since it could presage the development of algorithms with skills that humans do not have. ... AlphaGo Zero ... trained itself entirely through reinforcement learning. And, despite starting with no tactical guidance or information beyond the rules of the game, the newer algorithm managed to beat the older AlphaGo by 100 games to zero.&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.theguardian.com/science/2017/oct/18/its-able-to-create-knowledge-itself-google-unveils-ai-learns-all-on-its-own"&gt;&amp;quot;'It's able to create knowledge itself': Google unveils AI that learns on its own&amp;quot;&lt;/a&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;quot;In a major breakthrough for artificial intelligence, [AG0] took just three days to master the ancient Chinese board game of Go ... with no human help&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.inc.com/lisa-calhoun/google-artificial-intelligence-alpha-go-zero-just-pressed-reset-on-how-we-learn.html"&gt;&amp;quot; Google Artificial Intelligence 'Alpha Go Zero' Just Pressed Reset On How To Learn&amp;quot;&lt;/a&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;quot;Alpha Go Zero is changing the game for how we solve big problems.&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.theguardian.com/science/2017/oct/18/its-able-to-create-knowledge-itself-google-unveils-ai-learns-all-on-its-own"&gt;&amp;quot;'It's able to create knowledge itself': Google unveils AI that learns on its own&amp;quot;&lt;/a&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;quot;In a major breakthrough for artificial intelligence, [AG0] took just three days to master the ancient Chinese board game of Go ... with no human help&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.inc.com/lisa-calhoun/google-artificial-intelligence-alpha-go-zero-just-pressed-reset-on-how-we-learn.html"&gt;&amp;quot; Google Artificial Intelligence 'Alpha Go Zero' Just Pressed Reset On How To Learn&amp;quot;&lt;/a&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;quot;Alpha Go Zero is changing the game for how we solve big problems.&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure&gt;
     &lt;iframe width="560" height="315" src="https://www.youtube.com/embed/tXlM99xPQC8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen&gt;&lt;/iframe&gt;
     &lt;figcaption&gt;DeepMind's own explanation of why AlphaGo Zero is so exciting. 
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;AlphaGo Zero seems like a perfect testament to the usefulness of pure RL for solving complex problems. It has taken the field decades to get here, and, like all those slides in presentations about AI point out, the &lt;a href="http://theai.wiki/Branching%20Factor"&gt;branching factor&lt;/a&gt; of Go does indeed make it a challenging board game. This was also the first time a single algorithm was used to crack both Chess and Go, and was not specifically tailored for either game like &lt;a href="https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)"&gt;Deep Blue&lt;/a&gt; and the original AlphaGo were. Don’t get me wrong – AlphaGo Zero is certainly monumental and exciting work (and great PR).&lt;/p&gt;
&lt;figure&gt;
      &lt;img src="https://draftin.com:443/images/56473?token=3G6YQX_K8LNCSvkf-fxVAxQMIhG2_8ZOd3uGpjtGKF-myi9JaeEyNf4-cb_r3pynXoHYJr8_n2TPZvaY4OedIjA" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;AlphaGo is an inarguably historic achievement.&lt;b&gt;&lt;a href="http://www.andreykurenkov.com/writing/ai/a-brief-history-of-game-ai/"&gt;(source)&lt;/a&gt;&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h4 id="whyalphagozeroisnotabouttosolveallofai"&gt;Why AlphaGo Zero Is Not About To Solve All of AI&lt;/h4&gt;
&lt;p&gt;With those positive things having been said, some perspective: AlphaGo Zero is not really a testament to the usefulness of such techniques for solving the hard problems of AI. You see, Go is only hard within the context of the simplest category of AI problems. That is, it is in the category of problems with every property that makes a learning task easy: it is &lt;a href="https://en.wikibooks.org/wiki/Artificial_Intelligence/AI_Agents_and_their_Environments"&gt;deterministic, discrete, static, fully observable, fully-known, single-agent, episodic&lt;/a&gt;, cheap and easy to simulate, easy to score...&lt;/p&gt;
&lt;p&gt;Literally the only challenging aspect of Go is its huge branching factor. Predictions that &lt;a href="http://theai.wiki/AGI"&gt;AGI (Artificial General Intelligence)&lt;/a&gt; is imminent based only on AlphaGo's success can be safely dismissed — &lt;a href="https://medium.com/@karpathy/alphago-in-context-c47718cb95a5"&gt;the real world is vastly more complex than a simple game like Go&lt;/a&gt;. Even fairly similar problems that have most but not all of the properties that make a learning task easy, &lt;a href="http://localhost:2368/content/news/openai-dota-ii/"&gt;such as the strategic video game DotA II&lt;/a&gt;, are far beyond our grasp right now.&lt;/p&gt;
&lt;figure&gt;
     &lt;img src="https://draftin.com:443/images/56318?token=MjHXMguA3ItT0gpWGe7oTsXUaCiQD6zqiltIdZ898zov75aC1sUXDxqwb4DdG5ERU8USDWQSP-r_X57wSHGb1G0" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;A (rough) diagram of AI problem complexity. Note that Go and (most) Atari games are in the same league as chess; just about the only distinction is branching factor. Pure RL may solve games like Go and Pong, but as &lt;a href="http://www.skynettoday.com/news/alphago/"&gt; argued elsewhere in more detail&lt;/a&gt; most AI problems are vastly more difficult — categorically different.
    &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Another important thing to understand beyond the categorical simplicity of Go is its narrowness. Alpha Gi is an example of a &lt;a href="http://theai.wiki/Weak%20AI"&gt;Weak AI&lt;/a&gt;, an agent characterized by only being able to perform one 'narrow' task such as playing a 19 by 19 game of Go. Though AlphaGo has the impressive ability to learn 3 different board games, it learns them separately (that is, there is not a single trained neural net that can play the 3 games, but 3 separate neural nets with one for each game). Even then it can only learn a very narrow range of games: 2-player grid-based board games without any necessary memorization of prior positions or moves (that is, on any given move the board contains all the necessary information to decide on the next move; no memory of the past required).&lt;/p&gt;
&lt;figure&gt;
&lt;blockquote class="twitter-tweet" data-lang="en"&gt;&lt;p lang="en" dir="ltr"&gt;&amp;quot;Generalized AI is worth thinking about because it stretches our imaginations and it gets us to think about our core values and issues of choice and free will that actually do have significant applications for specialized AI.&amp;quot; - &lt;a href="https://twitter.com/BarackObama?ref_src=twsrc%5Etfw"&gt;@BarackObama&lt;/a&gt; &lt;a href="https://t.co/VFhJsMXuIq"&gt;pic.twitter.com/VFhJsMXuIq&lt;/a&gt;&lt;/p&gt;&amp;mdash; Lex Fridman (@lexfridman) &lt;a href="https://twitter.com/lexfridman/status/976461233443561477?ref_src=twsrc%5Etfw"&gt;March 21, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
     &lt;figcaption&gt;In a &lt;a href="https://www.wired.com/2016/10/president-obama-mit-joi-ito-interview/"&gt;lengthy interview with Wired&lt;/a&gt;, then-president Obama displayed an impressively nuanced understanding of the state of AI. If only some unnamed billionaires would communicate to the public similarly...
     &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;So, while AlphaGo Zero works and its achievement is impressive, it is fundamentally similar to Deep Blue in being an expensive system, engineered over many years, with millions of dollars of investment, purely for the task of playing a game — and nothing else. Though Deep Blue was great PR for IBM, all that work and investment is not usually seen as having contributed much to the progress of broader AI research, having been ultra-specific to solving the problem of playing Chess. Just as with the algorithms that power AlphaGo Zero, human-tweaked heuristics and sheer computational brute force can definitely be used to solve some challenging problems — but they ultimately did not get us far beyond Chess, not even to Go.&lt;/p&gt;
&lt;figure&gt;
     &lt;img src="https://draftin.com:443/images/56472?token=PT3XzKUl83mv1Ugo4meVAuSbd2yZt4O7yC0w8taldm0ESr3XQv9it5dvEk71YqQ4zAXgL9RCyOJIgiy-q9IsRvU" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;"One day after its chess computer defeated Garry Kasparov, the world chess champion, I.B.M. stock surged to a 10-year high and was only a bit shy of its record." &lt;b&gt;&lt;a href="http://www.nytimes.com/1997/05/13/business/ibm-s-stock-surges-by-3.6.html"&gt;(source)&lt;/a&gt;&lt;/b&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;I write this not to be controversial or take away from DeepMind's fantastic work, but rather to push back against all the unwarranted hype AlphaGo Zero's success has generated and encourage more conversation about the limitations of pure RL. And all that aside, it should still be asked:  might there be a better way for AI agents to learn to play Go? The very name “AlphaGo Zero” is a reference to the idea that the model learns to play Go &lt;a href="https://deepmind.com/blog/alphago-zero-learning-scratch/"&gt;&amp;quot;from scratch&amp;quot;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But let's recall that board game allegory. Trying to learn the board game 'from scratch' without any explanation from your friend was absurd, right? So why is it a goal to strive towards with AI?&lt;/p&gt;
&lt;p&gt;In fact, what if the board game you were trying to learn was Go — how would &lt;em&gt;you&lt;/em&gt; start learning it? You would read the rules, learn some high-level strategies, recall how you played similar games in the past, get some advice... right? Indeed, it's at least partially because of the 'learning from scratch' &lt;strong&gt;limitation&lt;/strong&gt; of AlphaGo Zero that it is not truly impressive compared to human learning: like Deep Blue, it still relies on seeing orders of magnitude more Go games and planning for orders of magnitude more scenarios in any given game than any human ever does.&lt;/p&gt;
&lt;figure&gt;
     &lt;img src="https://draftin.com:443/images/56301?token=pVbCvo4eZCulh3YQIgOjTGJKbwpF2BfMCUCWkytrr5-hsHVx-DJPE_uZcieYvsfJ5C07opZpqHb7-azGziKT-1k" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;The progression of AlphaGo Zero's skill. It is certainly impressive that it takes 'just' 3 days of non-stop computation to surpass the best humans in the world. But perhaps we should also note it takes a whole day and orders of magnitude more games than humans get to experience in their lifetimes to get to an ELO score of 0 (which even the weakest human can do easily)...? From &lt;a href="https://deepmind.com/blog/alphago-zero-learning-scratch/"&gt;&lt;b&gt;"DeepMind's AlphaGo Zero Blog Post"&lt;/b&gt;&lt;/a&gt;
     &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;To be fair, model-based pure RL techniques (including AlphaGo Zero) can be legitimately useful for 'narrow' constrained tasks. So, to be clear: yes, pure RL methods are reliably successful at many narrow tasks. However, with the success of Deep Learning the AI research community as a whole is now trying to tackle ever more complex tasks, ranging from driving cars to holding conversations. It is for these less narrow tasks (that is, the majority of problems AI needs to tackle), and for the long term future of AI as a whole, that the limitations of pure RL may not 'make sense'.&lt;/p&gt;
&lt;p&gt;So let's move on to tackling our revised question: is pure RL, and the idea of learning from scratch in general, the right approach for non-narrow/complex tasks?&lt;/p&gt;
&lt;h3 id="ispurerltherightideaforproblemsbeyondgames"&gt;Is Pure RL The Right Idea for Problems Beyond Games?&lt;/h3&gt;
&lt;p&gt;Okay – so it is valid to compare AI algorithms to human learning, and question whether learning 'from scratch' as pure RL does is the right idea to explore. One answer to our next question might be:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Yes, pure RL is the right approach to problems beyond those like Go. Though it makes no sense in the context of board games, it does make sense generally to learn things 'from scratch'. And inspiration from humans aside, it makes sense to start from scratch so the agent has no preconceptions and can be better than us (as with AlphaGo Zero). In fact, this is how babies learn everything at first: before we have any understanding of the world or ability to communicate, our parents' encouragement or concern acts as a direct reward signal.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Of course, as with the board game allegory, this hypothetical answer is not a real claim about how babies learn, but rather an informal analogy between an AI technique and human learning. And, as with the previous answer, it is unsatisfactory.&lt;/p&gt;
&lt;h4 id="islearningfromscratchreallytherightidea"&gt;Is Learning From Scratch Really The Right Idea?&lt;/h4&gt;
&lt;p&gt;Let's start with that last bit, ignoring human inspiration and considering the merits of learning from scratch in the context of AI in general. The typical justification of doing things “from scratch” is that the presumed alternative – hard-coding human intuitions into the model – might limit the model’s accuracy through unnecessary restrictions, or even worsen its performance with incorrect intuitions. This perspective has become mainstream with the success of deep learning methods, which learn 'end-to-end' models with millions of parameters, trained on staggering amounts of data and having only &lt;a href="http://www.abigailsee.com/2018/02/21/deep-learning-structure-and-innate-priors.html"&gt;a few innate priors&lt;/a&gt;. And it has limited research into alternatives to starting from scratch, such as incorporating prior knowledge or extra instruction during learning.&lt;/p&gt;
&lt;figure&gt;
     &lt;img src="https://draftin.com:443/images/56523?token=UB20Rx2Q_SoqSJ1woI7iVoaEg3OG5Uc6F04UYRW8pDGIRbVW_P55ElnkYPTVskdY-UdtaDmZBxVFnc1nSAeKMvQ" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;An illustration of both older non-traditional speech recognition and end-to-end Deep learning methods. The latter works much better and is the basis for modern state of the art speech recognition.  &lt;a href="http://blog.easysol.net/building-ai-applications/"&gt;&lt;b&gt;(source)&lt;/b&gt;&lt;/a&gt;
     &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Here's the thing: incorporating prior knowledge or instructions doesn't necessitate imposing a lot of limiting structure based on human intuition on the learning agent. In other words, it is possible to inform a learning agent or model about the task at hand without limiting its ability to learn in the Deep Learning style (that is, informed primarily by a lot of data, unlike Deep Blue and before that &lt;a href="https://en.wikipedia.org/wiki/Expert_system"&gt;expert systems&lt;/a&gt;). For most AI problems, not starting from scratch would not necessarily limit what the agent can learn in any way; at the very least Deep Learning models seem pretty adept at escaping &lt;a href="http://theai.wiki/Local%20Maxima"&gt;local maxima&lt;/a&gt; (which is presumably what some sort of initialization would result in). So, there is no clear reason for AlphaGo Zero to emphasize starting from scratch so much - it can likely be bootstrapped with human knowledge (as was done with the original AlphaGo) or from learning other board games beforehand and still converge to the same superhuman level of skill. Don't believe me? Just wait for the next section.&lt;/p&gt;
&lt;figure&gt;
     &lt;img src="https://draftin.com:443/images/56521?token=ACWFP3PjlRfKcOHNGGc_y0dMI3hYVjFkjwSkOp14-QO_fVtZeQELyYqPZD0k5z7dceD3GnEGiGnmqxKXtNMiFkQ" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;A Gif illustration of different optimization methods on an optimization landscape. Deep Learning methods have been surprisingly good at not getting trapped in situations like this despite having incredibly complex optimization landscapes. &lt;a href="http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html"&gt;&lt;b&gt;(source)&lt;/b&gt;&lt;/a&gt;
     &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;And &lt;strong&gt;all that aside&lt;/strong&gt;, even if you care about none of this and just want to start from scratch - is pure RL the only way to do so? The answer used to be a no-brainer; in the domain of &lt;a href="http://theai.wiki/Derivative-Free%20Optimization"&gt;gradient-free optimization&lt;/a&gt;, pure RL was the most principled and trusted approach you could pick. But multiple recent papers have seriously questioned that stance by showing that the relatively simpler and broadly less respected &lt;a href="http://theai.wiki/Evolution%20Strategy"&gt;Evolution Strategy&lt;/a&gt;-based methods seem to do just about as well on the same sorts of benchmarks pure RL has been routinely evaluated on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1803.07055"&gt;&amp;quot;Simple random search provides a competitive approach to reinforcement learning&amp;quot;&lt;/a&gt;: &amp;quot;A common belief in model-free reinforcement learning is that methods based on random search in the parameter space of policies exhibit significantly worse sample complexity than those that explore the space of actions. We dispel such beliefs by introducing a random search method for training static, linear policies for continuous control problems, matching state-of-the-art sample efficiency on the benchmark MuJoCo locomotion tasks. &amp;quot;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1712.06567"&gt;&amp;quot;Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning&amp;quot;&lt;/a&gt;: &amp;quot;We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. &amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
     &lt;img src="https://draftin.com:443/images/56524?token=bu5ANzJ6z-BJR9qcvXyTBu8MdgOosn9SL0x0lWhW_WthnfmCXxwCVtbKE2yrf1-nDnTfNXzGM5z_B4aczDHyq24" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;From &lt;a href="https://arxiv.org/abs/1712.06567"&gt;&lt;b&gt;Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning&lt;/b&gt;&lt;/a&gt;
     &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1703.03864"&gt;&amp;quot;Evolution Strategies as a Scalable Alternative to Reinforcement Learning&amp;quot;&lt;/a&gt;: &amp;quot;Deep artificial neural networks (DNNs) are typically trained via gradient-based learning algorithms, namely backpropagation. Evolution strategies (ES) can rival backprop-based algorithms such as Q-learning and policy gradients on challenging deep reinforcement learning (RL) problems. However, ES can be considered a gradient-based algorithm because it performs stochastic gradient descent via an operation similar to a finite-difference approximation of the gradient. That raises the question of whether non-gradient-based evolutionary algorithms can work at DNN scales. Here we demonstrate they can: we evolve the weights of a DNN with a simple, gradient-free, population-based genetic algorithm (GA) and it performs well on hard deep RL problems, including Atari and humanoid locomotion.&amp;quot;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1703.02660"&gt;&amp;quot;Towards Generalization and Simplicity in Continuous Control&amp;quot;&lt;/a&gt;: &amp;quot;This work shows that policies with simple linear and RBF parameterizations can be trained to solve a variety of continuous control tasks, including the OpenAI gym benchmarks. The performance of these trained policies are competitive with state of the art results, obtained with more elaborate parameterizations such as fully connected neural networks. Furthermore, existing training and testing scenarios are shown to be very limited and prone to over-fitting, thus giving rise to only trajectory-centric policies.&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;blockquote class="twitter-tweet" data-lang="en"&gt;&lt;p lang="en" dir="ltr"&gt;You may not like it, but this is what peak performance looks like. &lt;a href="https://t.co/QfjOIRjqMW"&gt;pic.twitter.com/QfjOIRjqMW&lt;/a&gt;&lt;/p&gt;&amp;mdash; hardmaru (@hardmaru) &lt;a href="https://twitter.com/hardmaru/status/976160956064587776?ref_src=twsrc%5Etfw"&gt;March 20, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
     &lt;figcaption&gt;&lt;a href="http://people.eecs.berkeley.edu/~brecht/"&gt;Ben Recht&lt;/a&gt;, a leading researcher on the theory and practice optimization algorithms and one of the authors of the "Simple random search provides a competitive approach to reinforcement learning" paper, has &lt;a href="http://www.argmin.net/2018/03/20/mujocoloco/
"&gt;recently written a blog post discussing these recent works&lt;/a&gt;: &lt;b&gt;We have seen that random search works well on simple linear problems and appears better than some RL methods like policy gradient. But does random search break down as we move to harder problems? Spoiler Alert: No. But keep reading!&lt;/b&gt; 
     &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;a href="http://people.eecs.berkeley.edu/~brecht/"&gt;Ben Recht&lt;/a&gt;, a leading researcher on the theory and practice optimization algorithms and one of the authors of the &amp;quot;Simple random search provides a competitive approach to reinforcement learning&amp;quot; paper, has &lt;a href="http://www.argmin.net/2018/03/20/mujocoloco/
"&gt;nicely summarized all the above points&lt;/a&gt; :&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We have seen that random search works well on simple linear problems and appears better than some RL methods like policy gradient. But does random search break down as we move to harder problems? Spoiler Alert: No.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id="theproblemswithlearningfromscratch"&gt;The Problems With Learning from Scratch&lt;/h4&gt;
&lt;p&gt;Now, to the idea of babies learning sort of by RL - since all of us start by 'learning from scratch', shouldn’t it be a good idea in general? Maybe... but probably not. For one, it is important to note that babies do much more than just pure RL; there is undeniably also some sort of unsupervised learning and non-pure RL in the vague notions of curiosity and intrinsic motivation. But even ignoring that, we must ask: for most problems we may want to leverage AI to solve, does it make sense to have the learning agents learn like babies? That is, do people ever start learning a complex new skill (such as putting together new IKEA furniture or even driving a car) given no information (no information at all, not even prior experience) except for what possible actions they have as part of that skill? No, right?&lt;/p&gt;
&lt;p&gt;Maybe for some very fundamental and general problems (such as locomotion or navigation) it makes sense to 'start from scratch' and do pure RL just as human babies do, since these problems ar so broad it's hard to do anything else. But the vast majority of problems AI can be used to tackle are far more specific (and far beyond the grasp of babies): board games, video games, and complex physical manipulations all require some prior understanding. For this vast majority of problems in AI, there is no clear benefit in starting from scratch. Just the opposite: starting from scratch is the primary reason for many of the &lt;a href="https://www.wired.com/story/greedy-brittle-opaque-and-shallow-the-downsides-to-deep-learning/"&gt;widely&lt;/a&gt;  [agreed]9http://www.wired.co.uk/article/deep-learning-automl-cloud-gary-marcus?utm_campaign=Revue%20newsletter&amp;amp;utm_medium=Newsletter&amp;amp;utm_source=The%20Wild%20Week%20in%20AI) &lt;a href="https://blog.keras.io/the-limitations-of-deep-learning.html"&gt;upon&lt;/a&gt; limitations of current AI and RL:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Current AI is &lt;strong&gt;data-hungry&lt;/strong&gt; (or in jargon, &lt;a href="https://en.wikipedia.org/wiki/Efficiency_(statistics)"&gt;sample-inefficient&lt;/a&gt; – in most cases, massive amounts of data are needed for state of the art AI methods to be useful.  This is particularly bad for pure RL techniques; recall how AlphaGo Zero needed millions of games of Go to get to an ELO score of 0, which most people would manage after right away. Learning from scratch is inherently just about the least sample efficient approach to learning there can be.&lt;/li&gt;
&lt;li&gt;Current AI is &lt;strong&gt;opaque&lt;/strong&gt; – in most cases, we have nothing but high-level intuitions about what an AI algorithm learns and how it works. For most AI problems, we want the algorithms to be predictable and explainable; a big neural net that just learns whatever it wants from scratch given just the low level reward signal and maybe an environment model (just how AlphaGo Zero works) is just about the least explainable and predictable approach to learning there can be.&lt;/li&gt;
&lt;li&gt;Current AI is &lt;strong&gt;narrow&lt;/strong&gt; – in most cases, the AI models we build can only do one very narrow task and can easily be broken. Learning every single skill from scratch limits the ability to learn anything but one specific thing.&lt;/li&gt;
&lt;li&gt;Current AI is &lt;strong&gt;brittle&lt;/strong&gt; – in most cases, our AI models generalize well to unseen inputs but are also surprisingly easy to trick or fool in ways that would never work on a human.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For most AI problems, there is just no point to learning from scratch. We know what we want the AI agent to learn. If the AI agent were a person, we could explain the task and probably provide some tips. So why don't we try to do just that?&lt;/p&gt;
&lt;h3 id="whydontwemovebeyondpurerl"&gt;Why Don't We Move Beyond Pure RL?&lt;/h3&gt;
&lt;p&gt;Okay then, so we should draw inspiration from how adult humans learn and try to do better than learning from scratch. Now, you might be thinking something like this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We cannot just move beyond pure RL to emulate human learning — pure RL is rigorously formulated, and our algorithms for training AI agents are proven based on that formulation. Though it might be nice to have a formulation that aligns more closely with how human adults learn, we just don't have one.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Well... maybe we do have such a formulation. Let's start by spelling in what ways the way we would learn is different from pure RL. When starting to learn a new skill, we basically do one of two things: guess at what the instructions might be (recall our prior experience with board games), or read/listen to some instructions (check the board game's rules). We always know or can at least guess the goal and broad approach for a particular skill from the get-go, and never reverse-engineer these things from a low-level reward signal.&lt;/p&gt;
&lt;figure&gt;
     &lt;img src="https://draftin.com:443/images/56296?token=z3zdGkZCJZrS5ULzTyDJBC6Q37UfuZ1IwKFtdJXuvuTv_K6KrXn5JC7S3ejaH5C0053uy2A7nzTmNp87SVA3eug" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;Researchers at UC Berkeley have recently demonstrated that humans learn much faster than pure RL in part due to making use of prior experience.
     From &lt;a href="https://arxiv.org/abs/1802.10217"&gt;&lt;b&gt;Investigating Human Priors for Playing Video Games&lt;/b&gt;&lt;/a&gt;
     &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h4 id="themanyknownapproachestoleveragingpriorexperienceandinstruction"&gt;The Many Known Approaches to Leveraging Prior Experience and Instruction&lt;/h4&gt;
&lt;p&gt;The ideas of leveraging prior experience and getting instructions have very direct analogues in AI research. Let's break those down:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Meta-learning&lt;/strong&gt;: tackles the problem of 'learning how to learn': making RL agents pick up new skills faster having already learned similar skills. Related is the idea of &lt;strong&gt;curriculum learning&lt;/strong&gt; which is roughly a type of meta-learning that specifically works by learning a sequence of progressively harder skills.&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
     &lt;img src="https://draftin.com:443/images/56149?token=-yVdLTalC1wPc_DSySA-CNY9dtMx7CjkyZ5C_R5RTFEeUgIVh9j4SwYZ0p77lZ5ZsqBaQZoLqvagvZb6yRcTzzU" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;A cutting-edge meta-learning algorithm, MAML. The agent is able to learn both backward and forward running with very few iterations by leveraging meta-learning.
     From &lt;a href="http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/"&gt;&lt;b&gt;"Learning to Learn"&lt;/b&gt;&lt;/a&gt;
     &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;Related to this is the notion of &lt;strong&gt;transfer learning&lt;/strong&gt;, which roughly speaking corresponds to 'transferring' skills attained in one problem to another potentially different problem.&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;blockquote class="twitter-tweet" data-lang="en"&gt;&lt;p lang="en" dir="ltr"&gt;&amp;quot;I think transfer learning is the key to general intelligence. And I think the key to doing transfer learning will be the acquisition of conceptual knowledge that is abstracted away from perceptual details of where you learned it from.&amp;quot; - Demis Hassabis &lt;a href="https://twitter.com/demishassabis?ref_src=twsrc%5Etfw"&gt;@demishassabis&lt;/a&gt; &lt;a href="https://t.co/oDQjvx4TLa"&gt;pic.twitter.com/oDQjvx4TLa&lt;/a&gt;&lt;/p&gt;&amp;mdash; Lex Fridman (@lexfridman) &lt;a href="https://twitter.com/lexfridman/status/975018912483020800?ref_src=twsrc%5Etfw"&gt;March 17, 2018&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
     &lt;figcaption&gt;Demis Hassabis, a co-founder and the most visible public voice for DeepMind, discussing Transfer Learning. "And I think that [Transfer Learning] is the key to actually general intelligence, and that's the thing we as humans do amazingly well. &lt;b&gt;For example, I played so many board games now, if someone were to teach me a new board game I would not be coming to that fresh anymore, straight away I could apply all these different heuristics that I learned from all these other games to this new one even if I've never seen this one before, and currently machines cannot do that... so I think that's actually one of the big challenges to be tackled towards general AI.&lt;/b&gt; 
     &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Zero-shot learning&lt;/strong&gt; is in a sense similar in that the idea is to learn new skills fast, but takes it further by not leveraging &lt;strong&gt;any&lt;/strong&gt; attempts at the new skill; the learning agent just receives 'instructions' for the new task, and is supposed to be able to perform well without any experience of the new task.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;One-shot&lt;/strong&gt; and &lt;strong&gt;few-shot&lt;/strong&gt; learning are also active areas of research, and differ from zero-shot learning in that they get demonstrations of the skill to be learned, or just a few iterations of experience, rather than indirect 'instructions' that do not involve the skill actually being executed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Life Long Learning&lt;/strong&gt; and &lt;strong&gt;Self Supervised Learning&lt;/strong&gt; are yet more examples of learning, in this case roughly being defined by long-term continuous learning without human guidance.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
     &lt;img src="https://draftin.com:443/images/56739?token=xG1woy3IrowMxwTVPqxpbjpnCs0cKSSoM464wCPnCv-Ywzzg-d2ndBsYdFg29BwOwtD_jzPFONZr4JBUsg4O_4c" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;From &lt;a href="https://hackernoon.com/effective-learning-the-near-future-of-ai-9bb671211d96"&gt;&lt;b&gt;Effective Learning: The Near Future of AI
&lt;/b&gt;&lt;/a&gt;
     &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;So then, these are all methodologies that go beyond learning from scratch. In particular, meta-learning and zero-shot learning capture different elements of how a human adult would actually approach that new board game situation. A meta-learning agent would leverage experience with prior board games to learn faster, though it would not not ask for the rules of the game. Complementarily, a zero-shot learning agent would ask for the instructions, but then not try to do any learning to get better beyond its initial guess of how to play the game well. One and few-shot learning in some sense do both, but are limited by only getting demonstrations of how the skill can be done — that is, the agent would observe others playing the board game, but not request for any of what is going on to be explained or to be told the rules of the game.&lt;/p&gt;
&lt;figure&gt;
     &lt;img src="https://draftin.com:443/images/56248?token=_CoKO8B3fwXJJu-raMFVF8yqMZdHiLuq_XH8EmrE4ZyG5pvH9uNlsfj1Mv77Pps-1NXIo-U-2eHx_ri1FDAAqFg" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;A recent 'hybrid' approach that combines one-shot and meta-learning. From &lt;a href="https://arxiv.org/abs/1802.01557"&gt;&lt;b&gt;"One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning".&lt;/b&gt;&lt;/a&gt;
     &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Intuitively, the broad notions of meta-learning and zero-shot learning are what 'make sense' in the context of the board game allegory. Better yet, hybrids of zero-shot, few-shot and meta learning come close to representing what people actually do: use prior experience as well as instructions to form an initial hypothesis of how the skill should be done, observe a few example executions, and then actually try doing the skill themselves, and then rely on the reward signal to test and fine-tune their ability to do the task beyond this initial hypothesis.&lt;/p&gt;
&lt;p&gt;It is therefore surprising that 'pure RL' approaches are still so common and research on meta-learning and zero-shot learning is still not as prevalent. But we can go further: it is surprising that the basic formulation of RL has not been questioned more, and that the notions of meta-learning and zero-shot learning have not been popularly encoded into its basic equations. Among research that has in fact suggested alternative formulations of RL, perhaps the most relevant piece is DeepMind's 2015 paper &lt;a href="https://deepmind.com/research/publications/universal-value-function-approximators/"&gt;&amp;quot;Universal Value Function Approximators&amp;quot;&lt;/a&gt;, which generalized the idea of 'General value functions' introduced by Richard Sutton (by far the most influential researcher in RL) and collaborators in 2011. DeepMind's abstract summarizes the idea well:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;quot;Value functions are a core component of [RL] systems. The main idea is&lt;br&gt;
to to construct a single function approximator V(s; θ) that estimates the long-term reward from any state s, using parameters θ. In this paper we introduce universal value function approximators (UVFAs) V(s, g; θ) that generalise not just over states s but also over goals g.&amp;quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure&gt;
     &lt;img src="https://draftin.com:443/images/56244?token=Qu_QgAaeVEJ7ZRayEdu_W8Ob8J56ct7fHuzi_SABK0U-98ER98Yfm8tJSy3yTrDIpRcIyKp467cg0BEfcxnJ0eo" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;This UVFA idea put to practice. From &lt;a href="https://deepmind.com/research/publications/universal-value-function-approximators/"&gt;&lt;b&gt;"Universal Value Function Approximators".&lt;/b&gt;&lt;/a&gt;
     &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;So, here is a rigorous, mathematical formulation of RL that treats goals (the high-level objective of the skill to be learned, which should yield good rewards) as a fundamental and necessary input rather than something to be discovered from just the reward signal. This places meta-learning and zero-shot learning in the core of the formulation of RL. It has been 3 years since this was published, and how many papers have cited it since? &lt;strong&gt;56&lt;/strong&gt;. A tiny fraction of all papers published in RL; for context,  DeepMind's  &amp;quot;Human-level control through deep RL&amp;quot; was also published in 2015 and as of now has &lt;strong&gt;2319&lt;/strong&gt; citations, and their 2016 &amp;quot;Mastering the game of Go with deep neural networks and tree search&amp;quot; has &lt;strong&gt;2286&lt;/strong&gt; citations according to Google Scholar.&lt;/p&gt;
&lt;p&gt;So, work is definitely being done towards this notion of incorporating meta learning and zero-shot learning with RL. But as shown by the aforementioned citation counts, this research direction is still relatively obscure. Here is the key question here: &lt;strong&gt;why is RL that incorporates meta-learning and/or zero-shot learning, as was formalized by DeepMind's work, not the default? Why is there so little work being done in this direction, and why is there not more excitement about the work that is being done?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Well, to some extent the answer is obvious: it's hard. AI research tends to tackle isolated, well defined problems in order to make progress on them, and there is less work on learning that strays from pure RL precisely because it is harder to define. But, this answer is not satisfactory: deep learning has enabled researchers to create more and more hybrid approaches, such as models that contain &lt;a href="https://cs.stanford.edu/people/karpathy/deepimagesent/"&gt;both Natural Language Processing and Computer Vision&lt;/a&gt; or for that matter &lt;a href="https://www.alphagomovie.com/"&gt;the original AlphaGo's approach of combining both classic techniques and Deep-Learning for playing Go extremely well&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="recentworkonmetalearningzeroshotlearningandrl"&gt;Recent Work On Meta-Learning/Zero-Shot Learning and RL&lt;/h3&gt;
&lt;p&gt;So we return to our original question: does pure RL make sense? And here, based on all that buildup, we get out answer:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;No, pure RL does not make sense; motivated by the board game allegory, we should reconsider the basic formulation of RL along the lines of DeepMind's Universal Value Function idea and double down on the already ongoing research that is implicitly doing just that through meta-learning, zero-shot learning, and more.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Much, if not most, of modern RL research still builds on pure RL approaches that leverage only the reward signal or model-based pure RL that only leverage an environment model and the reward signal. Not only that, but the majority of attention is still given to such work, with the already discussed &lt;a href="https://deepmind.com/blog/alphago-zero-learning-scratch/"&gt;AlphaGo Zero&lt;/a&gt; receiving more attention and praise than most recent AI work. The paper in which it was introduced &lt;a href="https://www.nature.com/articles/nature24270?sf123103138=1"&gt;&amp;quot;Mastering the game of Go without human knowledge&amp;quot;&lt;/a&gt;, was published just last year and already has &lt;strong&gt;144&lt;/strong&gt; citations; DeepMind's Universal Value Function paper has been published for thrice longer and has about a third of the citations with just &lt;strong&gt;56&lt;/strong&gt;. But among those 56 citations is some &lt;strong&gt;very&lt;/strong&gt; exciting work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1707.01495"&gt;&amp;quot;Hindsight Experience Replay&amp;quot;&lt;/a&gt; - &amp;quot;Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. &amp;quot;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1706.05064"&gt;&amp;quot;Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning&amp;quot;&lt;/a&gt; - &amp;quot;As a step towards developing zero-shot task generalization capabilities in RL, we introduce a new RL problem where the&lt;br&gt;
agent should learn to execute sequences of instructions after learning useful skills that solve subtasks. &amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
     &lt;img src="https://draftin.com:443/images/56150?token=DwYtnyKpnMfTCj9-O7JWfTHcBxxVE4rHnLGxM4icju8hG24FKc0psXheMgkos09wKWxs1br2-MWd6CYqRpfeAqQ" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;From &lt;a href="https://arxiv.org/pdf/1706.05064"&gt;&lt;b&gt;"Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning".&lt;/b&gt;&lt;/a&gt;
     &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1707.03938"&gt;&amp;quot;Representation Learning for Grounded Spatial Reasoning&amp;quot;&lt;/a&gt;  - &amp;quot;We consider the task of spatial reasoning in a simulated environment, where an agent can act and receive rewards. The proposed model learns a representation of the world steered by&lt;br&gt;
instruction text.&amp;quot;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1708.00133"&gt;&amp;quot;Deep Transfer in Reinforcement Learning by Language Grounding&amp;quot;&lt;/a&gt; - &amp;quot;In this paper, we explore the utilization of natural language to drive transfer for reinforcement learning (RL). Despite the wide-spread application of deep RL techniques, learning generalized policy representations that work across domains remains a challenging problem. We demonstrate that textual descriptions of environments provide a compact intermediate channel to facilitate effective policy transfer. &amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
     &lt;img src="https://draftin.com:443/images/56278?token=UIvgiiOu2jZrVIaGhSpbtIs2pWC0U5H_Z0kjR9UZybKW3YY5wsgc2hK340br9mrmjnQ45AAfYUFR6UHLkXqo1jU" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;From &lt;a href="https://arxiv.org/abs/1708.00133"&gt;&lt;b&gt;"Deep Transfer in Reinforcement Learning by Language Grounding".&lt;/b&gt;&lt;/a&gt;
     &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1705.09045"&gt;&amp;quot;Cross-Domain Perceptual Reward Functions&amp;quot;&lt;/a&gt; - &amp;quot;In reinforcement learning, we often define goals by specifying rewards within desirable states. One problem with this approach is that we typically need to redefine the rewards each time the goal changes, which often requires some understanding of the solution in the agents environment. When humans are learning to complete tasks, we regularly utilize alternative sources that guide our understanding of the problem. Such task representations allow one to specify goals on their own terms, thus providing specifications that can be appropriately interpreted across various environments. This motivates our own work, in which we represent goals in environments that are different from the agents.&amp;quot;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1136420&amp;amp;dswid=-7446"&gt;&amp;quot;Learning Goal-Directed Behaviour&amp;quot;&lt;/a&gt; - &amp;quot;Two of the core challenges in Reinforcement Learning are the correct assignment of credits over long periods of time and dealing with sparse rewards. In this thesis we propose a framework based on the notions of goals to tackle these problems. &amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
     &lt;img src="https://draftin.com:443/images/56283?token=TX_GZue3Yb992mLpJzoEM_TK-jqAStj80SxJcwyPxTTMh5peDZkSpnOW-Voxbh_dSXsD8lpTHEJgqa0-fBkxZ7c" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;From &lt;a href="http://www.diva-portal.org/smash/record.jsf?pid=diva2%3A1136420&amp;dswid=-7446"&gt;&lt;b&gt;"Learning Goal-Directed Behaviour"&lt;/b&gt;&lt;/a&gt;
     &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Now that's exciting! And, all these goal-specification/hybrid meta/zero/one shot approaches are arguably just the most obvious of directions to pursue for more human-inspired AI methods. Possibly even more exciting is the recent swell of work exploring intrinsic motivation and curiosity-driven exploration for learning (often motivated, I should mention, by the way babies learn):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1803.03835"&gt;&amp;quot;Kickstarting Deep Reinforcement Learning&amp;quot;&lt;/a&gt;: &amp;quot;We present a method for using previously-trained 'teacher' agents to kickstart the training of a new 'student' agent. To this end, we leverage ideas from policy distillation and population based training. Our method places no constraints on the architecture of the teacher or student agents, and it regulates itself to allow the students to surpass their teachers in performance.&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
     &lt;img src="https://draftin.com:443/images/56572?token=h_ym1nKlFVT0Q_7JqFQTt3ITFALXYqje8Mkgwu33P70Mgu1u8J8KZAPP5IVgcSt6OCImD_jAcRhVEvaqriQK778" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;From &lt;a href="https://arxiv.org/abs/1803.03835"&gt;&lt;b&gt;"Kickstarting Deep Reinforcement Learning"&lt;/b&gt;&lt;/a&gt;
     &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1703.01732"&gt;&amp;quot;Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning&amp;quot;&lt;/a&gt; - &amp;quot;One of our approximations results in using surprisal as intrinsic motivation, while the other gives the k-step learning progress. We show that our incentives enable agents to succeed in a wide range of environments with high-dimensional state spaces and very sparse rewards, including continuous control tasks and games in the Atari RAM domain, outperforming several other heuristic exploration techniques.&amp;quot;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1802.07245"&gt;&amp;quot;Meta-Reinforcement Learning of Structured Exploration Strategies&amp;quot;&lt;/a&gt;: &amp;quot;Exploration is a fundamental challenge in reinforcement learning (RL). Many of the current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we explore how prior tasks can inform an agent about how to explore effectively in new situations. &amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
     &lt;img src="https://draftin.com:443/images/56508?token=jxHaa7w8b3oaXZEIvusOaQhygW3_5_paFWeSctB5UxCyHoiaHltu2haGfwRvst30cLg6e-X7zm5PwhZgYLmK3gE" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;From &lt;a href="https://arxiv.org/abs/1802.07245"&gt;&lt;b&gt;"Meta-Reinforcement Learning of Structured Exploration Strategies"&lt;/b&gt;&lt;/a&gt;
     &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://sites.google.com/view/adversarial-irl"&gt;&amp;quot;Learning Robust Rewards with Adversarial Inverse Reinforcement Learning&amp;quot;&lt;/a&gt; : &amp;quot; Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose AIRL, a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. &amp;quot;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://pathak22.github.io/noreward-rl/"&gt;&amp;quot;Curiosity-driven Exploration by Self-supervised Prediction&amp;quot;&lt;/a&gt; - &amp;quot;In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. &amp;quot;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1802.10567"&gt;&amp;quot;Learning by Playing - Solving Sparse Reward Tasks from Scratch&amp;quot;&lt;/a&gt; - &amp;quot;We propose Scheduled Auxiliary Control (SAC-X), a new learning paradigm in the context of RL. SAC-X enables learning of complex behaviors - from scratch - in the presence of multiple sparse reward signals. To this end, the agent is equipped with a set of general auxiliary tasks, that it attempts to learn simultaneously via off-policy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary policies allows the agent to efficiently explore its environment - enabling it to excel at sparse reward RL.&amp;quot;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1802.07442"&gt;&amp;quot;Learning to Play with Intrinsically-Motivated Self-Aware Agents&amp;quot;&lt;/a&gt; - &amp;quot;Infants are experts at playing, with an amazing ability to generate novel structured behaviors in unstructured environments that lack clear extrinsic reward signals. We seek to mathematically formalize these abilities using a neural network that implements curiosity-driven intrinsic motivation. Using a simple but ecologically naturalistic simulated environment in which an agent can move and interact with objects it sees, we propose a &amp;quot;world-model&amp;quot; network that learns to predict the dynamic consequences of the agent's actions. Simultaneously, we train a separate explicit &amp;quot;self-model&amp;quot; that allows the agent to track the error map of its own world-model, and then uses the self-model to adversarially challenge the developing world-model. We demonstrate that this policy causes the agent to explore novel and informative interactions with its environment, leading to the generation of a spectrum of complex behaviors, including ego-motion prediction, object attention, and object gathering.&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
     &lt;img src="https://draftin.com:443/images/56311?token=F4H1auPTss8GS57EFjIEEptsxqwZuM2qdsJaY5d0S4kZ77mv4L9syy4vSC1Jimld8eyZ2QYbois3-8p_WzEXuHU" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;From &lt;a href="https://arxiv.org/abs/1802.07442"&gt;&lt;b&gt;"Learning to Play with Intrinsically-Motivated Self-Aware Agents"&lt;/b&gt;&lt;/a&gt;
     &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1803.10760"&gt;&amp;quot;Unsupervised Predictive Memory in a Goal-Directed Agent&amp;quot;&lt;/a&gt;: &amp;quot;Animals execute goal-directed behaviours despite the limited range and scope of their sensors. To cope, they explore environments and store memories maintaining estimates of important information that is not presently available. Recently, progress has been made with artificial intelligence (AI) agents that learn to perform tasks from sensory input, even at a human level, by merging reinforcement learning (RL) algorithms with deep neural networks, and the excitement surrounding these results has led to the pursuit of related ideas as explanations of non-human animal learning. However, we demonstrate that contemporary RL algorithms struggle to solve simple tasks when enough information is concealed from the sensors of the agent, a property called &amp;quot;partial observability&amp;quot;. An obvious requirement for handling partially observed tasks is access to extensive memory, but we show memory is not enough; it is critical that the right information be stored in the right format. We develop a model, the Memory, RL, and Inference Network (MERLIN), in which memory formation is guided by a process of predictive modeling.&amp;quot;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/abs/1803.10122"&gt;&amp;quot;World Models&amp;quot;&lt;/a&gt;: &amp;quot;We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. &amp;quot;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
     &lt;img src="https://draftin.com:443/images/56665?token=tsKO2gG3lajMDKwGOSspNeA0i5RvITiwsehy8HQFOAY5IkCDPPhblxJFS2651BxeUNflIftRKgrpYV0_EEzVCJ8" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;From &lt;a href="https://arxiv.org/abs/1803.10122"&gt;&lt;b&gt;"World Models"&lt;/b&gt;&lt;/a&gt;
     &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;And we can even go beyond taking inspiration from human learning: we can directly study it. In fact, both older and cutting edge neuroscience research directly suggest human and animal learning can be modeled as reinforcement learning mixed with meta-learning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://pdfs.semanticscholar.org/6b3f/41d409d7e2031ce55b2a7e85a9a621ae39fa.pdf"&gt;&amp;quot;Meta-learning in Reinforcement Learning&amp;quot;&lt;/a&gt;: &amp;quot;Meta-parameters in reinforcement learning should be tuned to the environmental dynamics and the animal performance. Here, we propose&lt;br&gt;
a biologically plausible meta-reinforcement learning algorithm for tuning these meta-parameters in a dynamic, adaptive manner. We tested our algorithm in both a simulation of a Markov decision task and in a non-linear control task. Our results show that the algorithm robustly finds appropriate meta-parameter values, and controls the meta-parameter time course, in both static and dynamic environments. We suggest that the phasic and tonic components of dopamine neuron firing can encode the signal required for meta-learning of reinforcement learning&amp;quot;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.biorxiv.org/content/early/2018/04/06/295964"&gt;&amp;quot;Prefrontal cortex as a meta-reinforcement learning system&amp;quot;&lt;/a&gt;:&lt;br&gt;
&amp;quot;Specifically, by adjusting the connection weights within the prefrontal network, DA-based RL creates a second RL algorithm, implemented entirely in the prefrontal network’s activation dynamics. This new learning algorithm is independent of the original one, and differs in ways that are suited to the task environment. Crucially, the emergent algorithm is a full-fledged RL procedure: It copes with the exploration-exploitation tradeoff, maintains a representation of the value function, and progressively adjusts the action policy. In view of this point, and in recognition of some precursor research, we refer to the overall effect as &lt;strong&gt;meta-reinforcement learning&lt;/strong&gt;.&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
     &lt;img src="https://draftin.com:443/images/56806?token=AYG-Yr1QKjcyh-SIjV105NYNur_RABpHqQpHopnNAn7GF8vCPYZ0Ds7_25YsIc_55JN2ksDreSB5hrdRWdKOc3Q" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;From &lt;a href="https://www.biorxiv.org/content/early/2018/04/06/295964"&gt;&lt;b&gt;"Prefrontal cortex as a meta-reinforcement learning system
"&lt;/b&gt;&lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Incredible!&lt;/p&gt;
&lt;p&gt;Now, let us finally get to a summary and the conclusion of this very long essay:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The fundamental formulation of Reinforcement Learning seems to not make sense for many AI problems, chiefly due to its implied assumptions of 'starting from scratch' and learning driven only by a low-level reward signal and potentially a model. As shown by the many papers cited here, going beyond 'starting from scratch' does not necessiate Deep Blue style hand coded heuristics and expert-system style rigid rules. Therefore, the AI research community should broadly double down on such research and 'meta-reinforcement learning' methods. That is, methods to empower AI agents to learn better through high level instructions, accumulated experience, examples of what it should learn to do, learning a model of the world intrinsic motivation, and more. Perhaps, even a fundamental reconsideration of the basic formulation of RL (as with DeepMind's UVFA RL abstraction) is worth considering.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So, let's end on this positive and optimistic note: the time is ripe for the AI community to embrace work such as the above and move beyond pure RL with more human-inspired learning approaches. It's possible the community is actually already doing this, actually; if so, I am merely making an explicit case in support of the trend. Based on the board-game allegory alone, it seems reasonable to claim that AI techniques must move towards this and away from pure RL in the long term. Which not to say that work on pure RL techniques and applications should immediately stop, but rather that the work should be seen as useful insofar as it is complementary to non pure RL methods and as long as we remain cognizant of pure RL's inherent limitations. If nothing else, methods based on meta learning, zero/few shot learning, transfer learning, and in particular hybrids of all of these should become the default rather than the exception. It just... makes sense. And as a young researcher about to embark on my PhD, I know I for one am willing to bet my most precious resource — my time — on it.&lt;/p&gt;
&lt;figure&gt;
     &lt;img src="https://draftin.com:443/images/56291?token=M_Ka-zJ4QACiB-oPM8iq2n7CV6-MyfgmgatOtyviXdvSXUmxgRuhUpgd-tcIZX73G_ma3jtDNNzpJjv_K49HJoo" alt="Reinforcement Learning is Dead, Long Live Meta-Reinforcement Learning!"&gt;
     &lt;figcaption&gt;The future of RL? &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;Andrey Kurenkov is a graduate student affiliated with the Stanford Vision Lab, and lead editor of &lt;a href="http://www.skynettoday.com/"&gt;Skynet Today&lt;/a&gt;. These opinions are solely his.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;</content:encoded></item><item><title>Learning From Humans: Inverse RL &amp; Apprenticeship Learning</title><description>&lt;div class="kg-card-markdown"&gt;&lt;p&gt;One of the goals of AI research is to teach machines how to do the same things people do, but better.&lt;/p&gt;
&lt;p&gt;In the early 2000s, this meant focusing on problems like &lt;a href="http://www.youtube.com/watch?v=M-QUkgk3HyE"&gt;flying helicopters&lt;/a&gt; and &lt;a href="http://www.youtube.com/watch?v=aqCmX5dMYHg"&gt;walking up flights of stairs&lt;/a&gt;. Now, these problems have been solved, and we’ve achieved near-human&lt;/p&gt;&lt;/div&gt;</description><link>http://localhost:2368/learning-from-humans-part-1-inverse-rl-apprenticeship-learning/</link><guid isPermaLink="false">5ad18868f092d13d8a5762ec</guid><category>Overviews</category><dc:creator>Gary Gradient</dc:creator><pubDate>Sat, 17 Mar 2018 01:14:54 GMT</pubDate><media:content url="http://localhost:2368/content/images/2018/03/DSC_0709.jpg" medium="image"/><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;img src="http://localhost:2368/content/images/2018/03/DSC_0709.jpg" alt="Learning From Humans: Inverse RL &amp; Apprenticeship Learning"&gt;&lt;p&gt;One of the goals of AI research is to teach machines how to do the same things people do, but better.&lt;/p&gt;
&lt;p&gt;In the early 2000s, this meant focusing on problems like &lt;a href="http://www.youtube.com/watch?v=M-QUkgk3HyE"&gt;flying helicopters&lt;/a&gt; and &lt;a href="http://www.youtube.com/watch?v=aqCmX5dMYHg"&gt;walking up flights of stairs&lt;/a&gt;. Now, these problems have been solved, and we’ve achieved near-human performance on harder problems like recognizing images and playing simple Atari games.&lt;/p&gt;
&lt;p&gt;However, there’s still a massive list of problems where humans outperform machines. Our advantage lies in solving problems that aren’t as well defined, like judging a well-executed backflip, cleaning a room while minimizing side effects, and perhaps the most human problem of them all, “reasoning about people’s values.” To solve these problems, machines need information about the world as well as a way to learn about the people within it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How can machines learn from humans?&lt;/strong&gt; We’ll be splitting this big question into three sub-questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How can an agent learn a policy that performs as well as an expert policy?&lt;/li&gt;
&lt;li&gt;How can we scale the rate at which agents are able to learn from people?&lt;/li&gt;
&lt;li&gt;How do we deal with the obvious fact that people do not behave optimally?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To answer our first question, we’ll start this series off with an overview of some of the foundational work on inverse reinforcement learning, apprenticeship learning, and imitation learning. The unifying thread between these techniques is that they all provide a way of integrating information from human experts into reinforcement learning methods, achieving impressive performance in driving simulators and autonomous helicopter flight tests, and gesturing at some of the key problems that later work will attempt to solve.&lt;/p&gt;
&lt;h1 id="inversereinforcementlearning"&gt;Inverse Reinforcement Learning&lt;/h1&gt;
&lt;br&gt;
&lt;h2 id="problem"&gt;Problem&lt;/h2&gt;
&lt;p&gt;In a traditional RL setting, the goal is to learn a decision process which maximizes some predefined reward function. Inverse reinforcement learning (IRL), as described by Andrew Ng and Stuart Russell in 2000, flips the problem and instead attempts to extract the reward function from the observed behavior of an agent.&lt;/p&gt;
&lt;p&gt;Consider the task of autonomous driving. A naive approach would be to create a reward function that captures the desired behavior of a driver, like stopping at red lights, staying off the sidewalk, avoiding pedestrians, etc. Unfortunately, this would require both an exhaustive list of every feature we’d want to consider as well as a list of weights describing how important each feature is (imagine having to decide exactly how much more important pedestrians are than stop signs).&lt;/p&gt;
&lt;p&gt;Instead, IRL takes a set of human-generated driving data and extracts an approximation of the reward function that the person is operating under. In doing this, all of the information necessary for solving a problem is essentially encoded within the reward function. As Ng and Russell put it, &lt;strong&gt;“the reward function, rather than the policy, is the most succinct, robust, and transferable definition of the task,”&lt;/strong&gt; since the problem of finding the right policy is already solvable with traditional RL methods.&lt;/p&gt;
&lt;p&gt;For our self-driving car example, we’d be using human driving data to directly learn what the right feature weights are for the reward. Since the task is described by the reward function, we do not even need to know what the specifics of the human policy are, so long as we have the right reward function to optimize for. In the general case, IRL can be seen as a method for leveraging expert knowledge to convert a task description into a compact reward function.&lt;/p&gt;
&lt;p&gt;The main problem when converting a complex task into a simple reward function is that a given policy may be optimal for many different reward functions. That is, even though we have the actions from an expert, there exist many different reward functions (some of them degenerate) that the expert might be attempting to maximize. For example, all policies are optimal for the reward function that is zero everywhere, so this reward function is always a possible solution to the IRL problem. Ideally, we would want a reward function that captures meaningful information about the task and is able to clearly differentiate between desired and undesired policies.&lt;/p&gt;
&lt;p&gt;To solve this, Ng and Russell formulate inverse reinforcement learning as an optimization problem where we seek to maximize certain properties of the reward function, under the constraint that the given expert policy is still the optimal solution for any reward function we consider.&lt;/p&gt;
&lt;br&gt;
&lt;h2 id="algorithms"&gt;Algorithms&lt;/h2&gt;
&lt;br&gt;
&lt;h3 id="inversereinforcementlearninginfinitestatespaces"&gt;Inverse Reinforcement Learning in Finite State Spaces&lt;/h3&gt;
&lt;p&gt;First, consider the set of optimal policies for a Markov Decision Process (MDP) with a finite state space $S$, set of actions $A$, and transition probability matrices $P_a$ for each action. We say that a policy $\pi$ given by $\pi(s) = a_1$ is optimal if and only if for all other actions $a$, the reward vector $R$ (which lists the rewards for each possible state) satisfies:&lt;/p&gt;
&lt;p&gt;$$ (P_{a1}−P_a)(I − \gamma P_{a_1})^{−1} R \succeq 0$$&lt;/p&gt;
&lt;p&gt;The proof from Ng’s paper is given in the Appendix, but the importance of the theorem is that &lt;strong&gt;it allows us to use linear programming to find a feasible point of the constraints in this equation&lt;/strong&gt;. Ng and Russell then formulate IRL as an optimization problem using the following heuristics for what makes a reward function “fit” the expert data well:&lt;/p&gt;
&lt;p&gt;Maximize “the difference between the quality of the optimal action and the quality of the next best action” (subject to a bound on the magnitude of the reward, to prevent arbitrarily large differences). The intuition here is that we want a reward function that clearly distinguishes the optimal policy from other possible policies.&lt;br&gt;
Minimize the size of the rewards in the reward function/vector. The intuition is that using small rewards corresponds to some kind of “simplicity” prior. Ng and Russell choose the L1 norm with an adjustable penalty coefficient, which comes with the nice property of the reward vector being non-zero in only a few states.&lt;/p&gt;
&lt;p&gt;While these constraints are intuitive, the authors note that other useful constraints may exist. In particular, later work, such as Maximum Entropy Inverse Reinforcement Learning (Ziebart et. al.), and Bayesian Inverse Reinforcement Learning (Ramachandran et. al.), are two examples of different criteria that have been used to achieve impressive results.&lt;/p&gt;
&lt;br&gt;
&lt;h3 id="inversereinforcementlearningfromsampledtrajectories"&gt;Inverse Reinforcement Learning from Sampled Trajectories&lt;/h3&gt;
&lt;p&gt;Ng and Russell also describe IRL algorithms for cases where, instead of a full optimal policy, we can only sample trajectories from an optimal policy. In applied cases, especially those dealing with human expert data, this situation is a bit more common.&lt;/p&gt;
&lt;p&gt;In this formulation of the problem, we replace the reward vector that we used for finite state spaces with a linear approximation of the reward function which uses basis functions $\phi_i$ from $\mathbb{R}^n \rightarrow \mathbb{R}$ to obtain real-valued features $\phi_i(s)$. These features are a way to capture important information from a high-dimensional state space (for instance, instead of storing the location of a car during every time-step, we can store its average speed as a feature). For each feature $\phi_i(s)$ and weight $\alpha_i$ we have:&lt;/p&gt;
&lt;p&gt;$$ R(s) = \alpha_1\phi_1(s) + \alpha_2\phi_2(s) + . . . + \alpha_d\phi_d(s) $$&lt;/p&gt;
&lt;p&gt;and &lt;strong&gt;our objective is to find the best-fitting values for each feature-weight&lt;/strong&gt; $\alpha_i$.&lt;/p&gt;
&lt;p&gt;The general idea behind IRL with sampled trajectories is to &lt;strong&gt;iteratively improve a reward function by comparing the value of the assumed optimal policy with a set of k generated policies&lt;/strong&gt;. We initialize the algorithm by randomly generating weights for the estimated reward function and initialize our set of candidate policies with a randomly generated policy. Then, the key steps are algorithm are to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Estimate the value of our optimal policy for the initial state $\hat{V}^\pi(s_0)$, as well as the value of every generated policy $\hat{V}^{\pi_i}(s_0)$ by taking the average cumulative reward of many randomly sampled trials.&lt;/li&gt;
&lt;li&gt;Generate an estimate of the reward function $R$ by solving a linear programming problem. Specifically, set $\alpha_i$ to maximize the difference between our optimal policy and each of the other $k$ generated policies:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$ \mathrm{maximize~} \sum_{i=1}^k f(\hat{V}^{\pi^*}(s_0) − \hat{V}^{\pi_i}(s_0))$$&lt;br&gt;
$$ \mathrm{with~} f(x)=2x \mathrm{~if~} x &amp;lt; 0 \mathrm{~else~} f(x)=x \mathrm{~to~penalize~valuing~a~policy~higher~than~the~expert}$$&lt;br&gt;
$$ \mathrm{s.t.~} |\alpha_i| \leq 1, ~ i=1, . . ., d $$&lt;/p&gt;
&lt;ol start="4"&gt;
&lt;li&gt;After a large number of iterations, end the algorithm at this step.&lt;/li&gt;
&lt;li&gt;Otherwise, use a standard RL algorithm to find the optimal policy for $R$. This policy may be different from the given optimal policy, since our estimated reward function is not necessarily identical to the reward function we are searching for.&lt;/li&gt;
&lt;li&gt;Add the newly generated policy to the set of $k$ candidate policies, and repeat the procedure.&lt;/li&gt;
&lt;/ol&gt;
&lt;br&gt;
&lt;h2 id="results"&gt;Results&lt;/h2&gt;
&lt;p&gt;When tested on grid-worlds and the mountain car problem, and executed with the correct penalty terms, IRL is able to achieve high levels of similarity with the desired reward function. Remarkably, there was &lt;strong&gt;no statistically significant difference between the reward for the true optimal policy and the reward for the policy generated using IRL’s estimation of the reward function&lt;/strong&gt; when testing on the actual MDP.&lt;/p&gt;
&lt;p&gt;On the practical side, Pieter Abbeel and Andrew Ng combine inverse reinforcement learning with apprenticeship learning to &lt;a href="http://ai.stanford.edu/~pabbeel//irl/"&gt;mimic various driving styles in these self-driving car simulators&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id="apprenticeshiplearning"&gt;Apprenticeship Learning&lt;/h1&gt;
&lt;br&gt;
&lt;h2 id="problem"&gt;Problem&lt;/h2&gt;
&lt;p&gt;One interesting alternative to learning a reward function from an expert is to directly learn a policy that is guaranteed to have comparable performance to the expert policy. This is particularly useful if we have an expert policy that, while not truly optimal, is a satisfactory approximation to optimal behavior. Here, an apprenticeship learning algorithm formulated by Pieter Abbeel and Andrew Ng in 2005, gives us a solution. This algorithm takes an MDP and an approximately optimal “teacher” policy, and then &lt;strong&gt;learns a policy with comparable or better performance to the teacher’s policy using minimal exploration&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This minimal exploration property turns out to be very useful in fragile tasks like autonomous helicopter flight. A traditional reinforcement learning algorithm might start out exploring at random, which would almost certainly lead to a helicopter crash in the first trial. Ideally, we’d be able to use expert data to start with a baseline policy that can be safely improved over time. We would also expect this baseline policy to be significantly better than a randomly initialized policy, which speeds up convergence.&lt;/p&gt;
&lt;br&gt;
&lt;h2 id="algorithm"&gt;Algorithm&lt;/h2&gt;
&lt;p&gt;The main idea behind Abbeel and Ng’s algorithm is to use trials from the expert policy to obtain information about the underlying MDP, then iteratively execute a best estimate of the optimal policy for the actual MDP. Executing a policy also gives us data about the transitions in the environment, which we can then use to improve the accuracy of the estimated MDP. The key steps of the algorithm for a discrete environment are:&lt;/p&gt;
&lt;p&gt;Learning about the MPD with the expert policy:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Run a fixed amount of trials using the expert policy, recording every state-action trajectory.&lt;/li&gt;
&lt;li&gt;Estimate the transition probabilities for every state-action pair using the recorded data via maximum likelihood estimation.&lt;/li&gt;
&lt;li&gt;Estimate the value of the expert policy by averaging the total reward in each trial.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Learning a new policy:&lt;/p&gt;
&lt;ol start="4"&gt;
&lt;li&gt;Learn an optimal policy for the estimated MDP using any standard reinforcement learning algorithm.&lt;/li&gt;
&lt;li&gt;Test the learning policy on the actual environment.&lt;/li&gt;
&lt;li&gt;If performance is not close enough to the value of the expert policy, add the state-action trajectories from this trial to the training set and repeat the procedure to learn a new policy.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The benefit of this method is that &lt;strong&gt;at each stage, the policy being tested is the best estimate for the optimal policy of the system&lt;/strong&gt;. There is diminished exploration, but the core idea of apprenticeship learning is that we can assume the expert policy is already near-optimal.&lt;/p&gt;
&lt;br&gt;
&lt;h2 id="results"&gt;Results&lt;/h2&gt;
&lt;br&gt;
&lt;h3 id="theoreticalresults"&gt;Theoretical Results&lt;/h3&gt;
&lt;p&gt;One of the key results in apprenticeship learning is that &lt;strong&gt;the reward of the learned policy, evaluated in the true MDP, is near-optimal compared to the teacher’s policy&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The main idea in the proof is that the estimated model remains accurate for evaluating the teacher’s policy, even after data from the estimated policy has been added. Another important point is that when estimating any state-action pair in the dynamics of the system, either:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A state-action pair will be visited a “small” number of times, and therefore will not contribute significantly to the overall estimated reward.&lt;/li&gt;
&lt;li&gt;A state-action pair will be frequently visited by the learned policy, and of these visits, only a “small” number of these must occur until the state-action pair is accurately modeled.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Abbeel and Ng formalize these notions and produce an interesting theoretical proof of the near-optimality of the estimated policy relative to the teacher’s policy, which we will not cover, but that can be seen in the original paper.&lt;/p&gt;
&lt;br&gt;
&lt;h3 id="practicalresults"&gt;Practical Results&lt;/h3&gt;
&lt;p&gt;Coates, Abbeel, and Ng have a particularly amazing application of apprenticeship learning in successfully &lt;a href="https://youtu.be/VCdxqn0fcnE"&gt;teaching a helicopter to autonomously fly a complete aerobatic airshow&lt;/a&gt;.&lt;/p&gt;
&lt;br&gt;
&lt;h1 id="furtherwork"&gt;Further Work&lt;/h1&gt;
&lt;p&gt;The foundational methods of inverse reinforcement learning and apprenticeship learning, as well as the similar method of imitation learning are able to achieve their results by leveraging information gleaned from a policy executed by a human expert. However, in the long run, the goal is for machine learning systems to &lt;strong&gt;learn from a wide range of human data and perform tasks that are beyond the abilities of human experts&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In particular, when discussing potential avenues for further research in inverse reinforcement learning, Andrew Ng and Stuart Russell highlight &lt;strong&gt;suboptimality in the agent’s actions&lt;/strong&gt; as an important problem. In the context of human experts, it’s clear that while experts may be able to provide a baseline level of performance that RL algorithms could learn from, it will rarely be the case that the expert policy given is actually the optimal one for the environment. The next problem to solve in inverse reinforcement learning would be to extract a reward function without the assumption that the human policy is optimal. In part 3 of this series, when we consider approaches to learning the reward functions of even more complex tasks, we’ll be examining a few methods of factoring uncertainty into the estimation of the true reward function as a way of solving this problem of suboptimality.&lt;/p&gt;
&lt;p&gt;Another remarkable extension to inverse reinforcement learning is one that does not require an optimal policy, and instead considers &lt;strong&gt;learning behaviors that agents can identify, but not necessarily demonstrate&lt;/strong&gt;, meaning that only a classifier is needed. For instance, it’s easy for people to identify whether an agent in a physics simulator is running correctly, but almost impossible to manually specify the right control sequence given the degrees of freedom in a robotics simulator. This extension would allow reinforcement learning systems to achieve human-approved performance without the need for an expert policy to imitate.&lt;/p&gt;
&lt;p&gt;Finally, while the authors also describe the use of linear function approximation to apply IRL to large state spaces, the challenge in going from 2000 to 2018 is to &lt;strong&gt;scale up these methods to work with deep learning&lt;/strong&gt;. We’ll be covering a few novel methods of doing just this in the next entry in this series: Deep learning from humans.&lt;/p&gt;
&lt;br&gt;
&lt;h1 id="appendix"&gt;Appendix&lt;/h1&gt;
&lt;p&gt;Proof of Ng’s IRL Theorem:&lt;br&gt;
&lt;img src="http://localhost:2368/content/images/2018/03/Screenshot-2018-03-16-17.02.38.png" alt="Learning From Humans: Inverse RL &amp; Apprenticeship Learning" style="width: 400px;"&gt;&lt;/p&gt;
&lt;br&gt;
&lt;h1 id="references"&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf"&gt;Abbeel, Pieter and Ng, Andrew Y. (2004) Apprenticeship learning via inverse reinforcement learning.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://dl.acm.org/citation.cfm?id=1102352"&gt;Abbeel, Pieter and Ng, Andrew Y. (2005) Exploration and apprenticeship learning in reinforcement learning.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/pdf/1606.06565.pdf"&gt;Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dan Mané. (2016) Concrete Problems in AI Safety.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://dl.acm.org/citation.cfm?id=1538812"&gt;Coates, Adam and Abbeel, Pieter and Ng, Andrew (2009). Apprenticeship learning for helicopter control.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ai.stanford.edu/~ang/papers/icml00-irl.pdf"&gt;Ng, Andrew and Russell, Stuart. (2000) Algorithms for inverse reinforcement learning.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.aaai.org/Papers/IJCAI/2007/IJCAI07-416.pdf"&gt;Ramachandran, Deepak and Amir, Eyal. (2007) Bayesian inverse reinforcement learning.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf"&gt;Brian D. Ziebart, Andrew Maas, J.Andrew Bagnell, and Anind K. Dey. (2008) Maximum Entropy Inverse Reinforcement Learning.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;</content:encoded></item><item><title>What does it actually mean for AI to be racist?</title><description>Once you try to understand bias mathematically, you start noticing that very few people actually can actually tell you what it is.</description><link>http://localhost:2368/what-does-it-actually-mean-for-a-robot-to-be-racist/</link><guid isPermaLink="false">5ad18868f092d13d8a5762e1</guid><category>Perspectives</category><dc:creator>Gary Gradient</dc:creator><pubDate>Fri, 09 Mar 2018 19:15:00 GMT</pubDate><media:content url="http://localhost:2368/content/images/2018/03/Redlining-map-all-Chicago-all-four-zones.jpg" medium="image"/><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;ul&gt;
&lt;li&gt;Aylin Caliskan, Joanna J. Bryson, Arvind Naranayan:&lt;br&gt;
&lt;em&gt;Semantics derived automatically from language corpora contain human-like biases.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, Aziz Huq:&lt;br&gt;
&lt;em&gt;Algorithmic decision making and the cost of fairness.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="ourproblemwithprediction"&gt;Our problem with prediction&lt;/h1&gt;
&lt;img src="http://localhost:2368/content/images/2018/03/Redlining-map-all-Chicago-all-four-zones.jpg" alt="What does it actually mean for AI to be racist?"&gt;&lt;p&gt;In 2011, researchers working with the LA Police Department's Foothill Division used statistical algorithms to reduce the area's property crime rates by &lt;a href="http://leb.fbi.gov/articles/featured-articles/predictive-policing-using-technology-to-reduce-crime"&gt;a staggering 12 percent&lt;/a&gt;. The crime prediction model they provided was twice as accurate as the LAPD's crime analysts, and it had previously been tested—with similar success—in the smaller city of Santa  Cruz. It was the result of a years-long effort by LAPD police chief William Bratton, who had long advocated for the use of large datasets and predictive analytics in policing.&lt;/p&gt;
&lt;p&gt;By 2012, &lt;a href="http://www.policeforum.org/assets/docs/Free_Online_Documents/Leadership/future%20trends%20in%20policing%202014.pdf"&gt;70 percent&lt;/a&gt; of US police departments were ready to follow the LAPD's lead. It was clear to everyone that the future of law enforcement was data-driven: by predicting sites of criminal activity, machine learning really could stop crimes before they happened. TIME magazine even named predictive policing one of the &amp;quot;best inventions of the year&amp;quot;.&lt;/p&gt;
&lt;p&gt;But thing have changed in the last seven years. &lt;em&gt;Predictive policing&lt;/em&gt; has become a dirty word, and the authors of an &lt;a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2016.00960.x/pdf"&gt;influential study on the topic&lt;/a&gt; tell us why:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Predictive policing software is designed to learn and reproduce patterns in data, but if biased data is used to train these predictive models, the models will reproduce and in some cases amplify those same biases. &lt;em&gt;At best, this renders the predictive models ineffective. At worst, it results in discriminatory policing.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Of course, this is an academic study. You'll get a much better sense of the general attitude toward predictive policing if you look at the words of Black Lives Matter activist &lt;a href="http://www.nydailynews.com/new-york/king-predictive-policing-technological-racism-article-1.2425028"&gt;Shaun King&lt;/a&gt;,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In essence, it's not predicting who will sell drugs and where they will sell it, as much as it is actually predicting where a certain race of people may sell or purchase drugs. &lt;em&gt;It's technological racism at its finest.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Or, for that matter, blogger-journalist &lt;a href="https://boingboing.net/2016/08/18/predictive-policing-predicts-p.html"&gt;Cory Doctorow&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Moreover...&lt;em&gt;racist training data produces racist predictive models&lt;/em&gt;, which allow racist institutions to claim to be undertaking objective and neutral measures while continuing to be totally racist.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So much for utopia. Something's gone horribly wrong, and it's not just predictive policing: across the board, our perception of algorithms has changed. The public, along with large parts of the technical community, has come to see them as tools of oppression, rather than objective arbiters of the truth. Resigned to the &amp;quot;fact&amp;quot; that robots are incorrigibly racist, classist, and sexist, we have become skeptical of applying machine learning to any domain more important than targeting ads, recommending movies, or laying off radiologists.&lt;/p&gt;
&lt;p&gt;But how productive is this cynicism, really? If humans are the source of our biased data, why should they be trusted more than algorithms? And if humans have the potential to be unbiased, why can't algorithms? Remember that the LAPD's algorithm was twice as accurate as human analysts. However biased they may be right now, statistical models are still our most powerful tools for understanding the world. We can't just give them up for some vague misgivings.&lt;/p&gt;
&lt;p&gt;This article aims to deeply examine the issue of bias in algorithms: where it comes from, how we might define it, and what we can do about it.  I want to ground the topic in a clear understanding of both its moral and the technical aspects. In the process, we'll confront a number of misconceptions about bias, all of which stem from the use of sensationalist snarl words like &amp;quot;racist&amp;quot; to describe algorithms and statistical models. My goal is to convince you that this usage...&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;discourages nuanced moral reasoning by sloppily, often incorrectly, &lt;a href="https://en.wikipedia.org/wiki/Pathetic_fallacy"&gt;projecting human values onto statistical estimators&lt;/a&gt; and judging them through appeals to outrage,&lt;/li&gt;
&lt;li&gt;obscures the critical distinction between two very different kinds of &amp;quot;bias&amp;quot; with radically different causes and solutions, and&lt;/li&gt;
&lt;li&gt;erroneously implies that all of the bias in algorithms can be explained by the biases of individuals.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bias is a serious problem. In some ways, it is even more serious than the media makes it out to be. But once you approach it as a technological problem to be solved rather than an unavoidable flaw of algorithmic reasoning, you start to notice something suspicious: not many people can give you a strict definition of what a biased algorithm is. And there's a reason for that.&lt;/p&gt;
&lt;h1 id="algorithmsarenthumans"&gt;Algorithms aren't humans&lt;/h1&gt;
&lt;p&gt;There has been no shortage of popular media coverage on the subject of biased algorithms. Commentators have blasted Google Images for being &lt;a href="https://www.washingtonpost.com/news/morning-mix/wp/2016/06/10/google-faulted-for-racial-bias-in-image-search-results-for-black-teenagers/"&gt;racist&lt;/a&gt;, Google Translate for being &lt;a href="http://www.dailymail.co.uk/sciencetech/article-5136607/Is-Google-Translate-SEXIST.html"&gt;sexist&lt;/a&gt;, and Microsoft’s Twitter chatbot, Tay, for being a &lt;a href="https://www.washingtonpost.com/news/the-intersect/wp/2016/03/24/the-internet-turned-tay-microsofts-fun-millennial-ai-bot-into-a-genocidal-maniac/?utm_term=.6b0cd8d953ad"&gt;genocidal maniac&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The articles are obviously correct that Google Images shouldn't return mug shots on searches for black teenagers, and that Microsoft's Tay shouldn't spout hate speech. In the case of Google Images, this behavior can reinforce racist beliefs; in the case of Tay, it can can cause significant emotional harm. But &lt;em&gt;none of the articles actually provides this justification&lt;/em&gt;. They focus instead on how algorithms can be just as bigoted as humans, glossing over the question of what exactly makes this behavior bigoted.&lt;/p&gt;
&lt;p&gt;There's nothing obviously wrong with this omission. By and large, we feel like we don't need an explicit justification for why certain things are reprehensible when we know what reprehensibility looks like. &lt;em&gt;We can recognize bias in humans; surely we can generalize this to algorithms.&lt;/em&gt; Spewing hate speech is what a genocidal maniac would do, so the Microsoft chatbot is clearly a genocidal maniac. Associating black teenagers with criminality is what an actual racist would do, so Google Images is clearly a racist.&lt;/p&gt;
&lt;p&gt;This is the reasoning behind much of the coverage on algorithmic bias: using rough analogies to project human feelings and motivations onto the algorithm, then judging it by those feelings and motivations as if it were really human. When we see Tay the chatbot learning to produce text strings calling for unspeakable atrocities, our initial impression is that the chatbot actually &amp;quot;wants&amp;quot; those atrocities to happen—that it really is a dangerous &amp;quot;genocidal maniac&amp;quot; and not a generative text model. By interpreting algorithmic outputs as human emotions, journalists sidestep the difficult moral question of actually defining algorithmic fairness, replacing it with the easier question of &amp;quot;is a person who does something roughly analogous to this a bigot?&amp;quot;.&lt;/p&gt;
&lt;p&gt;While this reframing of the question works fairly well in the cases of Google Images and Tay, there are serious problems with it. For example, consider the controversy about &lt;a href="https://www.engadget.com/2017/10/25/googles-sentiment-analysis-api-is-just-as-biased-as-humans/"&gt;supposed&lt;/a&gt; &lt;a href="https://motherboard.vice.com/en_us/article/j5jmj8/google-artificial-intelligence-bias"&gt;homophobia&lt;/a&gt; in Google’s sentiment analysis API.&lt;/p&gt;
&lt;p&gt;The story goes like this: according to the API, the single word &lt;em&gt;gay&lt;/em&gt; expresses a mildly negative attitude, while the single word &lt;em&gt;straight&lt;/em&gt; expresses a neutral attitude. Because of this difference, the authors of the articles conclude, the algorithm &amp;quot;thinks being gay is bad&amp;quot;.&lt;/p&gt;
&lt;p&gt;You might point out here that the sentiment analysis algorithm was built to handle paragraphs and passages, not individual words. You might also point out that sentences like “I’m proud to be gay” map to a “strongly positive” score of 0.7. Damning as these weaknesses might be, the article's real flaw might be that it completely ignores the fact that the algorithm is correct.&lt;/p&gt;
&lt;p&gt;The word &lt;em&gt;gay&lt;/em&gt; is regularly used by some as a pejorative. The word &lt;em&gt;straight&lt;/em&gt; suffers from no such usage. With the highly limited information in a single word, the algorithm can only guess at its connotation. All things considered, at least in the context of our current society, someone who leaves the laconic single-word comment &amp;quot;gay&amp;quot; on a YouTube video probably didn't like it very much. (Someone who leaves the single-word review &amp;quot;straight,&amp;quot; on the other hand, is just being confusing.)&lt;/p&gt;
&lt;p&gt;At first glance, this fits perfectly into the classic narrative that machines learn bias from biased humans. Our society has a history of thinking that being gay is bad. Our algorithm has captured this historical bias in its predictions. Therefore, our first instinct is that we need to correct this &amp;quot;biased&amp;quot; algorithm that has learned from humans that being gay is bad.&lt;/p&gt;
&lt;p&gt;But this instinct—and, in fact, the entire controversy—is based on a fundamentally flawed analogy between humans and algorithms. There is a world of difference between an actual homophobe and an algorithm that notices the word &lt;em&gt;gay&lt;/em&gt; being used with a negative connotation. The mistake of the two articles is assuming that Google's sentiment analysis API outputs repugnant value judgments, rather than likelihood estimates for the one specific task of understanding one specific feature of one specific kind of text. It is hard to imagine a legitimate use case of the sentiment analysis algorithm that would actually affect straight and gay people differently.&lt;/p&gt;
&lt;p&gt;Making the claim that the algorithm &amp;quot;thinks being gay is bad&amp;quot; blurs this difference between algorithms and people, encouraging us to haphazardly hallucinate human motivations and value judgments where there are none. This sloppy, muddleheaded thinking, ubiquitous in discussions of biased algorithms, plays on the anxieties of a public deeply distrustful of Silicon Valley’s motives and of impenetrable “black box” machine learning in general. It opens the door to accusations of bias in virtually any situation where an algorithm produces two unequal numbers vaguely related to some hot-button social issue. And it drowns reasonable discussion in a torrent of vapid, rabble-rousing takes that contribute nothing but outrage to the discourse.&lt;/p&gt;
&lt;p&gt;We can do better. If we're actually to solve the problem of biased algorithms, we need to be able to understand bias not as a vague sense of injustice but as a concrete problem with concrete solutions.&lt;/p&gt;
&lt;h1 id="twobiasesisandought"&gt;Two biases: &lt;em&gt;is&lt;/em&gt; and &lt;em&gt;ought&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/03/p01h2tss.jpg" alt="What does it actually mean for AI to be racist?"&gt;&lt;br&gt;
&lt;em&gt;David Hume, known for stating the is-ought problem&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Virtually every decision-making system that uses machine learning can be separated fairly cleanly into two parts, which I'll call &lt;em&gt;predictive&lt;/em&gt; and &lt;em&gt;prescriptive&lt;/em&gt; &lt;sup&gt;1&lt;/sup&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;em&gt;&lt;strong&gt;predictive&lt;/strong&gt;&lt;/em&gt; component is concerned with accurately modeling the way the world &lt;em&gt;is&lt;/em&gt;. It’s all about training computers to predict a certain ground truth, $y$, based on the observations $x$, and its goal is to model $y$ as well as possible. For instance, I might build a predictive model that takes in data about a neighborhood ($x$) and tries to predict the likelihood ($y$) of crime taking place in that area. The central question of prediction is: &lt;em&gt;what is the state of the world?&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;&lt;strong&gt;prescriptive&lt;/strong&gt;&lt;/em&gt; component is concerned with choosing what action &lt;em&gt;ought&lt;/em&gt; to be taken. As Hume &lt;a href="https://plato.stanford.edu/entries/hume-moral/#io"&gt;observed&lt;/a&gt;, it's impossible to directly convert empirical predictions into an understanding of the way the world ought to be. Continuing the example from above, maybe my algorithm automatically recommends that police officers go to the areas with predicted likelihoods $\hat{y}$ above a certain threshold. The algorithm, or its users, often need to be able to answer the question: &lt;em&gt;given the predicted state of the world, what ought to be done?&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This distinction is apparently trivial. But it illuminates the question of bias in a very important way. If you buy &amp;quot;Hume's Guillotine,&amp;quot; the philosophical argument that you can't derive an &lt;em&gt;ought&lt;/em&gt; from an &lt;em&gt;is&lt;/em&gt;, then these two parts of a coherent algorithm will always be cleanly separate, and any instance of &amp;quot;bias,&amp;quot; whatever it is, must occur in one of these two parts.&lt;/p&gt;
&lt;p&gt;Therefore, all algorithmic bias is either of the &lt;em&gt;is&lt;/em&gt; variety (biased belief) or the &lt;em&gt;ought&lt;/em&gt; variety (biased action).&lt;/p&gt;
&lt;p&gt;In fact, the problem of algorithmic bias is in fact two completely different, almost unrelated problems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;quot;Biased belief&amp;quot; is better known as &lt;em&gt;statistical bias&lt;/em&gt;, and it's fundamentally a statistical problem.&lt;/li&gt;
&lt;li&gt;&amp;quot;Biased action&amp;quot; is what we might call &lt;em&gt;algorithmic unfairness&lt;/em&gt;, and it's fundamentally an ethical problem.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the rest of this section, I'll discuss both types of bias in more detail, and make the case that people have focused far too much on statistical bias when the real culprit is algorithmic unfairness.&lt;/p&gt;
&lt;p&gt;But first, why is this distinction rarely made (especially if the problems are so different)? It's the same deal: treating algorithms as humans. In humans, biased belief and biased action go hand in hand. We don't need separate words for them; both are coextensive within the monolithic concepts of &amp;quot;racism,&amp;quot; &amp;quot;sexism,&amp;quot; and the lot. As long as we keep thinking of algorithms in terms of this monolithic bigotry, we won't be able to tease out even the most basic distinctions between these completely different issues, and we won't be able to get a proper intellectual foothold on “biased” algorithms.&lt;/p&gt;
&lt;h3 id="statisticalbias"&gt;Statistical bias&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Statistical bias&lt;/em&gt; is the average difference between a technique’s approximation of reality and the ground truth. Roughly speaking, it’s the tendency of an algorithm to be &lt;em&gt;consistently factually incorrect in one direction&lt;/em&gt;. Any data scientist worth their salt is acutely aware of the various sources of statistical bias, because it is their responsibility to model the world as well as possible.&lt;/p&gt;
&lt;p&gt;For example, &lt;em&gt;sampling bias&lt;/em&gt; is the source of the statistical bias that is accused of affecting predictive policing algorithms. Suppose I wanted to dispatch police officers to the places where crime was most likely to take place. If my dataset only contains &lt;em&gt;reported&lt;/em&gt; crimes, and if crimes committed by white people are more likely to be reported than crimes committed by black people, my crime estimates for white neighborhoods will be consistently lower than reality.&lt;/p&gt;
&lt;p&gt;Truth be told, statistical bias is not that tricky to solve. It’s often possible to check if your model exhibits statistical bias using real data. &lt;em&gt;Calibration,&lt;/em&gt; for instance, is a typical (although insufficient&lt;sup&gt;2&lt;/sup&gt;) test for statistical bias in which the algorithm ensures that predictions “mean the same thing” for each subgroup. More concretely, if my algorithm marks certain neighborhoods of a city as “rather likely to experience violent crime”, I could experimentally verify that the violent crime rate in white neighborhoods with that label should be similar to the percentage of black neighborhoods with the label. (And violent crime is reported essentially uniformly between races.)&lt;/p&gt;
&lt;p&gt;The real problem in these situations is rarely statistical bias. Not only are data scientists well-equipped to deal with it, they also almost always have strong incentives to eliminate it. For corporations, accurate predictions means a tighter grip on the market and protection from being undercut by a more accurate competitor. For governments, accurate predictions mean more efficient and effective deployment of personnel and resources.&lt;/p&gt;
&lt;h3 id="whenisbiasnotstatisticalbias"&gt;When is &amp;quot;bias&amp;quot; not statistical bias?&lt;/h3&gt;
&lt;p&gt;One of the commonly cited examples of algorithmic bias is &lt;a href="http://science.sciencemag.org/content/356/6334/183"&gt;this paper by Caliskan et al. in Science&lt;/a&gt;. The article is an examination of the social biases implicit in &lt;a href="https://en.wikipedia.org/wiki/Word_embedding"&gt;&lt;em&gt;word embeddings&lt;/em&gt;&lt;/a&gt;, which map English words into a 300-dimensional vector space representing their semantic content. The closer two words are in this space, the closer their estimated meanings.&lt;/p&gt;
&lt;p&gt;Word embeddings are generated by reading millions of texts, so it's unsurprising that the researchers found that the embeddings reflected a huge number of common social biases. Just one of the many examples: male names and pronouns were much closer to concepts like “career” than female ones, which were closer to concepts like &amp;quot;homemaking&amp;quot; and &amp;quot;family&amp;quot;. Sparkling in all of its 300-dimensional glory, this cornucopic display of virtually every social bias imaginable put the paltry sentiment analysis kerfluffle to shame.&lt;/p&gt;
&lt;p&gt;But are these examples of statistical bias? Not really. In both cases, there is a ground truth that both algorithms measure better than an “unbiased” algorithm would.&lt;/p&gt;
&lt;p&gt;The more detailed analysis of word embeddings shows just how “accurate” these algorithms are to reality. As the Caliskan paper notes, the algorithm reproduces certain social inequalities, but in a perversely accurate way. For instance, the paper finds a strong correlation ($r = 0.90$) between the strength of a profession’s linguistic association with the female gender and the percentage of workers in that profession who are women. And the strength of the model’s prejudices are also closely tied to the strength of human social biases experimentally observed using &lt;a href="https://implicit.harvard.edu/implicit/"&gt;implicit association tests&lt;/a&gt; (IATs).&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/03/F1.large.jpg" alt="What does it actually mean for AI to be racist?"&gt;&lt;br&gt;
&lt;em&gt;Photo from &lt;a href="http://science.sciencemag.org/content/356/6334/183"&gt;Caliskan et al.&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We cannot say these predictive models are flawed because they fail to align with our ideals of equality. On the contrary, they are powerful and accurate enough to detect the structural inequalities in our society--inequalities we try to pretend don't exist.&lt;/p&gt;
&lt;p&gt;There are some &lt;a href="https://arxiv.org/pdf/1607.06520.pdf"&gt;rather clever methods&lt;/a&gt; for removing social bias of different kinds from word embeddings by projecting the vector representations onto &amp;quot;unbiased&amp;quot; subspaces. But Princeton’s Arvind Naranayan, one of the Caliskan paper’s authors, &lt;a href="https://www.technologyreview.com/s/602950/how-to-fix-silicon-valleys-sexist-algorithms/"&gt;argues against&lt;/a&gt; blindly and unconditionally adjusting the predictive model simply because it contains bias:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We should think of these not as a bug but a feature... What constitutes a terrible bias or prejudice in one application might actually end up being exactly the meaning you want to get out of the data in another application.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Given such an impressively accurate model for mapping the meanings and connotations of words, the automatic removal of social bias will necessarily increase statistical bias, distorting a predictive algorithm’s perception of the real world. We should not do this without good reason.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/03/word_embed.png" alt="What does it actually mean for AI to be racist?"&gt;&lt;br&gt;
&lt;em&gt;A projection of word embeddings. The x-axis is parallel to $v(he) - v(she)$; the y-axis measures the strength of the gender association. Photo from &lt;a href="https://arxiv.org/pdf/1607.06520.pdf"&gt;Man is to Computer Programmer as Woman is to Homemaker?&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Still, the alarmists have a point. It's a bad idea to naively apply an algorithm based on word embeddings to tasks like screening resumes to only hire &amp;quot;career-minded&amp;quot; individual. Such an algorithm, even if it were statistically accurate, really &lt;em&gt;would&lt;/em&gt; be unfair. Faced with the juxtaposition between factual correctness and moral incorrectness, the authors of the Caliskan paper recognize that there is something dangerous about predictive algorithms that deal with situations of inequality. But the danger lies in the careless way we've applied the algorithm’s results, not in the results themselves. And that's the domain of algorithmic injustice.&lt;/p&gt;
&lt;h1 id="theinjusticeofunconstrainedoptimization"&gt;The injustice of unconstrained optimization&lt;/h1&gt;
&lt;p&gt;To understand the second kind of bias, let me you a story about how you can do everything right and still be wrong.&lt;/p&gt;
&lt;p&gt;Suppose I am starting a banking system that services universities in the San Francisco Bay Area. It just so happens in this scenario that Stanford students are, on average, less likely to default on their loans than UC Berkeley students. With machine learning, I decide to estimate someone's future financial solvency through factors including income, credit scores, and GPA. Although my algorithm doesn’t ask loan applicants where they went to school, it does ask for these other observations.&lt;/p&gt;
&lt;p&gt;Now, being a rather skilled data scientist, I make sure that my prediction algorithm is as statistically unbiased as possible (i.e. that it provides the best possible estimate of the risk of default, regardless of which school one went to). It certainly does appear that it is &lt;em&gt;calibrated&lt;/em&gt;: Stanford and Berkeley students with the same income, credit score, and GPA have about the same probability of defaulting.&lt;/p&gt;
&lt;p&gt;And my decision algorithm is simple! I’ll automatically approve loans to people who fall below a certain risk score, and deny all the others.&lt;/p&gt;
&lt;p&gt;Unsurprisingly, it turns out that my bank generally gives loans to Stanford students a little more often than to Berkeley students because of this pre-existing variation. I rest easy, however, knowing that my algorithm doesn’t actually discriminate on the basis of whether the applicant is a Bear or a Tree. Plenty of Berkeley students are still getting loans, and plenty of Stanford students are still being rejected.&lt;/p&gt;
&lt;p&gt;Thanks to my data science skills, my bank does well. So well, in fact, that other banks by the Bay begin adopting similar statistical algorithms to avoid being outcompeted by me. None of them are explicitly considering school either (perhaps this is even illegal), but they do use the same observations as I do. And their algorithms are statistically unbiased as well--they have to be, because a bank that can’t accurately estimate the risk of default will do worse than one that can.&lt;/p&gt;
&lt;p&gt;Things go on like this for a while. Suddenly, I wake up one day to find hundreds of Berkeley students protesting outside my window.&lt;/p&gt;
&lt;p&gt;As it turns out, something terrible has happened. Once all the other banks adopted my statistical techniques, more Stanford students were provided with social mobility through the loans than Berkeley students. The algorithms ended up translating an inequality of income into an inequality of social mobility, which further exacerbated the original inequality of income. Over time, the wealth distribution of Berkeley students fell further and further behind that of Stanford students because of my unequal actions, until the Berkeley students eventually took to the streets in righteous anger.&lt;/p&gt;
&lt;p&gt;Our prediction algorithm was unbiased. We didn't take the students' alma mater into account. Nobody in the Bay Area harbors any kind of bias against Cal students. So what went wrong?&lt;/p&gt;
&lt;p&gt;This about as appropriate a place as any to observe that this thought experiment is absolutely ridiculous. But the underlying dynamics of it are real. We've noted before that prediction algorithms will inevitably learn real-world inequalities. If we're careless with the prescriptive actions we take based on those predictions, we could inadverdently reproduce and worsen those inequalities. Even if we could predict the number of crimes in each neighborhood with perfect accuracy,  more policing in that neighborhood may lead to &lt;a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2016.00960.x/pdf"&gt;&amp;quot;worsening mental and physical health&amp;quot;&lt;/a&gt;. Even if we knew that an individual defendant was likely to reoffend, imprisoning them could destabilize their family. Supposedly &amp;quot;rational&amp;quot; predictive policing measures could theoretically cause more crime than they prevent.&lt;/p&gt;
&lt;p&gt;But I want to be a good person instead of simply maximizing short-term utility. Somehow, I need to take the consequences of broader social inequality into account. The way I map predictions to decisions needs to satisfy some additional definition of &lt;em&gt;algorithmic fairness&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;For this reason, Corbett-Davies et al. interpret algorithmic fairness as a kind of &lt;em&gt;constrained optimization&lt;/em&gt; in their landmark paper &lt;a href="https://arxiv.org/pdf/1701.08230.pdf"&gt;&lt;em&gt;Algorithmic Decision Making and the Cost of Fairness&lt;/em&gt;&lt;/a&gt;. Here's what that means.&lt;/p&gt;
&lt;p&gt;My loan-granting algorithm starts with a predictive machine-learning algorithm that determines the likelihood of a default. But, based on this likelihood, the algorithm still must decide whether or not to grant the loan. An unconstrained algorithm, like my own, simply maximizes my expected return. However, I might realize that unconstrained optimization has a worsening effect on group inequality. To remedy this, I’d add one of several proposed (incompatible) “fairness” constraints to their optimization:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Statistical parity:&lt;/em&gt; the average outcome for each group is the same. As director of the bank, I decree that we will give out loans to the same proportion of Stanford students and Berkeley students.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Conditional statistical parity:&lt;/em&gt; controlling for a select few “legitimate” observations, the average outcome for each group is the same. I institute a policy that a Stanford student and a Berkeley student who have the same credit score will have the same chance of getting a loan.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Predictive equality:&lt;/em&gt; the &amp;quot;false positive rate&amp;quot; for each group is the same. Nevertheless, I guarantee that is equally likely for a financially solvent Stanford student to be wrongly denied a loan as it is for a financially solvent Berkeley student to be wrongly denied one.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The paper goes on to formulate each of these definitions as formal constraints on the decision algorithm, and proves two striking results:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;I cannot implement any of the proposed fairness policies optimally without having different standards for each group.&lt;/li&gt;
&lt;li&gt;If I didn't have fairness policies and my prediction data was unbiased, the optimum decision rule for an unconstrained algorithm is to judge each group similarly.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first result's imlications are clear. If I wanted to ameliorate or avoid worsening the Stanford-Cal inequality, I’d have to be willing to accept more risk from Cal students than from Stanford students.&lt;/p&gt;
&lt;p&gt;That's an unsettling tradeoff: in situations like these, anyone wanting to guarantee equal outcomes between groups must, paradoxically, consider which group someone belongs to when making decisions. Thus is often called “fairness through awareness,” although some would certainly disagree that such a system is fair. For instance, this is bound to upset the Stanford students who are denied loans when comparable Berkeley students would be granted them. And as the authors point out, this may even run afoul of federal discrimination legislation in some cases.&lt;/p&gt;
&lt;p&gt;With the second result, the authors identify another tradeoff: between fairness and the value being optimized. They examine the problem of a hypothetical algorithm that determines whether a criminal defendant should be sentenced to prison based on their COMPAS score (a calibrated estimator of the likelihood that a criminal defendant will commit another crime). When the authors try to implement the proposed fairness constraints above, we find 9%, 4%, and 7% increases in projected violent crime by released defendants. Similarly, if I tried to give more loans to Cal students, my bank wouldn’t be as profitable as my competitors. Fairness always comes at a cost.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/03/cof.png" alt="What does it actually mean for AI to be racist?"&gt;&lt;/p&gt;
&lt;p&gt;[&lt;em&gt;The impact of a uniform risk score threshold on different fairness measures. We see that statistical parity (equal detention rate), predictive parity (equal false positive rate), and conditional statistical parity (equal detention rate given legitimate priors) are all unsatisfied. Diagrams from Corbett-Davies et al.&lt;/em&gt;]&lt;/p&gt;
&lt;p&gt;Thus, we must sacrifice some degree of effectiveness to achieve algorithmic fairness. This can be problematic, especially in competitive market economies where a high premium is placed on the ability to maximize short-term profit and on gaining an edge over competitors. In the end, the problem may even require a legislative solution.&lt;/p&gt;
&lt;p&gt;As the paper argues, policymakers need to be aware of these two tradeoffs (the legal and moral questions of “fairness through awareness” and the practical question of the “cost of fairness”) to make informed decisions about algorithmic fairness. We can’t simply ignore the second-order social consequences of unconstrained optimization, but a number of obstacles stand in the path of a decision-making process that is truly &amp;quot;fair.&amp;quot;&lt;/p&gt;
&lt;p&gt;All of this goes to show that algorithmic unfairness is much, much more complicated—and certainly more relevant—than statistical bias. It doesn't just come from biased individuals—it comes from inequalities in society. That's the real problem with prediction: not that it's imprecise enough to learn false beliefs from humans, bue because it's precise enough to make unconstrained optimization dangerous.&lt;/p&gt;
&lt;h1 id="whatshouldwedoaboutthis"&gt;What should we do about this?&lt;/h1&gt;
&lt;p&gt;A good first step would be to stop calling most algorithms racist. The word has connotations that don't exactly hold when its meaning is expanded to include algorithms, and it covers up the serious issues that are unique to algorithmic decision-making.&lt;/p&gt;
&lt;p&gt;Another step would be to interrogate the concept of &lt;em&gt;fairness&lt;/em&gt;. When it comes to algorithms, fairness can mean completely different things, from equal treatment (unconstrained optimization) to preventing inequality (algorithmic fairness). It's time to stop framing the issue as a Manichean battle between discrimination and justice, and understand the various incompatible ideas of justice whose contradictions we're witnessing now.&lt;/p&gt;
&lt;p&gt;Why am I focusing so much on how we should talk about the issue, rather than activism and agitation? Because at the end of the day, &lt;em&gt;this is really not our fight to fight&lt;/em&gt;. By and large, machine learning researchers and practitioners are responsible only for predictions. As long as we can eliminate statistical bias—that is, factual inaccuracy—from our models, we've done all we can do in our capacity to remove bias.&lt;/p&gt;
&lt;p&gt;It's the managers, the product designers, and the policymakers who decide how our models affect the world. And it's everyone's duty to think deeply about the issues of fairness and debate about what we really mean when we say we want a just society.&lt;/p&gt;
&lt;p&gt;No, we can't abandon statistical modeling. But we need to have a conversation about how we interpret its results, and it hasn't been happening.&lt;/p&gt;
&lt;nbsp&gt;
&lt;p&gt;[&lt;em&gt;Title image: a screenshot of Hitachi Visualization Suite's Predictive Crime Analytics feature&lt;/em&gt;]&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; Reinforcement learning is an exception, although not often used in industry.&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;2&lt;/sup&gt; See Corbett-Davies et al.&lt;/p&gt;
&lt;/nbsp&gt;&lt;/div&gt;</content:encoded></item><item><title>Exploiting Structured Data with Neural Networks</title><description>&lt;div class="kg-card-markdown"&gt;&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Data across many domains have underlying relational structure that may be represented through graphs. Friendships on a social network, for example, induce a network that connects users with other related users. Autonomous agents, such as a self-driving car, coexist with many objects, such as other vehicles, pedestrians, and stationary&lt;/p&gt;&lt;/div&gt;</description><link>http://localhost:2368/exploiting-structured-data-with-neural-networks/</link><guid isPermaLink="false">5ad18868f092d13d8a5762e0</guid><category>Overviews</category><dc:creator>Gary Gradient</dc:creator><pubDate>Fri, 09 Mar 2018 08:45:00 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1495592822108-9e6261896da8?ixlib=rb-0.3.5&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ&amp;s=c7f31a8c5fd0fea1620b5595ce12eb73" medium="image"/><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;img src="https://images.unsplash.com/photo-1495592822108-9e6261896da8?ixlib=rb-0.3.5&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ&amp;s=c7f31a8c5fd0fea1620b5595ce12eb73" alt="Exploiting Structured Data with Neural Networks"&gt;&lt;p&gt;Data across many domains have underlying relational structure that may be represented through graphs. Friendships on a social network, for example, induce a network that connects users with other related users. Autonomous agents, such as a self-driving car, coexist with many objects, such as other vehicles, pedestrians, and stationary obstacles, that together induce a multimodal network that represents spatial relationships between these entities. The labels applied to images may share semantic relationships inferred from WordNet or some other knowledge graph. Additionally, the local topology of an array of sensors may be represented through a graph embedding.&lt;br&gt;
$$\newcommand{\argmin}{\text{arg min}}\newcommand{\argmax}{\text{arg max}}\newcommand{\minimize}{\text{minimize}}\newcommand{\maximize}{\text{maximize}}\newcommand{\abs}{\text{abs}}$$&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/04/wordnet-hierarchy.png" alt="Exploiting Structured Data with Neural Networks"&gt;&lt;/p&gt;
&lt;p&gt;Despite this, relational structure is often discarded when developing machine learning models. For example, image classification tasks over photos scraped from social media may disregard the underlying social structure in spite of its apparent usefulness in characterizing entities. Recently, a plethora of papers have approached the &amp;quot;fashion-recommendation problem&amp;quot;, which seeks to determine sets of clothing items that constitute a cohesive &amp;quot;style&amp;quot;, often personalized for a particular individual. One successful approach has been to scrape images from a social media site, such as Instagram, extract the collection of clothing items present in the image, and then use the popularity of the image as a proxy for the fashionability of the basket of clothing. While it is certainly possible to learn fashion-styles in this way, the social network and other graphs, such as the Amazon product graph, contain valuable structural information that may improve the model.&lt;/p&gt;
&lt;p&gt;The reason structural information is often ignored is clear: it has been notoriously challenging to extract meaningful features from graphs in general. Although adjacency matrices (or, equivalently, incidence vectors) are the most straightforward way to express graphs in vector form, this representation is indefeasibly large for real-world graphs, which often contain millions, if not billions, of nodes. Additionally, since graphs are inherently an unordered structure, it does not necessarily make sense to specify an ordering as implied by an adjacency matrix. Convolutional neural networks (CNNs) applied to image classification and natural language processing (NLP) successfully leverage the strong locality in images and human language. Since pixel and text neighborhoods are of constant size, a constant-sized weight matrix may be convolved with the structured input to produce higher-order features. Since the shape of node neighborhoods in a graph are variable (some nodes may be low-degree while others may be high-degree), there is not an immediate graph analogue to convolutional layers.&lt;/p&gt;
&lt;p&gt;We shall explore a variety of recent developments in machine learning that attempt to leverage graphical structure in order to either directly improve performance or decrease the number of parameters used in the model. We shall first consider techniques such as &lt;em&gt;DeepWalk&lt;/em&gt; and &lt;em&gt;node2vec&lt;/em&gt; that seek to extract structural summary features from the neighborhood of nodes. Then, we shall look at &lt;em&gt;graph convolutional networks&lt;/em&gt; (GCNs) and dynamic graphical filters that generalize CNNs to arbitrary graph structures through spectral analogy.&lt;/p&gt;
&lt;h2 id="higherorderlatentfeatures"&gt;Higher-order latent features&lt;/h2&gt;
&lt;p&gt;We may construct a graph &lt;em&gt;summary&lt;/em&gt; in order to simply machine learning over graphs. Given a graph $\mathbf{G} = (\mathbf{V},\mathbf{E})$, we would like to identify a latent $d$-dimensional feature vector for each vertex, which encodes its neighborhood in $d$-dimensional space. Generically, the neighborhood of a vertex $v \in \mathbf{V}$ is any subset $N(v) \subset \mathbf{V}$. However, meaningful definitions of neighborhoods typically aim to capture a notion of locality, such as the 1-hop or 2-hop neighborhood around a vertex. Recently, random walks have been preferred as a way to generate neighborhoods without assuming too much about the structure of a network. Random walks are a more flexible definition of neighborhoods that provide stronger weighting to more closely related nodes. Since the length and character of random walks can be tuned, they may also improve training times and decrease the complexity of models seeking to leverage graph structure.&lt;/p&gt;
&lt;p&gt;We would like to learn a function $\Phi:\mathbf{V}\rightarrow{\mathbb R}^d$ such that $\Phi(u)$ and $\Phi(v)$ map to similar vectors in ${\mathbb R}^d$ if $u$ and $v$ have similar neighborhoods in $\mathbf{G}$. The image of $\mathbf{G}$ under $\Phi$ may be represented as a small matrix $\mathbf{F} \in {\mathbb R}^{\abs{V} \times d}$ of latent features. After performing this embedding of $\mathbf{G}$ into $(\abs{V}\times d)$-dimensional space, we may augment the features $\mathbf{X}$ associated with the vertices $\mathbf{V}$ with the structural summary $\mathbf{F}$. Since all available data has been transformed into tractable vector form, we may  solve our machine learning problem using any traditional method such as neural networks.&lt;/p&gt;
&lt;h1 id="learninglatentfeatures"&gt;Learning latent features&lt;/h1&gt;
&lt;p&gt;Let us consider $\mathbf{G} = (\mathbf{V},\mathbf{E})$ with node-wise features $\mathbf{X}$. Our goal is to determine for each $v^i \in \mathbf{V}$ some $d$-dimensional vector $\mathbf{f}^i$ such that $\mathbf{f}^i$ expresses the neighborhood of $v^i$.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;DeepWalk&lt;/strong&gt; algorithm approaches this problem through optimization techniques originally developed for natural language modeling &lt;sup&gt;1&lt;/sup&gt;. Assuming that the degree distribution of $\mathbf{G}$ follows a power-law distribution (which is true of many real-world social graphs), short random (first-order) walks over $\mathbf{G}$ rooted at $v^i$ obey Zipf's law (also see &lt;sup&gt;2&lt;/sup&gt; for more details on power law distributions in social graphs) and thereby may be viewed as &amp;quot;sentences&amp;quot; over a special language. Generalizing the language modeling task of maximizing the probability of words given their contexts, the DeepWalk algorithm seeks to learn a latent features that estimate the likelihood that vertex $u_i$ is observed in a random walk given its context $u_1,\ldots,u_{i - 1}$:&lt;/p&gt;
&lt;p&gt;\begin{equation}&lt;br&gt;
\maximize_{\mathbf{F} \in {\mathbb R}^{n\times d}} \quad \Pr{u_i~|~(\mathbf{f}^{u_1},\ldots,\mathbf{f}^{u_{i-1}})}&lt;br&gt;
\end{equation}&lt;/p&gt;
&lt;p&gt;This objective is untractable as the length of the random walks grows, so the authors adopted a certain language modeling relaxation that re-frames the problem in terms of predicting the (unordered) context of a vertex $u_i$ in a random walk given its latent representation $\mathbf{f}^{u_i}$. Phrased as an optimization problem:&lt;/p&gt;
&lt;p&gt;\begin{equation}&lt;br&gt;
\minimize_{\mathbf{F}\in {\mathbb R}^{n\times d}}\quad - \log \Pr{{u_{i - w},\ldots,u_{i-1},u_{i+1},\ldots,u_{i + w}}~|~\mathbf{f}^{u_i}}&lt;br&gt;
\label{eqobj}&lt;br&gt;
\end{equation}&lt;/p&gt;
&lt;p&gt;with sliding window length $w &amp;gt; 0$, this objective captures an unordered sense of locality and may be trained incrementally.&lt;/p&gt;
&lt;p&gt;But how, exactly, would one go about minimizing this objective over the random walk space of $\mathbf{G}$? The most obvious way to update $\mathbf{F}$ is to iteratively sample $v \in \mathbf{V}$, conduct a (truncated) random walk $\mathcal{W} = v_1,\ldots,v_t$ rooted at $v$, and consider $u_{k}$ for $k \in [j-w,j+w]$ for each $v_j \in \mathcal{W}$. We may compute $J(\mathbf{F}) = -\log\Pr{u_k~|~F_{v_j}}$ and then update $\mathbf{F} = \mathbf{F} - \alpha \frac{\partial J}{\partial F}$, which is a single step of gradient descend. Yet, this update procedure masks the complexity of the final gradient descend step $\mathbf{F} = \mathbf{F} - \alpha \frac{\partial J}{\partial F}$, which may require small updates to $\abs{\mathbf{V}} \times d$ parameters. In the paper &lt;sup&gt;3&lt;/sup&gt;, the authors give a two-part algorithm, &lt;strong&gt;DeepWalk&lt;/strong&gt; and &lt;strong&gt;SkipGram&lt;/strong&gt; (also see &lt;sup&gt;4&lt;/sup&gt;), that provides a training approach to minimize the objective by considering only a small subset of the negative examples. The authors construct a hierarchical binary classifier that reduces the number of parameters that need to be updated in a single gradient descent step from $O(\abs{\mathbf{V}}\times d)$ to $O(\log\abs{\mathbf{V}} \times d)$.&lt;/p&gt;
&lt;p&gt;Grover and Leskovec &lt;sup&gt;5&lt;/sup&gt; suggest a more flexible second-order random walk approach to neighborhood generation with similar NLP motivations to &lt;strong&gt;DeepWalk&lt;/strong&gt;. Their algorithm, &lt;strong&gt;node2vec&lt;/strong&gt;, generalizes across a variety of sampling strategies with two parameters $ p,q \in {\mathbb R}$ that influence the &amp;quot;locality&amp;quot; and &amp;quot;breadth-first&amp;quot; character of the walk, respectively. The authors explain that machine learning tasks over nodes in a graph typically seek to leverage two types of structural features, which they refer to as the &amp;quot;homophily hypothesis&amp;quot; and the &amp;quot;structural hypothesis&amp;quot; of neighborhoods. Under the homophily hypothesis, highly connected nodes are part of the same neighborhood since they are proximate to each other in the underlying graph. Under the structural hypothesis, nodes that serve similar structural functions (for example, nodes that act as a hub) are part of the same neighborhood due to their higher-order structural significance. The two tuning parameters provide a smooth way to interpolate between these two notions of neighborhood.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;node2vec&lt;/strong&gt; paper makes a conditional independence assumption that states the likelihood of seeing any node in the random walk is independent of the other nodes that appear in the random walk. This assumption is not precise, but simplifies our objective from earlier by expressing&lt;/p&gt;
&lt;p&gt;\begin{equation}&lt;br&gt;
\Pr{u_1,\ldots,u_t~|~\mathbf{f}^{j}} = \prod_{i=1}^t \Pr{u_i~|~\mathbf{f}^{j}}&lt;br&gt;
\end{equation}&lt;/p&gt;
&lt;p&gt;The paper additionally imposes a symmetry constrain in the feature space under which the feature vector of a node and a node in its neighborhood have symmetric effect on each other. This property is reflective of both the &amp;quot;homophily hypothesis&amp;quot; and the &amp;quot;structural hypothesis&amp;quot;, which are inherently symmetric interpretations of neighborhoods in graphs. One popular way to capture symmetric similarity in low-dimensional embeddings is through a softmax unit parameterized by the dot-product of the feature vectors. This symmetry assumptions yields an explicit formulation of the &lt;strong&gt;node2vec&lt;/strong&gt; objective function:&lt;/p&gt;
&lt;p&gt;\begin{equation}&lt;br&gt;
\maximize_{\mathbf{F}\in {\mathbb R}^{n\times d}} \sum_{v^i \in \mathbf{V}}&lt;br&gt;
\left( -\log \Gamma_i + \sum_{v^j \in N(v^i)} \mathbf{f}^{i}\cdot \mathbf{f}^{j}\right)&lt;br&gt;
\end{equation}&lt;/p&gt;
&lt;p&gt;where $\Gamma_i = \sum_{v^j \in \mathbf{V}} \exp(\mathbf{f}^i\cdot \mathbf{f}^j)$ is the per-node partition function (which is a byproduct of the softmax normalization) and $N(u)$ is the neighborhood of $u \in \mathbb{V}$ generated by $(p,q)$-random-walk sampling strategy. Just as with &lt;strong&gt;DeepWalk&lt;/strong&gt;, the &lt;strong&gt;node2vec&lt;/strong&gt; objective may be optimized by sampling $v_i \in \mathbf{V}$, taking a $(p,q)$-random-walk $v_1,\ldots,v_t$ rooted at $v_i$, and then updating $\mathbf{F}$ through gradient descent with respect to each $(\mathbf{f}^i, \mathbf{f}^j)$ pair. While the second term of the &lt;strong&gt;node2vec&lt;/strong&gt; objective is only dependent on $(\mathbf{f}^{i},\mathbf{f}^j)$ for $v_j \in N(v)$, the partition $\Gamma_i$ is a function of $\mathbf{f}^j$ for all $v_j \in \mathbf{V}$. For social graphs with hundreds of millions or billions of nodes, it is computationally intractable to update $O(\abs{V}\times d)$ parameters for every random walk that we sample. Using negative sampling, we may approximate $\sum_i \log \Gamma_i$ without incurring computational cost quadratic in network size (see &lt;sup&gt;6&lt;/sup&gt; for additional details).&lt;/p&gt;
&lt;h2 id="latentfeaturesovertemporalnetworks"&gt;Latent features over temporal networks&lt;/h2&gt;
&lt;p&gt;Neither &lt;strong&gt;DeepWalk&lt;/strong&gt; nor &lt;strong&gt;node2vec&lt;/strong&gt; are designed to immediately provide latent feature representations of temporal networks that generalize over time. While one may compute a separate embedding at each time step, there are two immediate downsides to this process: firstly, it may be extremely expensive to run &lt;strong&gt;DeepWalk&lt;/strong&gt; or &lt;strong&gt;node2vec&lt;/strong&gt; on many snapshots of the network and, secondly, there is no formal guarantee that multiple applications of &lt;strong&gt;DeepWalk&lt;/strong&gt; or &lt;strong&gt;node2vec&lt;/strong&gt; produce similar latent feature spaces.&lt;/p&gt;
&lt;p&gt;We may attempt to resolve the first problem by reusing as much structural information as possible between successive applications of our summarization algorithm. Instead of reapplying &lt;strong&gt;DeepWalk&lt;/strong&gt; or &lt;strong&gt;node2vec&lt;/strong&gt; after each modification of $\mathbf{G}$, we may wait until enough edges have been modified to significantly compromise the quality of the original embedding. Exactly how many edges constitute a &amp;quot;significant&amp;quot; enough change in the structure of $\mathbf{G}$ depends highly on the role of the modified edges in $\mathbf{G}$ and the $(p,q)$-character of the random walk. For example, consider the following pairs of graphs:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/04/K6-1.png" alt="Exploiting Structured Data with Neural Networks"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/04/K6.png" alt="Exploiting Structured Data with Neural Networks"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/04/P6.png" alt="Exploiting Structured Data with Neural Networks"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/04/C6.png" alt="Exploiting Structured Data with Neural Networks"&gt;&lt;/p&gt;
&lt;p&gt;Neighborhoods are similar in $K_6 - e$ and $K_6$, but very different in $P_6$ and $C_6$.&lt;/p&gt;
&lt;p&gt;In this figure, the neighborhoods of nodes in $K_6 - e$, the complete graph on six nodes with an edge removed, induced by a first-order random walk, are very similar to the neighborhoods of $K_6$. However, the marginal edge that transforms the path graph, $P_6$, into a cycle, $C_6$, significantly alters the first-order random walk neighborhoods of many nodes in graph. In general, edges that acts as &amp;quot;bridges&amp;quot; between otherwise disconnected or weakly connected parts of a network are liable to have greater impact on node neighborhoods.&lt;/p&gt;
&lt;p&gt;Luckily, many real world graphs, such as social networks, contain a large, strongly connected core with many properties similar to complete graphs. In the case of first-order random walks (such as those used in &lt;strong&gt;DeepWalk&lt;/strong&gt;), the marginal addition or deletion of an edge in the expander-like core a network has very little impact on node embeddings. Additionally, for social graphs, many edge additions are simply ``triangle closing&amp;quot; events, which have very little impact on the neighborhoods of nodes outside that immediate locality.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/04/bow-tie.png" alt="Exploiting Structured Data with Neural Networks"&gt;&lt;/p&gt;
&lt;p&gt;The above discussion of properties of first-order random walks and their relationship to graph connectivity does not necessarily extend to the second-order random walks used by the &lt;strong&gt;node2vec&lt;/strong&gt; algorithm. Many of the guarantees pertaining to first-order random walks on $\mathbf{G}$ and the connectivity of $\mathbf{G}$ are implied by spectral properties of the graph Laplacian matrix of $\mathbf{G}$. However, higher-order random walks may not respect as strong of a relationship between the neighborhood induced by the random walk and local connectivity. For example, &lt;strong&gt;node2vec&lt;/strong&gt; parameterized by $p \gg 1$ and $q \ll 1$, which corresponds to a random walk biased to stray away from its origin, does not generate neighborhoods that strongly depend on the &lt;em&gt;local&lt;/em&gt; connectivity of the root node. These DFS-like walks may relate more strongly to &lt;em&gt;global&lt;/em&gt; measures of connectivity as all random-walks have a tendency to be funneled through global bottleneck edges. To the best of my knowledge, there is no literature that studies the relationship between the neighborhoods induced by various second-order random walks and more primitive graph properties.&lt;/p&gt;
&lt;p&gt;I believe that some combination of dynamic graph partitioning and iterative updates to the graph summary matrix $\mathbf{F}$ may yield a computationally tractable approach to temporal graph latent feature extraction. In general, further unification of Knowledge Discovery and Data Mining (KDD) literature with the state-of-the-art combinatorial and spectral algorithms may provide greater insight into graph representations. For temporal graphs, tools such as the dynamic spectral sparsifier given by &lt;em&gt;Abraham et al.&lt;/em&gt; (2016) may inspire one approach towards incremental updates of graph embeddings of time-varying networks &lt;sup&gt;7&lt;/sup&gt;. However, the current asymptotically-best dynamic spectral sparsifiers are impractically slow on the scale of real-world graphs (although sub-logarithmic algorithms exist, the constant factor of these algorithms may be enormous). Further work relating spectral properties of graphs to latent features may be valuable for data mining over temporal graphs such as social networks.&lt;/p&gt;
&lt;p&gt;While unexplored in the literature, &lt;strong&gt;node2vec&lt;/strong&gt; could potentially be generalized to temporal networks by introducing an additional parameter $\lambda$ that influences the likelihood of random walks to traverse temporal boundaries. Just as the random walk parameters $p$ and $q$ influence the locality and breadth-first character of the walk, $\lambda$ may express the temporal affinity. Some prediction tasks may assume a &amp;quot;temporal locality&amp;quot; in which frames of $\mathbf{G}$ at similar timestamps are strongly related while other problems may have longer range temporal relationships with weaker locality. Although GCNs, which shall be described in the subsequence section, provide a way to capture spatio-temporal structural features, a generalization of second-order $(p,q)$-random-walks to temporal networks may provide another way to approach machine learning over temporal graphs.&lt;/p&gt;
&lt;h1 id="graphconvolutionalnetworks"&gt;Graph convolutional networks&lt;/h1&gt;
&lt;p&gt;While the latent feature approaches outlined in the last section provide a context-free schema to summarize arbitrary graphs into lower-dimensional space (possibly with a few tunable parameters), these approaches cannot capture all possible interactions between structural features and per-node features. Since the graph summary is constructed independent of the problem to which it is applied, per-node attributes that impact the role of nodes in the graph are never reflected in the structural features captured by the summary. For example, a multimodal attributed network of users and pages in which edges correspond to friendships (user-user), hyperlinks (page-page), and likes (user-page) could benefit from an end-to-end learning approach since the structural features relevant to users and pages are not necessarily the same. Although we know whether a node corresponds to a user or a page a priori, graph summary models such as &lt;strong&gt;node2vec&lt;/strong&gt; treat all nodes as equivalents, which may omit useful structure from some summary vectors. Perhaps we can do better by fitting structural features at the same time as the parameters of the model to produce a more &amp;quot;fine-tuned&amp;quot; notion of structure per node. An end-to-end approach to machine learning tasks over graphs may be able to discover more context dependent features than &lt;strong&gt;node2vec&lt;/strong&gt; or &lt;strong&gt;DeepWalk&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Convolutional layers have replaced a great deal visual and textual feature engineering with fully trainable end-to-end CNN models. Instead of developing features by hand, one may fit the parameters of a CNN to identify local structures that are highly dependent on the particular learning problem. Convolutional layers in a traditional CNN compute:&lt;/p&gt;
&lt;p&gt;$$\mathbf{y}_i = \mathbf{b} + {\sum_{m = 1}^M {\mathbf{W}_m \mathbf{x}_{j(m,i)}}}$$&lt;/p&gt;
&lt;p&gt;for weight matrices $\mathbf{W}_m\in {\mathbb R}^{E\times D}$, inputs $\mathbf{x}_j \in {\mathbb R}^{D}$, activations $\mathbf{y}_i \in {\mathbb R}^E$, and index map $j:M\times M\rightarrow M$ that sends $(m,i)$ to the index of the $m$th neighbor of the $i$th input. Framing our equation in terms of image data for an $h\times w$ pixel image, let $M = h\times w$, $D$ equal the number of input channels, $E$ equal the number of output channels, and $j$ map neighborhoods in a &amp;quot;spiral-like&amp;quot; fashion (see &lt;sup&gt;8&lt;/sup&gt; for helpful figures). Often, the matrices $\mathbf{W}_m = \mathbf{0}$ for $m &amp;gt; M'$ where $M'$ is some small constant, which significantly improves training time in practice and helps to reduce over-fitting.&lt;/p&gt;
&lt;p&gt;To generalize the equation to graphs, &lt;em&gt;Verma et al.&lt;/em&gt; propose a soft-assignment function $q_m : {\mathbb R}^D\times {\mathbb R}^D\rightarrow {\mathbb R}$ and modified equation:&lt;/p&gt;
&lt;p&gt;\begin{equation}&lt;br&gt;
y_i = \mathbf{b} + \sum_{m = 1}^M \frac{1}{|\mathcal{N}_i|} \sum_{j \in \mathcal{N}_i} q_m(\mathbf{x}_i,\mathbf{x}_j)\mathbf{W}_m \mathbf{x}_{j}&lt;br&gt;
\end{equation}&lt;/p&gt;
&lt;p&gt;where $\mathcal{N}_i$ is the neighborhood of the $i$th datum. Soft-assignment makes sense in the context of graphs since node neighborhoods $\mathcal{N}_i$ are unordered. This equation reduces to the last one when $\mathcal{N}_i = {1,\ldots,M}$, $q_m:{\mathbb R}^D\times {\mathbb R}^D\rightarrow {0,1}$ and $q_m(\mathbf{x}_i,\mathbf{x}_j) = 1$ iff $j(m,i) = j$. The authors suggest that&lt;/p&gt;
&lt;p&gt;\begin{equation}&lt;br&gt;
q_m(\mathbf{x}_i,\mathbf{x}_j) \propto \exp\left(\mathbf{u}^T_m\mathbf{x}_i + \mathbf{v}^T_m \mathbf{x}_j + c_m\right)&lt;br&gt;
\label{asym}&lt;br&gt;
\end{equation}&lt;/p&gt;
&lt;p&gt;where $\mathbf{u}_m$, $\mathbf{v}_m$, and $c_m$ can either be learned along with $\mathbf{W}_m$ or selected a priori. This equation may be seen as a relaxation of the &amp;quot;neighborhood symmetry&amp;quot; assumption made by Grover and Leskovec that permits asymmetric relationships smoothed by a soft-max unit. This relaxation is flexible enough to capture asymmetric &amp;quot;similarity-from&amp;quot; and &amp;quot;similarity-to&amp;quot; relationships with soft application that is particularly important for learning over multimodal or directed networks.&lt;/p&gt;
&lt;p&gt;Various other authors have suggested alternatives to the GCN described above, including spectral filters that leverage the relationship between graph Laplacians and community structure in graphs. For example, Defferrard &lt;em&gt;et al.&lt;/em&gt; (2016) provides a local spectral filtering approach that considers only interactions within an order $k$-ball around each node &lt;sup&gt;9&lt;/sup&gt;. These local spectral filters are promising for two reasons: firstly, they provide an alternative generalization of CNNs to graphs by considering strictly low-dimensional filters in the spectral domain of $\mathbf{G}$, and, secondly, for many of the reasons detailed earlier, spectral-based GCNs may be able to capture stronger community-based properties of $\mathbf{G}$ than  spatial GCNs.&lt;/p&gt;
&lt;h1 id="conclusion"&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Machine learning over graphs is a thriving topic of academic interest. So far, most applications of machine learning over graphs have concerned social networks, product recommendation, or proof-of-concept. Yet, relational structure is abundant everywhere. Imagine leveraging the relational structure defined over objects by WordNet to improve the performance of object detection in computer vision. Imagine social robots deriving interpersonal relationships from the spatial relationships between the bodies of human subjects. And, imagine bioinformatic algorithms that directly harness molecular structure to reduce the complexity of lattice based approaches to protein-folding simulation. While I cannot promise that any of the approaches described in this paper will necessarily improve on the current state-of-the-art in these applications, I think that using all available structure may be a productive front for future research.&lt;/p&gt;
&lt;/div&gt;</content:encoded></item><item><title>Software that writes software: the state of program induction</title><description>&lt;div class="kg-card-markdown"&gt;&lt;p&gt;Writing correct software is surprisingly hard.&lt;/p&gt;
&lt;p&gt;This collection of &lt;a href="https://github.com/danluu/post-mortems"&gt;prominent software post-mortems&lt;/a&gt; by Dan Luu is a humbling reminder of this harrowing truth.  It features Knight Capital's $459 million loss in 45 minutes due to conflicting deployed software versions, and a Linux kernel bug related to leap-second handling taking down&lt;/p&gt;&lt;/div&gt;</description><link>http://localhost:2368/software-that-writes-software-on-program-learning-2/</link><guid isPermaLink="false">5ad18868f092d13d8a5762df</guid><category>Overviews</category><dc:creator>Gary Gradient</dc:creator><pubDate>Fri, 09 Mar 2018 08:44:00 GMT</pubDate><media:content url="http://localhost:2368/content/images/2018/03/th--1-.jpeg" medium="image"/><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;img src="http://localhost:2368/content/images/2018/03/th--1-.jpeg" alt="Software that writes software: the state of program induction"&gt;&lt;p&gt;Writing correct software is surprisingly hard.&lt;/p&gt;
&lt;p&gt;This collection of &lt;a href="https://github.com/danluu/post-mortems"&gt;prominent software post-mortems&lt;/a&gt; by Dan Luu is a humbling reminder of this harrowing truth.  It features Knight Capital's $459 million loss in 45 minutes due to conflicting deployed software versions, and a Linux kernel bug related to leap-second handling taking down much of the Internet - Amazon Web Services, Reddit, and Mozilla included.&lt;/p&gt;
&lt;p&gt;This alone motivates a question - is it possible to write software that writes software?  By one definition, we already have; compilers, which programmers use every day, are ubiquitous.  While it is theoretically possible to write optimized x86 assembly code, the magnificent GNU Compiler Collection (gcc) allows us to program in C, a higher level programming language.&lt;/p&gt;
&lt;p&gt;Still, this is an unsatisfying example.  While compilers let us program with greater levels of abstraction, at the end of the day, humans still need to write code.  A better question to ask might be: is it possible to generate useful programs directly from a list of requirements, without human intervention?  Recent research efforts have contended with this problem, with early but promising results.&lt;/p&gt;
&lt;p&gt;Perhaps the most compelling motivation to consider this problem is program complexity.  Software that writes software might produce more sophisticated, flexible systems than humans are capable of designing.  A recent research paper by Google Brain applied this idea to designing neural network architectures; the algorithmically designed tf.contrib.rnn.NASCell was even merged into TensorFlow.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/03/NAS.png" alt="Software that writes software: the state of program induction"&gt;&lt;/p&gt;
&lt;p&gt;As Smolensky &lt;a href="https://link.springer.com/content/pdf/10.1007/BF00130011.pdf"&gt;asserted in 1987&lt;/a&gt;, there have been two broad paradigms in AI: symbolic and connectionist. Symbolic systems operate on entities (“symbols”) that are representations of data.  The classic example is expert systems, in which human experts hard code knowledge.  For instance, a stock trader might encode the rule “if GOOGL stock falls below $900, sell 1000 shares.”&lt;/p&gt;
&lt;p&gt;Connectionist systems can be described as “large networks of extremely simple processors, massively interconnected and running in parallel” (Smolensky).  Neural networks, of course, fall into this camp.  These systems tend to apply statistical inference techniques, and operate on numeric features.   These statistical systems tend to be less interpretable, but often perform better, given more training data.&lt;/p&gt;
&lt;p&gt;Programs that write programs are interesting because it fuses the symbolic and the connectionist in a unique way.&lt;/p&gt;
&lt;p&gt;They are symbolic in that programming languages provide a syntactic, human interpretable language for computation.  They are connectionist in that the final output code is often computed with the assistance of neural networks, gradient descent, and other statistical tools.&lt;/p&gt;
&lt;p&gt;It seems that the field of program induction has been less noticed than more conventional supervised approaches in deep learning.  This might be because the learned programs are relatively simple.  For example, DeepCoder (Balog et al. 2017) is able to construct programs of only up to 5-10 lines.  While the practical applications of these programs are limited, being able to synthesize them at all over a large search space is a significant research milestone.&lt;/p&gt;
&lt;h2 id="machinelearningassearchingtheprogramspace"&gt;Machine learning as searching the program space&lt;/h2&gt;
&lt;p&gt;Supervised machine learning can be thought of as “searching a program space.”  In particular, we have some &lt;em&gt;target behavior&lt;/em&gt;, for instance “identify vehicles in images,” that we do not know how to encode procedurally.  We encode priors about this particular in a &lt;em&gt;model choice&lt;/em&gt;; we might opt to choose convolutional neural networks to learn features on image data.  And the training algorithm searches the &lt;em&gt;program space&lt;/em&gt; for the best neural network weights that solve our problem.&lt;/p&gt;
&lt;p&gt;One might liken this to a version of test driven development.  In this case, our &amp;quot;tests&amp;quot; are each training example in our dataset, and gradient descent enables us to identify the weights that satisfy our &amp;quot;test suite.&amp;quot;&lt;/p&gt;
&lt;p&gt;What is a more appropriate space, weights or code?  In a certain sense, searching over symbolic programs is substantially more difficult.  A neural network that has its weights perturbed by some small epsilon $\epsilon$ may only yield a small difference in the overall prediction output.  However, a single character discrepancy in a program will result in code that may not compile at all.&lt;/p&gt;
&lt;p&gt;Source code might be able to produce algorithmic complexity that is hard for current deep learning architectures to encode.  Training an RNN to sort a sequence of numbers is very hard (and possibly intractable, see &lt;a href="https://arxiv.org/pdf/1511.06279.pdf"&gt;Reed and de Freitas 2015&lt;/a&gt;).  Whereas implementing sorting in a high level programming language is nearly trivial.&lt;/p&gt;
&lt;p&gt;Source code also provides a unique level of interpretability.  At least for small programs encoded in a suitably high level programming language, it is straightforward to read through the code to get a sense of what it is computing.  Whereas for even small neural networks, investigating the weights of a model trained on MNIST is fairly difficult without more specialized tools.&lt;/p&gt;
&lt;h2 id="definingtheproblem"&gt;Defining the problem&lt;/h2&gt;
&lt;p&gt;We can broadly divide current approaches to program learning into three categories: a) rule based program synthesis, b) statistical program synthesis, and c) statistical program induction.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Rule based program synthesis&lt;/em&gt; was the predominant classical approach two decades ago.  In this setting, “a formal grammar is used to derive a program from a specification” (&lt;a href="https://arxiv.org/pdf/1703.07469"&gt;Devlin 2017&lt;/a&gt;).  You begin with a mathematical spec of the program you want to derive.  For example, a sorting algorithm might be defined as follows.  Then, you use theorem proving techniques to iterate towards a program satisfying the specification.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/03/sorting.png" alt="Software that writes software: the state of program induction"&gt;&lt;/p&gt;
&lt;p&gt;(for example: a spec to sort a list of numbers.  From Zohar Manna (add reference)).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Statistical program synthesis&lt;/em&gt; refers to using a statistical or machine learning algorithm to synthesize a literal program.  A common paradigm is to feed a neural network a training set of input-output pairs, which learns to output program source code.  This code can then be executed to compute outputs given a general input not in the training set.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Statistical program induction&lt;/em&gt; refers to abstractly learning to compute the output using a latent program representation.  Importantly, in this setting, the output is not code.  Instead, the algorithm learns to compute the output using an internal “architecture” that simulates computation.  A concrete example is the Neural Turing Machine, which couples a differentiable neural network architecture with external memory (Graves et al. 2014).&lt;/p&gt;
&lt;h2 id="neuralprogramsynthesis"&gt;Neural Program Synthesis&lt;/h2&gt;
&lt;p&gt;Balog et al. 2017 presented &lt;em&gt;DeepCoder&lt;/em&gt;, a neural program synthesis method to learn solutions to programming contest problems.  This is a natural test-case to study, because we can find thousands of problems of highly variable levels of difficulty.  Being able to generate solutions to these problems could open the door to program learning more generally.&lt;/p&gt;
&lt;p&gt;The core of the method is a neural network that accepts a dataset of input-output pairs $(X_1, Y_1), (X_2, Y_2), \dots, (X_N, Y_N)$ and outputs an &lt;em&gt;attribute vector&lt;/em&gt; a, whose elements indicate the probability of various programming keywords appearing the resulting source code.  This prediction is combined with a depth-first search algorithm that searches over programs that satisfy the conditions of the attributes.&lt;/p&gt;
&lt;p&gt;An attribute is useful for the program synthesis task if it a) is predictable from input-output examples, and b) conditioning on its value reduces the size of the program search space.  An example attribute prediction is shown below, reproduced from Balog et al. 2017.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/03/deepcoder_kw.png" alt="Software that writes software: the state of program induction"&gt;&lt;/p&gt;
&lt;p&gt;A key component here is the choice of sufficiently expressive programming keywords, in the choice of a domain-specific language (DSL).  The choice of DSL determines the “building blocks” of the program language, and has a direct correlation with the difficulty of the program synthesis task.  A desirable DSL is both expressive – with a single keyword, one should be able to encode a sophisticated computational process – and specific  – it should be possible to achieve sufficient customization to solve the types of the problems appearing in programming contests.&lt;/p&gt;
&lt;p&gt;The DSL chosen includes functional operators like &lt;a href="http://hackage.haskell.org/package/base-4.10.1.0/docs/Prelude.html#v:scanl1"&gt;scanl1&lt;/a&gt;, which successively applies a reduce operation to elements in a list; for instance &lt;code&gt;scanl1 (+) [1, 2, 3, 4] -&amp;gt; [1, 3, 6, 10]&lt;/code&gt;.  It also includes higher order functions like &lt;code&gt;map&lt;/code&gt;, &lt;code&gt;filter&lt;/code&gt;, and &lt;code&gt;zipWith&lt;/code&gt;.  This functional style of programming often makes it possible to describe algorithms with less code than an equivalent imperative program.&lt;/p&gt;
&lt;p&gt;The neural network is trained using a data generation strategy.  Programs are enumerated in the DSL, applying a heuristic pruning strategy to remove programs that are redundant (for example, ones for which there is a shorter equivalent program).  Input-output pairs are generated by executing the programs over predetermined bounded ranges of integers.  Binary attribute vectors are computed directly from the source code in the DSL.&lt;/p&gt;
&lt;p&gt;The network architecture consists of an &lt;code&gt;encoder&lt;/code&gt; and a &lt;code&gt;decoder&lt;/code&gt;.  For the encoder, a feed-forward architecture was used.  The input and output types (singleton or array) are featurized using a one-hot-encoding, and the inputs and outputs are padded to a maximum length.  Each integer in the inputs and the outputs are mapped to a learned embedding vector of size E = 20.  The authors use average pooling to combine learned representations for each input-output examples in a set generated from the same program.  It was found empirically that this feedforward encoded architecture performs at least as well as an RNN baseline.&lt;/p&gt;
&lt;p&gt;The decoder simply multiples the encoding of input-output examples by a learned $C \times K$ matrix, where $C = 34$ is the number of functions in in the DSL, and $K = 256$ is the number of sigmoid units in each of the three hidden layers of the encoder.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/03/deepcoder_arch.png" alt="Software that writes software: the state of program induction"&gt;&lt;/p&gt;
&lt;p&gt;The central idea in this paper is to use a neural network’s attribute predictions to condition the program search.  There are a number of search strategies.  The authors experiment with an optimized DFS, “sort and add” enumeration, Sketch, and $\lambda^2$, which can incorporate the attribution prediction as a prior to condition the search space.  Many of these search algorithms are based on SMT solver approaches.&lt;/p&gt;
&lt;p&gt;Remarkably, the authors report an order of magnitude speedup over baselines without the attribute predictions.  DeepCoder is able to solve problems at the level of the easiest problems on programming competition databases:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/03/deepcoder_prog.png" alt="Software that writes software: the state of program induction"&gt;&lt;/p&gt;
&lt;p&gt;Given the search space of programs of this size, this is a compelling research achievement.&lt;br&gt;
DeepCoder combines both the symbolic and the connectionist paradigms in a pipeline that is able to learn interpretable programs.  However, it remains to be seen whether input-output examples have sufficient information to permit useful attribute prediction for larger programs.  Will this scale?  We will probably need more attributes, as the authors note in their &lt;a href="https://openreview.net/forum?id=ByldLrqlx"&gt;OpenReview thread&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="differentiableinterpreters"&gt;Differentiable Interpreters&lt;/h2&gt;
&lt;p&gt;How do we incorporate prior knowledge into a machine learning model?  Bosnjak et al. 2017 consider this fundamental problem from a unique angle -- suppose the prior knowledge we have is &lt;em&gt;procedural&lt;/em&gt;.  They present an end-to-end approach to program learning, where a programmer can provide a program sketches with slots that need to be filled in.  Gradient descent is then applied to “fill in” learned behavior from program input-output data.&lt;/p&gt;
&lt;p&gt;The authors use the programming language Forth to conduct their work.  They choose Forth in part because of its expressivity, but most importantly because its abstract machine is simple enough to implement a continuous approximation.&lt;/p&gt;
&lt;p&gt;Forth’s abstract machine is represented by a state $S = (D, R, H, c)$, which contains two stacks.  $D$ is a data evaluation pushdown stack, and $R$ is a return address pushdown stack.  $H$ stores the heap, and $c$ is a program counter.  A Forth program $P$ can be represented as a sequence of words $P = w_1 w_2 \dots w_n$, which can be regarded as a sequence of state transition functions.  To make gradient based learning possible, the machine output must be differentiable with respect to these functions $w_i$.  The author’s implementation, which they call $\delta 4$, models the Forth abstract machine as a recurrent neural network, parametrized by the transition functions at each time step.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/03/D4_sketch.png" alt="Software that writes software: the state of program induction"&gt;&lt;/p&gt;
&lt;p&gt;In the above figure, there are two ways to provide a sketch for the sorting algorithm.  In blue, the authors present the &lt;code&gt;PERMUTE&lt;/code&gt; sketch, which specifies that the top two elements of the stack and the top of the return stack must be permuted based on the values of the former.  In this setting, functionality to compare values and perform the permutation must be learned.  In yellow, the authors write the &lt;code&gt;COMPARE&lt;/code&gt; sketch, which provides additional procedural knowledge to the model.  In this context, only the comparison between the top two elements in the stack must be learned.&lt;/p&gt;
&lt;p&gt;Let $M$ represent a memory buffer corresponding to the data stack, return stack, or the heap.  Then, differentiable read and write operations can be implemented as&lt;/p&gt;
&lt;p&gt;$$\text{read}_\mathbf{M}(\mathbf{a}) = \mathbf{a}^T \mathbf{M}$$&lt;br&gt;
$$\text{write}_\mathbf{M}(\mathbf{x}, \mathbf{a}) : \mathbf{M} \leftarrow \mathbf{M} - (\mathbf{a1}^T \odot \mathbf{M} + \mathbf{xa}^T)$$&lt;/p&gt;
&lt;p&gt;Here, $\mathbf{a}$ represents an &lt;em&gt;address pointer&lt;/em&gt;, and $\odot$ is elementwise multiplication.  These operations are modeled after the way a Neural Turing Machine's memory functions (Graves et al. 2014).&lt;/p&gt;
&lt;p&gt;A slot $w$ can be encoded by specifying a state encoder and a decoder.  Users can choose from many operations for encoders and decoders, listed below.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/03/D4_encdec.png" alt="Software that writes software: the state of program induction"&gt;&lt;/p&gt;
&lt;p&gt;Finally, a recurrent neural network is used to model execution.  The RNN computes a state $S_{n+1}$ conditioned on a previous state $S_n$.  We have:&lt;/p&gt;
&lt;p&gt;$$ \mathbf{S}_{n+1} = \text{RNN}(\mathbf{S}_{n}, \mathbf{P}_{\theta}) = \sum_{i=1}^{|P|} \mathbf{c}_{i} \mathbf{w}_{i} (\mathbf{S}_n). $$&lt;/p&gt;
&lt;p&gt;This is quite computationally expensive.  At every time step, a new machine state is calculated by executing all words in the program and then weighting the result states by the program counter.  Thus, the authors use a number of optimizations to avoid full RNN steps.  First, they use symbolic execution to “collapse” transition functions.  For example, &lt;code&gt;R &amp;gt; SWAP &amp;gt; R&lt;/code&gt; can be collapsed into a single transition.  Second, they execute if branches in parallel and weigh their output states by the value of the condition.&lt;/p&gt;
&lt;p&gt;This work is similar to probabilistic programming languages such as Church.  In this setting, users can incorporate probabilities densities into programs to specify distributions over possible execution traces.  While traditional probabilistic programming languages typically use a form of Bayesian inference to compute the posterior, $\delta 4$ updates parameters using gradient descent.  The authors note that this style is similar to the TerpreT FMGD algorithm (Gaunt et al. 2016).&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/03/D4_diag.png" alt="Software that writes software: the state of program induction"&gt;&lt;/p&gt;
&lt;p&gt;The authors are able to learn a sorting procedure using only a few hundred training examples!  Experimentally, they show that their D4 implementation vastly outperforms a Seq2Seq baseline.  The seq2seq implementation fails to generalize at all to lengths longer than the training lengths. By contrast, the sketch highlighted earlier defines the outer loop which repeatedly calls the BUBBLE function; this guarantees that the functionality of the network is invariant to the input sequence length.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/03/D4_results.png" alt="Software that writes software: the state of program induction"&gt;&lt;/p&gt;
&lt;h2 id="unsupervisedlearningbyprogramsynthesis"&gt;Unsupervised Learning By Program Synthesis&lt;/h2&gt;
&lt;p&gt;Given a dataset, how can we capture latent structure using a program?  Ellis et al. 2017 applied program synthesis techniques uniquely to unsupervised learning.   Examples include include visual concepts, linguistic rules, and multitask regression:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/03/USL_ex.png" alt="Software that writes software: the state of program induction"&gt;&lt;/p&gt;
&lt;p&gt;More concretely stated, we would like to explain an observed dataset as the output of a program run on unknown inputs.  We can formulate this joint inference over the program and the program inputs, optimizing for the smallest solution.  The loss function is constructed as follows, with the objective of minimizing program length, data reconstruction error, and data encoding length.&lt;/p&gt;
&lt;p&gt;$$\underbrace{- \log P_f(f)}_{\text{program length}} + \sum_{i=1}^{N} \left ( \underbrace{- \log P_{x|z} (x_i | f(I_i))}_{\text{data reconstruction error}} - \underbrace{\log P_I (I_i)}_{\text{data encoding length}} \right )$$&lt;/p&gt;
&lt;p&gt;The authors again use the idea of a &lt;em&gt;sketch&lt;/em&gt;, which they combine with symbolic search (namely, SMT solving), to produce a program.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/03/USL_pipeline.png" alt="Software that writes software: the state of program induction"&gt;&lt;/p&gt;
&lt;p&gt;The authors work with visual training examples from the Synthetic Visual Reasoning Test (SVRT), a test consisting of classification problems parsed into distinct shapes.  They define a space of simple graphics programs referred to as “turtle programs” that can control the rendering of different shapes.  The following context free grammar is used to construct “sentences” that can be mapped to different semantic models over visual concepts:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/03/USL_grammar.png" alt="Software that writes software: the state of program induction"&gt;&lt;/p&gt;
&lt;p&gt;A parser first analyzes each image and outputs structured information:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/03/USL_parser.png" alt="Software that writes software: the state of program induction"&gt;&lt;/p&gt;
&lt;p&gt;Then their learning algorithm constructs a program from this parsed data:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/03/USL_prog.png" alt="Software that writes software: the state of program induction"&gt;&lt;/p&gt;
&lt;p&gt;The authors use 23 SVRT problems and evaluate the accuracy of their parser relative to several baselines.  Unsupervised program synthesis outperforms a decision stump, convolutional network, and other discriminative baselines.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://localhost:2368/content/images/2018/03/USL_results.png" alt="Software that writes software: the state of program induction"&gt;&lt;/p&gt;
&lt;h2 id="theroadahead"&gt;The road ahead&lt;/h2&gt;
&lt;p&gt;While the programs learned by both of these systems are rather simple, the fact that this process is possible at all is encouraging.  As hardware and methods improve, there is a plausible path towards scaling many of these approaches.&lt;/p&gt;
&lt;p&gt;A debate remains.  Is it more promising to build program synthesis or program induction systems?  By training a neural network to generate human-readable source code, we benefit from transparent model interpretability.  End to end models might achieve greater performance, but often at the cost of interpretability.&lt;/p&gt;
&lt;p&gt;Programs that generate programs could one day enable us to develop systems that are more sophisticated than human-engineered ones.  Neural networks are known to be powerful function approximators that can learn arbitrary mappings.  Combining the powerful expressivity of neural networks with the symbolic logic of programming languages could yield uniquely powerful systems, that neither paradigm on its own could produce.&lt;/p&gt;
&lt;p&gt;As we have seen, there is a broad spectrum of approaches to perform this merging between the two approaches to computation.  Identifying the specifics of this symbiosis remains a unique question for the field to consider.&lt;/p&gt;
&lt;/div&gt;</content:encoded></item><item><title>Is deep art real art?</title><description>&lt;div class="kg-card-markdown"&gt;&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam venenatis, nulla et maximus feugiat, libero tortor varius nulla, vitae commodo libero lacus vel quam. Cras rhoncus neque ut ligula fermentum malesuada. Donec mollis tincidunt enim, eu ullamcorper sapien maximus eget. Nam in lectus eget mi porttitor pulvinar eu vel magna.&lt;/p&gt;&lt;/div&gt;</description><link>http://localhost:2368/can-machine-learning-algorithms-be-used-to-create-art/</link><guid isPermaLink="false">5ad18868f092d13d8a5762e7</guid><category>Perspectives</category><dc:creator>Gary Gradient</dc:creator><pubDate>Sun, 25 Feb 2018 01:54:00 GMT</pubDate><media:content url="http://localhost:2368/content/images/2018/02/1.png" medium="image"/><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;img src="http://localhost:2368/content/images/2018/02/1.png" alt="Is deep art real art?"&gt;&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam venenatis, nulla et maximus feugiat, libero tortor varius nulla, vitae commodo libero lacus vel quam. Cras rhoncus neque ut ligula fermentum malesuada. Donec mollis tincidunt enim, eu ullamcorper sapien maximus eget. Nam in lectus eget mi porttitor pulvinar eu vel magna. Nam auctor dictum lorem, ac suscipit justo posuere vitae. Etiam placerat pellentesque scelerisque. Suspendisse potenti. Nullam pharetra ante non nibh placerat tristique. Fusce tincidunt sit amet ipsum sit amet lacinia. Mauris eget fringilla eros. Proin id felis eu dolor pulvinar tincidunt in vitae enim. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Suspendisse potenti. Aenean sed neque id elit dignissim elementum ut non metus. In sed nisi in neque aliquam egestas. Sed tincidunt maximus nulla sit amet facilisis.&lt;/p&gt;
&lt;p&gt;Duis quam quam, eleifend pretium lectus ac, suscipit rhoncus nisl. Aenean ullamcorper vitae orci non interdum. Mauris lobortis suscipit imperdiet. Phasellus tempor quam massa, sit amet blandit lectus aliquet a. Morbi lorem ligula, sagittis a pharetra at, convallis non enim. Proin sodales vitae massa ac fringilla. Cras a leo aliquet, maximus justo nec, tempus odio.&lt;/p&gt;
&lt;p&gt;Duis non iaculis tortor, ac feugiat ex. Donec blandit nisi ullamcorper, tempor neque non, vehicula elit. In eget consequat turpis. Donec fringilla, odio et commodo iaculis, tortor erat tincidunt justo, quis lobortis erat massa id quam. Mauris elementum porttitor enim ut volutpat. Quisque quis erat sed sem facilisis dapibus nec a elit. Phasellus eu volutpat velit, vel feugiat mauris. Nunc vitae mattis erat, id ullamcorper ex. Quisque tellus turpis, bibendum non ex et, tristique tincidunt mi. Maecenas euismod dolor pellentesque orci tempor hendrerit. Aliquam maximus at urna non venenatis. Nunc dui velit, ultrices nec cursus ut, pellentesque nec nunc.&lt;/p&gt;
&lt;p&gt;Ut lorem sem, blandit eu pellentesque efficitur, hendrerit vehicula ligula. Aliquam porta neque purus, vel vehicula ante luctus quis. Proin interdum rhoncus sodales. Curabitur fringilla libero turpis, et maximus eros dictum non. Duis vitae velit consequat, eleifend lacus sed, aliquet lectus. Donec ut lobortis justo, gravida maximus lectus. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Etiam fermentum malesuada posuere. Praesent id porttitor nibh, at pretium ante. Maecenas ut congue augue.&lt;/p&gt;
&lt;p&gt;Cras varius semper nisi. Aliquam tempor nisl nec augue egestas, at porttitor leo facilisis. Quisque tempus velit gravida ipsum molestie egestas. Donec consectetur tortor vitae egestas gravida. Nam at dapibus magna. Nulla facilisi. Etiam dictum magna ut eros posuere, vel rhoncus magna rutrum. Nam vehicula nisi quis dui pellentesque mollis. Curabitur lobortis ornare libero, ut ullamcorper ipsum gravida eu. Proin mollis, metus non varius vestibulum, orci augue pulvinar metus, pulvinar dictum nunc ligula sed tellus. Nulla maximus felis ut efficitur varius. Duis vehicula efficitur ex quis porta. Fusce malesuada mi eu tortor elementum, dictum efficitur massa commodo. Vestibulum tempor, justo ac rutrum dictum, urna ipsum auctor mi, eu vestibulum erat arcu sed quam.&lt;/p&gt;
&lt;/div&gt;</content:encoded></item><item><title>Inside Libratus, the algorithm that beats world-class poker players</title><description>&lt;div class="kg-card-markdown"&gt;&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam venenatis, nulla et maximus feugiat, libero tortor varius nulla, vitae commodo libero lacus vel quam. Cras rhoncus neque ut ligula fermentum malesuada. Donec mollis tincidunt enim, eu ullamcorper sapien maximus eget. Nam in lectus eget mi porttitor pulvinar eu vel magna.&lt;/p&gt;&lt;/div&gt;</description><link>http://localhost:2368/on-the-success-of-libratus-superhuman-poker-ai/</link><guid isPermaLink="false">5ad18868f092d13d8a5762e3</guid><category>Overviews</category><dc:creator>Gary Gradient</dc:creator><pubDate>Wed, 21 Feb 2018 08:51:06 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1503482618211-c70b46ede8b0?ixlib=rb-0.3.5&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ&amp;s=a342e88a2f791f025c6a391ceb011472" medium="image"/><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;img src="https://images.unsplash.com/photo-1503482618211-c70b46ede8b0?ixlib=rb-0.3.5&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=1080&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ&amp;s=a342e88a2f791f025c6a391ceb011472" alt="Inside Libratus, the algorithm that beats world-class poker players"&gt;&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam venenatis, nulla et maximus feugiat, libero tortor varius nulla, vitae commodo libero lacus vel quam. Cras rhoncus neque ut ligula fermentum malesuada. Donec mollis tincidunt enim, eu ullamcorper sapien maximus eget. Nam in lectus eget mi porttitor pulvinar eu vel magna. Nam auctor dictum lorem, ac suscipit justo posuere vitae. Etiam placerat pellentesque scelerisque. Suspendisse potenti. Nullam pharetra ante non nibh placerat tristique. Fusce tincidunt sit amet ipsum sit amet lacinia. Mauris eget fringilla eros. Proin id felis eu dolor pulvinar tincidunt in vitae enim. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Suspendisse potenti. Aenean sed neque id elit dignissim elementum ut non metus. In sed nisi in neque aliquam egestas. Sed tincidunt maximus nulla sit amet facilisis.&lt;/p&gt;
&lt;p&gt;Duis quam quam, eleifend pretium lectus ac, suscipit rhoncus nisl. Aenean ullamcorper vitae orci non interdum. Mauris lobortis suscipit imperdiet. Phasellus tempor quam massa, sit amet blandit lectus aliquet a. Morbi lorem ligula, sagittis a pharetra at, convallis non enim. Proin sodales vitae massa ac fringilla. Cras a leo aliquet, maximus justo nec, tempus odio.&lt;/p&gt;
&lt;p&gt;Duis non iaculis tortor, ac feugiat ex. Donec blandit nisi ullamcorper, tempor neque non, vehicula elit. In eget consequat turpis. Donec fringilla, odio et commodo iaculis, tortor erat tincidunt justo, quis lobortis erat massa id quam. Mauris elementum porttitor enim ut volutpat. Quisque quis erat sed sem facilisis dapibus nec a elit. Phasellus eu volutpat velit, vel feugiat mauris. Nunc vitae mattis erat, id ullamcorper ex. Quisque tellus turpis, bibendum non ex et, tristique tincidunt mi. Maecenas euismod dolor pellentesque orci tempor hendrerit. Aliquam maximus at urna non venenatis. Nunc dui velit, ultrices nec cursus ut, pellentesque nec nunc.&lt;/p&gt;
&lt;p&gt;Ut lorem sem, blandit eu pellentesque efficitur, hendrerit vehicula ligula. Aliquam porta neque purus, vel vehicula ante luctus quis. Proin interdum rhoncus sodales. Curabitur fringilla libero turpis, et maximus eros dictum non. Duis vitae velit consequat, eleifend lacus sed, aliquet lectus. Donec ut lobortis justo, gravida maximus lectus. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Etiam fermentum malesuada posuere. Praesent id porttitor nibh, at pretium ante. Maecenas ut congue augue.&lt;/p&gt;
&lt;p&gt;Cras varius semper nisi. Aliquam tempor nisl nec augue egestas, at porttitor leo facilisis. Quisque tempus velit gravida ipsum molestie egestas. Donec consectetur tortor vitae egestas gravida. Nam at dapibus magna. Nulla facilisi. Etiam dictum magna ut eros posuere, vel rhoncus magna rutrum. Nam vehicula nisi quis dui pellentesque mollis. Curabitur lobortis ornare libero, ut ullamcorper ipsum gravida eu. Proin mollis, metus non varius vestibulum, orci augue pulvinar metus, pulvinar dictum nunc ligula sed tellus. Nulla maximus felis ut efficitur varius. Duis vehicula efficitur ex quis porta. Fusce malesuada mi eu tortor elementum, dictum efficitur massa commodo. Vestibulum tempor, justo ac rutrum dictum, urna ipsum auctor mi, eu vestibulum erat arcu sed quam.&lt;/p&gt;
&lt;/div&gt;</content:encoded></item><item><title>Deep Learning for Semantic Segmentation</title><description>&lt;div class="kg-card-markdown"&gt;&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam venenatis, nulla et maximus feugiat, libero tortor varius nulla, vitae commodo libero lacus vel quam. Cras rhoncus neque ut ligula fermentum malesuada. Donec mollis tincidunt enim, eu ullamcorper sapien maximus eget. Nam in lectus eget mi porttitor pulvinar eu vel magna.&lt;/p&gt;&lt;/div&gt;</description><link>http://localhost:2368/deep-learning-for-semantic-segmentation/</link><guid isPermaLink="false">5ad18868f092d13d8a5762e2</guid><category>Overviews</category><dc:creator>Gary Gradient</dc:creator><pubDate>Wed, 21 Feb 2018 08:49:15 GMT</pubDate><media:content url="http://localhost:2368/content/images/2018/02/semseg.png" medium="image"/><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;img src="http://localhost:2368/content/images/2018/02/semseg.png" alt="Deep Learning for Semantic Segmentation"&gt;&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam venenatis, nulla et maximus feugiat, libero tortor varius nulla, vitae commodo libero lacus vel quam. Cras rhoncus neque ut ligula fermentum malesuada. Donec mollis tincidunt enim, eu ullamcorper sapien maximus eget. Nam in lectus eget mi porttitor pulvinar eu vel magna. Nam auctor dictum lorem, ac suscipit justo posuere vitae. Etiam placerat pellentesque scelerisque. Suspendisse potenti. Nullam pharetra ante non nibh placerat tristique. Fusce tincidunt sit amet ipsum sit amet lacinia. Mauris eget fringilla eros. Proin id felis eu dolor pulvinar tincidunt in vitae enim. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Suspendisse potenti. Aenean sed neque id elit dignissim elementum ut non metus. In sed nisi in neque aliquam egestas. Sed tincidunt maximus nulla sit amet facilisis.&lt;/p&gt;
&lt;p&gt;Duis quam quam, eleifend pretium lectus ac, suscipit rhoncus nisl. Aenean ullamcorper vitae orci non interdum. Mauris lobortis suscipit imperdiet. Phasellus tempor quam massa, sit amet blandit lectus aliquet a. Morbi lorem ligula, sagittis a pharetra at, convallis non enim. Proin sodales vitae massa ac fringilla. Cras a leo aliquet, maximus justo nec, tempus odio.&lt;/p&gt;
&lt;p&gt;Duis non iaculis tortor, ac feugiat ex. Donec blandit nisi ullamcorper, tempor neque non, vehicula elit. In eget consequat turpis. Donec fringilla, odio et commodo iaculis, tortor erat tincidunt justo, quis lobortis erat massa id quam. Mauris elementum porttitor enim ut volutpat. Quisque quis erat sed sem facilisis dapibus nec a elit. Phasellus eu volutpat velit, vel feugiat mauris. Nunc vitae mattis erat, id ullamcorper ex. Quisque tellus turpis, bibendum non ex et, tristique tincidunt mi. Maecenas euismod dolor pellentesque orci tempor hendrerit. Aliquam maximus at urna non venenatis. Nunc dui velit, ultrices nec cursus ut, pellentesque nec nunc.&lt;/p&gt;
&lt;p&gt;Ut lorem sem, blandit eu pellentesque efficitur, hendrerit vehicula ligula. Aliquam porta neque purus, vel vehicula ante luctus quis. Proin interdum rhoncus sodales. Curabitur fringilla libero turpis, et maximus eros dictum non. Duis vitae velit consequat, eleifend lacus sed, aliquet lectus. Donec ut lobortis justo, gravida maximus lectus. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Etiam fermentum malesuada posuere. Praesent id porttitor nibh, at pretium ante. Maecenas ut congue augue.&lt;/p&gt;
&lt;p&gt;Cras varius semper nisi. Aliquam tempor nisl nec augue egestas, at porttitor leo facilisis. Quisque tempus velit gravida ipsum molestie egestas. Donec consectetur tortor vitae egestas gravida. Nam at dapibus magna. Nulla facilisi. Etiam dictum magna ut eros posuere, vel rhoncus magna rutrum. Nam vehicula nisi quis dui pellentesque mollis. Curabitur lobortis ornare libero, ut ullamcorper ipsum gravida eu. Proin mollis, metus non varius vestibulum, orci augue pulvinar metus, pulvinar dictum nunc ligula sed tellus. Nulla maximus felis ut efficitur varius. Duis vehicula efficitur ex quis porta. Fusce malesuada mi eu tortor elementum, dictum efficitur massa commodo. Vestibulum tempor, justo ac rutrum dictum, urna ipsum auctor mi, eu vestibulum erat arcu sed quam.&lt;/p&gt;
&lt;/div&gt;</content:encoded></item><item><title>Editor's Note</title><description>&lt;div class="kg-card-markdown"&gt;&lt;p&gt;In 1961, cognitive scientist and AI pioneer &lt;a href="http://courses.csail.mit.edu/6.803/pdf/steps.pdf"&gt;Marvin Minsky declared&lt;/a&gt; that “I believe… that we are on the threshold of an era that will be strongly influenced, and quite possibly dominated, by intelligent problem-solving machines.”  Fifty years later, this era has come to pass.  Automated, personalized news feeds are quickly&lt;/p&gt;&lt;/div&gt;</description><link>http://localhost:2368/editorsnote/</link><guid isPermaLink="false">5ad18868f092d13d8a5762e8</guid><dc:creator>Gary Gradient</dc:creator><pubDate>Mon, 01 Jan 2018 23:32:00 GMT</pubDate><media:content url="http://localhost:2368/content/images/2018/03/fb-profile.png" medium="image"/><content:encoded>&lt;div class="kg-card-markdown"&gt;&lt;img src="http://localhost:2368/content/images/2018/03/fb-profile.png" alt="Editor's Note"&gt;&lt;p&gt;In 1961, cognitive scientist and AI pioneer &lt;a href="http://courses.csail.mit.edu/6.803/pdf/steps.pdf"&gt;Marvin Minsky declared&lt;/a&gt; that “I believe… that we are on the threshold of an era that will be strongly influenced, and quite possibly dominated, by intelligent problem-solving machines.”  Fifty years later, this era has come to pass.  Automated, personalized news feeds are quickly redefining the way we interact with media. Computer vision is bringing us towards a world of driverless vehicles. Predictive policing is raising serious questions about how we conceptualize and address crime. The sheer effectiveness of intelligent algorithms is one of the central issues of our time; in issues from political polarization to technological unemployment, it has already made itself felt at a global scale.&lt;/p&gt;
&lt;p&gt;Despite AI’s impact and prevalence, only a few non-practitioners understand it beyond a superficial level. As the pace of developments in the field accelerates, media coverage and popular understanding of AI is beginning to lose touch with reality, falling back to outdated science-fiction tropes and sensationalism. The bleeding edge of AI research is hidden in a rapidly expanding corpus of technically dense papers with few inroads to the general public. As a result, real and pressing issues are being ignored.&lt;/p&gt;
&lt;p&gt;This is why we are starting The Gradient: to ground the discussion in technical facts. The core of our approach is providing critical, interesting, and well-informed perspectives on AI and our technological future. We have a set of diverse--and sometimes conflicting--views, but all of us share the goal of democratizing the future by bridging the gap between researchers and the public.&lt;/p&gt;
&lt;p&gt;The Gradient, like any gradient, points in the direction of the field. Supported by a strong community of researchers from university research labs like the Stanford AI Lab as well as industry research groups, we aim to publish frequently on both long-term issues and recent developments. We want to cut through both the hype and the cynicism to offer truly meaningful analysis and reportage on what has come to be the pinnacle technology of the twenty-first century.&lt;/p&gt;
&lt;p&gt;Join the conversation.&lt;/p&gt;
&lt;p&gt;--The Editors&lt;/p&gt;
&lt;/div&gt;</content:encoded></item></channel></rss>